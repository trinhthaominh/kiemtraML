{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3c2b16-d37d-4a17-977d-d3a20ee8e958",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:36px; font-weight:bold; color:#4A4A4A; background-color:#fff6e4; padding:10px; border:3px solid #f5ecda; border-radius:6px\">\n",
    "    Machine Learning Template\n",
    "    <p style=\"text-align:center; font-size:14px; font-weight:normal; color:#4A4A4A; margin-top:12px;\">\n",
    "        Author: Jens Bender <br> \n",
    "        May 2025\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7c4ae-408e-4aeb-9fe9-e20263d2ac7f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Introduction</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553068ba-3fd3-4de9-8fe7-47c03d4f9c79",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Template Overview</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è This Jupyter Notebook file provides a comprehensive <strong>machine learning template</strong> to streamline the key stages of the machine learning workflow for tabular data:\n",
    "    <ul>\n",
    "        <li><strong>Data Preprocessing:</strong>\n",
    "            <ul>\n",
    "                <li>Load, clean, transform, and save data using <code>pandas</code> and <code>sklearn</code>.</li>\n",
    "                <li>Handle duplicates, data types, missing values, and outliers.</li>\n",
    "                <li>Perform train-validation-test split, feature engineering, scaling, and encoding.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Exploratory Data Analysis (EDA):</strong>\n",
    "            <ul>\n",
    "                <li>Analyze descriptive statistics using <code>pandas</code> and <code>numpy</code>.</li>\n",
    "                <li>Visualize distributions, correlations, and relationships using <code>seaborn</code> and <code>matplotlib</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Modeling:</strong>\n",
    "            <ul>\n",
    "                <li>Train baseline models and perform hyperparameter tuning for regression and classification tasks with <code>sklearn</code> and <code>xgboost</code>.</li>\n",
    "                <li>Evaluate model performance for regression (RMSE, MAPE, R-squared) and classification (accuracy, precision, recall, F1-score).</li>\n",
    "                <li>Visualize feature importance, show model prediction examples, and save the final model.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    This template provides a flexible, customizable foundation for various datasets and use cases, making it an ideal starting point for efficient and reproducible machine learning projects. It is specifically tailored to structured tabular data (e.g., .csv, .xls, or SQL tables) using Pandas and Scikit-learn. It is not optimized for text, image, or time series data, which require specialized preprocessing, models, and tools (e.g., TensorFlow, PyTorch).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a4584-3318-42b5-9f40-a9ef0ae6b595",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Project Overview</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>üí° Example: Predicting Rental Prices</strong> <br>\n",
    "    This is an illustrative example of a project overview designed to help you write your own.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4138c51-6c52-44fc-887b-60b40faed00e",
   "metadata": {},
   "source": [
    "**Summary**  \n",
    "This project aims to build a machine learning model to predict rental prices for properties in Berlin using historical data from property listings. The model will enable stakeholders like landlords, tenants, investors, and market analysts to make data-driven decisions about rental prices, property investments, and market analysis through automated property evaluations.\n",
    "\n",
    "**Problem**  \n",
    "Determining rental prices is challenging due to the complex interactions between the many factors involved, such as property characteristics (e.g., size, bedrooms, amenities), location (e.g., neighborhood, access to public transport), and market dynamics (e.g., trends in demand, seasonality). Traditional estimation methods can be subjective and inconsistent. Machine learning offers enhanced predictive capability by capturing non-linear patterns and intricate dependencies in property data, enabling more accurate rental price predictions.\n",
    "\n",
    "**Objectives**  \n",
    "- Develop a machine learning model to accurately predict rental prices based on information from property listings.\n",
    "- Compare different models (e.g., Linear Regression, Random Forest, XGBoost) using suitable evaluation metrics (e.g., RMSE, MAPE, R¬≤).\n",
    "- Identify key price drivers through feature importance analysis.\n",
    "\n",
    "**Value Proposition**  \n",
    "This project provides actionable, data-driven insights for key stakeholders:\n",
    "- Landlords: Set competitive rental prices to attract tenants while maximizing returns.\n",
    "- Tenants: Identify reasonable and fair rental rates.\n",
    "- Investors: Evaluate property investment opportunities based on potential rental income and returns.\n",
    "- Market Analysts: Gain insights into rental price trends, key influencing factors, and market dynamics.\n",
    "\n",
    "**Business Goals**  \n",
    "- Increase rental revenue by 5%-10% within 12 months of model deployment: By accurately predicting competitive rental prices, landlords can optimize their pricing strategies and reduce vacancy rates.  \n",
    "- Cut time spent on rental price evaluation by 30%-40%: Automate the price-setting process with the model, reducing the time needed for landlords, property managers, and investors to evaluate property pricing compared to traditional manual methods.\n",
    "\n",
    "**Data**  \n",
    "The dataset contains historical information from rental properties in Berlin listed on Zillow between 2023-09-01 and 2024-08-31, provided in a single `.csv` file.\n",
    "\n",
    "Dataset Statistics:\n",
    "- Dataset size: 44,300 records\n",
    "- Target variable: Monthly rent\n",
    "- Features: 9  \n",
    "\n",
    "Data Overview Table:\n",
    "| Column | Description | Storage Type | Semantic Type | Theoretical Range | Observed Range |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Monthly Rent | Rental price per month in euros | Integer | Numerical | [0, ‚àû] | [600, 3100] |\n",
    "| Size | Living space in square meters | Float | Numerical | [0, ‚àû] | [30, 240] |\n",
    "| Bedrooms | Number of bedrooms | Integer | Numerical | [0, ‚àû] | [1, 6] |\n",
    "| Bathrooms | Number of bathrooms | Integer | Numerical | [0, ‚àû] | [1, 4] |\n",
    "| Property Type | Category of property | String | Categorical (Nominal) | Any property type [e.g., \"Apartment\", \"House\"] | 8 unique types |\n",
    "| Neighborhood | District within Berlin | String | Categorical (Nominal) | Any Berlin district [e.g., \"Charlottenburg\"] | 12 unique districts |\n",
    "| Built Year | Year of construction | Integer | Numerical | [1600, 2024] | [1904, 2024] |\n",
    "| Furnishing | Whether property is furnished | String | Categorical (Binary) | [\"Yes\", \"No\"] | [\"Yes\", \"No\"] |\n",
    "| Heating | Type of heating system | String | Categorical (Nominal) | Any heating type [e.g., \"Gas\", \"Electric\"] | 8 unique types |\n",
    "| Amenities | Available features (comma-separated list) | String | Categorical | Any combination of amenities [e.g., \"Balcony, Elevator\"] | 34 unique amenities |\n",
    "\n",
    "Example Data:\n",
    "| Size (m¬≤) | Bedrooms | Bathrooms | Property Type | Neighborhood | Built Year | Furnishing | Heating | Amenities | Monthly Rent (‚Ç¨) |\n",
    "| :-------- | :------- | :-------- | :----------- | :----------- | :--------- | :--------- | :------ | :-------- | :-------------- |\n",
    "| 80 | 2 | 1 | Apartment | Prenzlauer Berg | 1990 | Yes | Gas | Balcony, Elevator, River View | 1200 |\n",
    "| 120 | 3 | 2 | House | Charlottenburg | 2005 | No | Electric | Garden, Parking, Newly Renovated | 2500 |\n",
    "\n",
    "**Technical Requirements**  \n",
    "- Data Preprocessing:\n",
    "  - Load, clean, transform, and save data using `pandas` and `sklearn`.\n",
    "  - Handle duplicates, data types, missing values, and outliers.\n",
    "  - Perform train-validation-test split, feature engineering, scaling, and encoding.\n",
    "- Exploratory Data Analysis (EDA):\n",
    "  - Analyze descriptive statistics using `pandas` and `numpy`.\n",
    "  - Visualize distributions, correlations, and relationships using `seaborn` and `matplotlib`.\n",
    "- Modeling:\n",
    "  - Train baseline models and perform hyperparameter tuning for regression task with `sklearn` and `xgboost`.\n",
    "  - Baseline models: Linear Regression, K-Nearest Neighbors, Support Vector Machine, Random Forest, Multi-Layer Perceptron, XGBoost.\n",
    "  - Evaluate model performance using Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and R-squared (R¬≤).\n",
    "    - Success criteria: Maximum RMSE of 1200, maximum MAPE of 0.15, and minimum R¬≤ of 0.80 on the test data.\n",
    "  - Visualize feature importance, show model prediction examples, and save the final model with `pickle`.\n",
    "- Deployment:\n",
    "  - Expose the final model via a REST API (e.g., Flask, FastAPI) for easy integration with existing platforms.  \n",
    "  - Implement batch processing capabilities to deliver predictions for up to 1M data points in under 2 minutes.  \n",
    "  - Deploy on cloud infrastructure (e.g., AWS, Microsoft Azure, Google Cloud Platform) to ensure scalability.\n",
    "  - Set up model performance monitoring and data drift detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9288e0-3815-4c47-94c3-33d0a6aafc8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Setup</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00680c12-3986-4262-9d6d-e330b55fd079",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Imports</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100528be-33fb-4f95-b8f8-e791de38fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Evaluation Metrics: Regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "# Evaluation Metrics: Classification\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4caf6-1eef-4d0e-a40b-9878e1b82301",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Environment Variables</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>üîí Security Note:</strong> Setting environment variables is optional, but it is recommended if you store sensitive information (such as API keys or database credentials) in a <code>.env</code> file. Using environment variables helps keep such information secure and separate from your codebase.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bb3ec-8e57-460d-8b31-0508025f19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from .env \n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Get SQL database credentials from .env\n",
    "sql_username = os.getenv(\"SQL_USERNAME\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93f5c7-8968-4a0e-907f-a6bc60d0dd3d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Loading and Inspection</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759d1c8-d6c7-4cc0-96fe-43ca5703d6c4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">CSV</h2>\n",
    "</div>  \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Load data from a <code>.csv</code> file into a Pandas DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d0cf0-e26b-4ab4-9070-cf96d67e5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"data/your_csv_file_here.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file content could not be parsed as a CSV.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied when accessing the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63482d3-000f-4235-a16a-51d6d8a08dba",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">MySQL</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Load data from a MySQL database table into a Pandas DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81da48-0441-4908-ba1a-990449b65ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sqlalchemy import create_engine, exc\n",
    "\n",
    "# Database info\n",
    "mysql_host = \"localhost\"  # Default hostname for a MySQL server running locally\n",
    "mysql_port = 3306  # Default port for MySQL\n",
    "mysql_database_name = \"your_mysql_database_name_here\"\n",
    "mysql_table_name = \"your_mysql_table_name_here\"\n",
    "\n",
    "try:\n",
    "    # Create an SQLAlchemy engine for interacting with the MySQL database\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{sql_username}:{sql_password}@{mysql_host}:{mysql_port}/{mysql_database_name}\")\n",
    "    \n",
    "    # Load data from MySQL into DataFrame\n",
    "    with engine.connect() as connection:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {mysql_table_name}\", con=connection)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "except exc.SQLAlchemyError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7809f-dd55-4350-a52d-7c20c761a102",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Initial Data Inspection</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Basic exploration of the dataset to understand its structure and detect obvious issues.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d22f7-282d-47ee-bfcf-e85352c450c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00719b-3f66-405d-97a0-79907e91ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows to get a sense of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad1422-e239-4d5d-b6e4-31661d8050be",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Preprocessing</h1>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f47028-71ec-4f00-b5f8-9de8fbd5d9b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Standardizing Names and Labels</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Column Names</strong> <br>\n",
    "    üìå Convert all column names to snake_case for consistency, improved readability, and to minimize the risk of errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe64a2-0a4c-4fdb-b87e-3fe3353eb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to snake_case\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .str.replace(r\"[-/\\s+]\", \"_\", regex=True)  # Replace spaces and special characters with \"_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27289383-a18b-4e0e-9137-8dac6147f1fa",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Categorical Labels</strong> <br>\n",
    "    üìå Convert all categorical labels to snake_case for consistency, improved readability, and to minimize the risk of errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f49b8-66c6-45a5-ab85-e6c4157692c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_labels(categorical_label):\n",
    "    return (\n",
    "        categorical_label\n",
    "        .strip()  # Remove leading/trailing spaces\n",
    "        .lower()  # Convert to lowercase\n",
    "        .replace(\"-\", \"_\")  # Replace hyphens with \"_\"\n",
    "        .replace(\"/\", \"_\")  # Replace slashes with \"_\"\n",
    "        .replace(\" \", \"_\")  # Replace spaces with \"_\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define categorical columns to standardize labels\n",
    "columns_to_standardize = [\"categorical_column_1\", \"categorical_column_2\", \"categorical_column_3\"]\n",
    "\n",
    "# Apply standardization of categorical labels\n",
    "for column in columns_to_standardize:\n",
    "    df[column] = df[column].apply(standardize_categorical_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f1a0d-a98f-4446-8d44-d7e553e6230f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Duplicates</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and remove duplicates based on all columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71162a8d-4f29-470c-a919-8494ef16e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on all columns\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0253293-b7c9-4335-9eef-61b29a09ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b2986-ba9a-4182-9ae7-de5cc89cb98c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and remove duplicates based on the ID column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fe17b-3b29-4c37-a016-0034d582e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on the ID column\n",
    "df.duplicated([\"id\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a448227-a451-4260-a196-05918a9dc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates([\"id\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f43e8-f544-414c-8979-d3a52b0b919d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and remove duplicates based on a combination of specific columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec11f9-42d4-4bd2-a769-c6affd2778d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on a combination of specific columns\n",
    "df.duplicated([\"column_1\", \"column_2\", \"column_3\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620ab52-9ff0-4b3d-a0d6-ee856fdfb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates([\"column_1\", \"column_2\", \"column_3\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28c409-34a3-493f-8cd9-856b5c25e0d3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Data Types</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and convert incorrect storage data types.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a666c9-9037-4de3-9c46-3aa9e7fd3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify storage data types\n",
    "df.dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92d718-ec5a-491c-873b-f2b8230f71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column from str to int\n",
    "df[\"int_column\"] = df[\"str_column\"].astype(\"Int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb41a-a715-4c64-a6ca-9db164c4dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column from str to datetime\n",
    "df[\"datetime_column\"] = pd.to_datetime(df[\"str_column\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5059d1-bc0d-4a7f-8d65-bdcee4328403",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify object columns with two unique categories and convert them to boolean columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c2573-36aa-41b2-994a-4118f925f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify object columns with two unique categories \n",
    "df.select_dtypes(include=[\"object\"]).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5686b71-8f6b-48e3-b82f-4498dc7ef48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column from object to boolean\n",
    "df[\"bool_column\"] = df[\"object_column\"].map({\"category_1\": True, \"category_2\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116c8e3-2bcf-4843-bdd8-df3d1d920f6d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Train-Validation-Test Split</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è The ideal split depends on the dataset size and the task. A general guideline is:\n",
    "    <table style=\"margin-left:0; margin-top:15px; border-collapse: collapse;\">\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th style=\"text-align:left; background-color:#e8f4fd; padding:8px;\">Dataset Size</th>\n",
    "                <th style=\"text-align:left; background-color:#e8f4fd; padding:8px;\">Training Set</th>\n",
    "                <th style=\"text-align:left; background-color:#e8f4fd; padding:8px;\">Validation Set</th>\n",
    "                <th style=\"text-align:left; background-color:#e8f4fd; padding:8px;\">Test Set</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td style=\"background-color:#e8f4fd; text-align:left; padding:8px;\">Smaller datasets (<10,000)</td>\n",
    "                <td style=\"background-color:#e8f4fd; text-align:left; padding:8px;\">70%</td>\n",
    "                <td style=\"background-color:#e8f4fd; text-align:left; padding:8px;\">15%</td>\n",
    "                <td style=\"background-color:#e8f4fd; text-align:left; padding:8px;\">15%</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"background-color:#d0e7fa; text-align:left; padding:8px;\">Larger datasets (‚â•10,000)</td>\n",
    "                <td style=\"background-color:#d0e7fa; text-align:left; padding:8px;\">80%</td>\n",
    "                <td style=\"background-color:#d0e7fa; text-align:left; padding:8px;\">10%</td>\n",
    "                <td style=\"background-color:#d0e7fa; text-align:left; padding:8px;\">10%</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c905a81-4c0d-4acf-85cd-04f81059d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X features and y target\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7fc1d-be3a-4623-9e16-aac8e85d3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (70% train, 30% temporary)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary data into validation and test sets (50% each)\n",
    "# Note: This accomplishes a 70% training, 15% validation and 15% test set size\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Delete the temporary data to free up memory\n",
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff6c41-2244-462e-968a-9e8473a89ed6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Engineering New Features</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Derive a new feature from a raw text column or a categorical text column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a8b2b-ba41-4074-812a-045f4a05ecab",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">From Raw Text</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Derive a categorical, numerical, or boolean feature from a raw text column, which contains unstructured text data stored as strings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9931c-d4f6-4255-a894-2be60a02c26e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Categorical Feature</strong> <br>\n",
    "    üìå Extract a categorical feature from a raw text column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639472a6-9633-4972-b19f-e57b10ad2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a category from a string   \n",
    "def extract_category_from_string(string):\n",
    "    # Map categories to their corresponding list of keywords\n",
    "    category_keywords_map = {\n",
    "        \"Category 1\": [\"Keyword 1\", \"Keyword 2\", \"Keyword 3\"],\n",
    "        \"Category 2\": [\"Keyword 4\", \"Keyword 5\", \"Keyword 6\"],\n",
    "        \"Category 3\": [\"Keyword 7\", \"Keyword 8\", \"Keyword 9\"]\n",
    "    }\n",
    "\n",
    "    # Loop through each category and its associated keywords \n",
    "    for category, keywords in category_keywords_map.items():\n",
    "        # Check if any keyword is present in the string\n",
    "        if any(keyword in string for keyword in keywords):\n",
    "            return category  # Return the category corresponding to the keyword\n",
    "    return np.nan  # Return a missing value if no keyword matches\n",
    "\n",
    "# Apply function on an existing string column to create a new categorical feature column\n",
    "X_train[\"categorical_feature\"] = X_train[\"str_column\"].apply(extract_category_from_string)\n",
    "X_val[\"categorical_feature\"] = X_val[\"str_column\"].apply(extract_category_from_string)\n",
    "X_test[\"categorical_feature\"] = X_test[\"str_column\"].apply(extract_category_from_string)\n",
    "\n",
    "# Show category frequencies\n",
    "print(X_train[\"categorical_feature\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9089b-a2fd-4305-aa23-086332d45c01",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Numerical Feature</strong> <br>\n",
    "    üìå Extract a numerical feature from a raw text column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41134f56-1d30-435d-9f32-6ff428d940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "# Function to extract the first number in a string \n",
    "def extract_number_from_string(string):\n",
    "    first_number = re.search(r\"\\b-?\\d+([.,]\\d+)?\\b\", string)  # searches for first integer or float (positive or negative; decimal separator \".\" or \",\")\n",
    "    if first_number:\n",
    "        return float(first_number.group().replace(\",\", \".\"))  # Replace \",\" with \".\" as decimal separator  \n",
    "    else:\n",
    "        return np.nan  # Return a missing value if no number in string\n",
    "\n",
    "# Apply function on an existing string column to create a new numerical feature column\n",
    "X_train[\"numerical_feature\"] = X_train[\"str_column\"].apply(extract_number_from_string)\n",
    "X_val[\"numerical_feature\"] = X_val[\"str_column\"].apply(extract_number_from_string)\n",
    "X_test[\"numerical_feature\"] = X_test[\"str_column\"].apply(extract_number_from_string)\n",
    "\n",
    "# Show descriptive statistics of numerical feature\n",
    "X_train[\"numerical_feature\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba6942-83b2-4bd4-9223-94bfc963beef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Boolean Feature</strong> <br>\n",
    "    üìå Extract a boolean feature from a raw text column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5277ca-3ef2-4e5f-8b06-e2701b601f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords to determine if the feature is present or absent\n",
    "keywords = [\"keyword 1\", \"keyword 2\", \"keyword 3\"]\n",
    "\n",
    "# Extract boolean feature column: True if any keyword is found in the string column\n",
    "X_train[\"boolean_feature\"] = X_train[\"str_column\"].apply(lambda x: any(keyword.lower() in x.lower() for keyword in keywords))\n",
    "X_val[\"boolean_feature\"] = X_val[\"str_column\"].apply(lambda x: any(keyword.lower() in x.lower() for keyword in keywords))\n",
    "X_test[\"boolean_feature\"] = X_test[\"str_column\"].apply(lambda x: any(keyword.lower() in x.lower() for keyword in keywords))\n",
    "\n",
    "# Show frequencies of boolean feature\n",
    "print(X_train[\"boolean_feature\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7e6a1-82f2-4d28-9370-289ebd6eb7b8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">From Categorical Text</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Derive a new feature from a categorical text column, which contains a predefined set of unique categories respresented as strings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb54ce-f8dc-4103-97f4-451b3c238ef2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Target Encoding</strong> <br>\n",
    "    üìå Derive a numerical feature from a categorical text column by encoding categories based on the mean of the target variable. This method is especially useful for categorical columns with high cardinality (i.e., a large number of unique categories).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c78b63-8b23-49d5-91aa-e556ceca0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_train and y_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Calculate mean target by category and global mean based on the training data\n",
    "mean_target_by_category = df_train.groupby(\"str_column\")[\"target_variable\"].mean()\n",
    "mean_target = df_train[\"target_variable\"].mean()\n",
    "\n",
    "# Replace string categories with corresponding mean target in training, validation, and test data (use global mean for unseen categories) \n",
    "X_train[\"numerical_feature\"] = X_train[\"str_column\"].map(mean_target_by_category).fillna(mean_target)\n",
    "X_val[\"numerical_feature\"] = X_val[\"str_column\"].map(mean_target_by_category).fillna(mean_target)\n",
    "X_test[\"numerical_feature\"] = X_test[\"str_column\"].map(mean_target_by_category).fillna(mean_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07075cb-2601-4e9f-b1af-eb76d0c7c650",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Tiering</strong> <br>\n",
    "    üìå Group categories into predefined tiers based on domain-specific factors that are relevant for predicting the target variable using subject matter expertise. This method is useful for improving predictive performance and handling categorical columns with high cardinality (i.e., a large number of unique categories). <br><br>\n",
    "\n",
    "üí° Example: Categorize 300+ cities into 3 tiers based on factors such as employment opportunities, cost of living, and population density.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3581c-7b0b-4a70-ac27-db1b41c3eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_tier(category):\n",
    "    # Define a dictionary mapping categories to their respective tiers\n",
    "    tier_map = {\n",
    "        \"category_1\": \"tier_1\", \"category_2\": \"tier_1\", \"category_3\": \"tier_1\", \n",
    "        \"category_4\": \"tier_1\", \"category_5\": \"tier_1\", \"category_6\": \"tier_1\", \n",
    "        \"category_7\": \"tier_1\", \"category_8\": \"tier_1\", \"category_9\": \"tier_1\", \n",
    "        \"category_10\": \"tier_1\",\n",
    "        \n",
    "        \"category_11\": \"tier_2\", \"category_12\": \"tier_2\", \"category_13\": \"tier_2\",\n",
    "        \"category_14\": \"tier_2\", \"category_15\": \"tier_2\", \"category_16\": \"tier_2\",\n",
    "        \"category_17\": \"tier_2\", \"category_18\": \"tier_2\", \"category_19\": \"tier_2\",\n",
    "        \"category_20\": \"tier_2\",\n",
    "        \n",
    "        \"category_21\": \"tier_3\", \"category_22\": \"tier_3\", \"category_23\": \"tier_3\",\n",
    "        \"category_24\": \"tier_3\", \"category_25\": \"tier_3\", \"category_26\": \"tier_3\",\n",
    "        \"category_27\": \"tier_3\", \"category_28\": \"tier_3\", \"category_29\": \"tier_3\",\n",
    "        \"category_30\": \"tier_3\"\n",
    "        # Add more categories as needed...\n",
    "    }\n",
    "\n",
    "    # Return the tier based on the category (default to \"tier_2\" for unknown categories)\n",
    "    return tier_map.get(category, \"tier_2\")\n",
    "\n",
    "# Apply the function to create the tier feature in training, validation, and test data\n",
    "X_train[\"tier\"] = X_train[\"str_column\"].apply(derive_tier)\n",
    "X_val[\"tier\"] = X_val[\"str_column\"].apply(derive_tier)\n",
    "X_test[\"tier\"] = X_test[\"str_column\"].apply(derive_tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416056c2-ce84-4a88-902e-42ac63b4b96b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Defining Semantic Types</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Define semantic column types (numerical, categorical, boolean) for downstream tasks like additional preprocessing steps, exploratory data analysis, and machine learning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266e9b3-4b24-41e6-bd6d-b883f051ab6d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Manual</strong> <br>\n",
    "    üìå Option 1: Define semantic column types manually for small datasets or when you have specific requirements and need precise control.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925e70a-c6c1-4c77-b1e3-c598ed772021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify storage data types\n",
    "print(X_train.dtypes())\n",
    "print(y_train.dtypes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f22c0-b7bf-42f6-97e3-aefbcd9a05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define semantic column types manually\n",
    "numerical_columns = [\"numerical_column_1\", \"numerical_column_2\", \"numerical_column_3\"]\n",
    "categorical_columns = [\"categorical_column_1\", \"categorical_column_2\", \"categorical_column_3\"]\n",
    "boolean_columns = [\"boolean_column_1\", \"boolean_column_2\", \"boolean_column_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52666e48-c4da-4f3d-ae3d-46329239f669",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Programmatic</strong> <br>\n",
    "    üìå Option 2: Define semantic column types programmatically for large datasets or when you need to automate the process, ensuring efficiency and scalability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4574d0-883f-4adf-97db-c835ad3a2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_train and y_train\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Define semantic column types programmatically based on storage data types\n",
    "numerical_columns = df_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_columns = df_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "boolean_columns = df_train.select_dtypes(include=[\"bool\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4d1a9-db9b-4e28-ba65-986831dd8b38",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Missing Values</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Identification</strong> <br>\n",
    "    üìå Identify missing values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bb454-d9c9-4b6e-8203-a4eaf0418e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values in training, validation, and test data\n",
    "print(\"Training Data - Features:\")\n",
    "print(X_train.isnull().sum())\n",
    "print(\"\\nTraining Data - Target Variable:\")\n",
    "print(y_train.isnull().sum())\n",
    "\n",
    "print(\"\\nValidation Data - Features:\")\n",
    "print(X_val.isnull().sum())\n",
    "print(\"\\nValidation Data - Target Variable:\")\n",
    "print(y_val.isnull().sum())\n",
    "\n",
    "print(\"\\nTest Data - Features:\")\n",
    "print(X_test.isnull().sum())\n",
    "print(\"\\nTest Data - Target Variable:\")\n",
    "print(y_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c06ef-bbbf-4d18-bde4-e7a50fc1a1a3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Imputation</strong> <br>\n",
    "    üìå Impute missing values in a numerical column using the median.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680d14e-5853-4c3f-ac50-d639b7033fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of numerical column\n",
    "X_train[\"numerical_column\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ca973-3ef1-43a3-b690-69c0ed2f0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median from training data\n",
    "median = X_train[\"numerical_column\"].median()\n",
    "\n",
    "# Impute median in training, validation, and test data\n",
    "X_train[\"numerical_column\"] = X_train[\"numerical_column\"].fillna(median)\n",
    "X_val[\"numerical_column\"] = X_val[\"numerical_column\"].fillna(median)\n",
    "X_test[\"numerical_column\"] = X_test[\"numerical_column\"].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018d4b9-c440-4b15-aafb-a4bb3a3e79fa",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Impute missing values in a categorical column using the mode (most frequent value). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638cab2-bc30-4ae3-9cc1-310b50d5cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies of categorical column\n",
    "X_train[\"categorical_column\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c624b-2aba-4c16-99a3-b3717a9d1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mode from training data\n",
    "mode = X_train[\"categorical_column\"].mode()[0]\n",
    "\n",
    "# Impute mode in training, validation, and test data\n",
    "X_train[\"categorical_column\"] = X_train[\"categorical_column\"].fillna(mode)\n",
    "X_val[\"categorical_column\"] = X_val[\"categorical_column\"].fillna(mode)\n",
    "X_test[\"categorical_column\"] = X_test[\"categorical_column\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9519b6-a492-4308-abc0-89a470543652",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Deletion</strong> <br>\n",
    "    üìå Delete rows with missing values on any column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc408e-1e71-4ec1-a678-9b14e16878ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where any column has a missing value \n",
    "X_train.dropna(inplace=True)\n",
    "X_val.dropna(inplace=True)\n",
    "X_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a23ff0-21b6-499c-b635-23da154564a5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Delete rows with missing values on specific columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf359f7-2519-47cf-bb29-60d64918626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where either column_1 or column_2 has a missing value \n",
    "X_train.dropna(subset=[\"column_1\", \"column_2\"], how=\"any\", inplace=True)\n",
    "X_val.dropna(subset=[\"column_1\", \"column_2\"], how=\"any\", inplace=True)\n",
    "X_test.dropna(subset=[\"column_1\", \"column_2\"], how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e082e3-b3ab-4c7b-932b-339610614132",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Outliers</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c361d5-d89b-49af-a64b-f95995b3ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creating a custom transformer class to handle outliers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5625577-0d32-402f-bac1-1d4184d765c6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">3SD Method</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and remove univariate outliers in numerical columns by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75088ce-c0c6-4e46-bd24-07661ed7eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "            \n",
    "        # Calculate statistics (mean, standard deviation, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"mean\"] = df[self.numerical_columns_].mean()\n",
    "        self.stats_[\"sd\"] = df[self.numerical_columns_].std()\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"mean\"] - 3 * self.stats_[\"sd\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"mean\"] + 3 * self.stats_[\"sd\"]\n",
    "        \n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "     \n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        # Show outliers across all columns\n",
    "        if len(self.numerical_columns_) == 1:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in the '{self.numerical_columns_[0]}' column.\")\n",
    "        else:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in one or more numerical columns.\")\n",
    " \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        masks = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        final_mask = masks.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        print(f\"Removed {(~final_mask).sum()} rows ({(~final_mask).sum() / len(final_mask) * 100:.1f}%) with outliers.\")\n",
    "        return df[final_mask]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_3sd.fit(X_train, numerical_columns)\n",
    "\n",
    "# Show mean, sd, cutoff values, and outliers by column for training data\n",
    "print(\"\\nOutliers by column:\")\n",
    "round(outlier_remover_3sd.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58debb7b-adfd-425a-96b1-bf309a4ef761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "print(\"Training Data:\")\n",
    "X_train_no_outliers = outlier_remover_3sd.transform(X_train)\n",
    "print(\"\\nValidation Data:\")\n",
    "X_val_no_outliers = outlier_remover_3sd.transform(X_val)\n",
    "print(\"\\nTest Data:\")\n",
    "X_test_no_outliers = outlier_remover_3sd.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a030d86-a4da-4b67-bd83-11dcc2af33ec",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">1.5 IQR Method</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and remove univariate outliers in numerical columns using the 1.5 interquartile range (IQR) rule. Specifically, a data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83870620-24da-4801-9520-ed1a3650a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to identify and remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "        \n",
    "        # Calculate statistics (first quartile, third quartile, interquartile range, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"Q1\"] = df[self.numerical_columns_].quantile(0.25)\n",
    "        self.stats_[\"Q3\"] = df[self.numerical_columns_].quantile(0.75)\n",
    "        self.stats_[\"IQR\"] = self.stats_[\"Q3\"] - self.stats_[\"Q1\"]\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"Q1\"] - 1.5 * self.stats_[\"IQR\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"Q3\"] + 1.5 * self.stats_[\"IQR\"]\n",
    "\n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "\n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        # Show outliers across all columns\n",
    "        if len(self.numerical_columns_) == 1:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in the '{numerical_columns[0]}' column.\")\n",
    "        else:\n",
    "            print(f\"\\nIdentified {self.outliers_} rows ({self.outliers_ / len(self.final_mask_) * 100:.1f}%) with outliers in one or more numerical columns.\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for new df\n",
    "        masks = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        final_mask = masks.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        print(f\"Removed {(~final_mask).sum()} rows ({(~final_mask).sum() / len(final_mask) * 100:.1f}%) with outliers.\")\n",
    "        return df[final_mask]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_iqr.fit(X_train, numerical_columns)\n",
    "\n",
    "# Show outliers by column for training data\n",
    "print(\"\\nOutliers by column:\")\n",
    "round(outlier_remover_iqr.stats_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798816c-e967-4cd5-a68d-a1bbe2028057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "print(\"Training Data:\")\n",
    "X_train_no_outliers = outlier_remover_iqr.transform(X_train)\n",
    "print(\"\\nValidation Data:\")\n",
    "X_val_no_outliers = outlier_remover_iqr.transform(X_val)\n",
    "print(\"\\nTest Data:\")\n",
    "X_test_no_outliers = outlier_remover_iqr.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa22ef-8cb3-4db1-9757-faef72f78355",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Isolation Forest</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and remove multivariate outliers using the isolation forest algorithm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725328d-f457-45b2-9aeb-8f82feb2206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize isolation forest\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Define columns to use: All numerical and boolean features \n",
    "numerical_boolean_features = numerical_columns + boolean_columns  # Remove the target variable if necessary\n",
    "\n",
    "# Fit isolation forest on training data\n",
    "isolation_forest.fit(X_train[numerical_boolean_features])\n",
    "\n",
    "# Predict outliers on training, validation, and test data\n",
    "X_train[\"outlier\"] = isolation_forest.predict(X_train[numerical_boolean_features])\n",
    "X_train[\"outlier_score\"] = isolation_forest.decision_function(X_train[numerical_boolean_features])\n",
    "X_val[\"outlier\"] = isolation_forest.predict(X_val[numerical_boolean_features])\n",
    "X_val[\"outlier_score\"] = isolation_forest.decision_function(X_val[numerical_boolean_features])\n",
    "X_test[\"outlier\"] = isolation_forest.predict(X_test[numerical_boolean_features])\n",
    "X_test[\"outlier_score\"] = isolation_forest.decision_function(X_test[numerical_boolean_features])\n",
    "\n",
    "# Show number of outliers\n",
    "n_outliers_train = X_train[\"outlier\"].value_counts()[-1]\n",
    "contamination_train = X_train[\"outlier\"].value_counts()[-1] / X_train[\"outlier\"].value_counts().sum()\n",
    "print(f\"Training Data: Identified {n_outliers_train} rows ({100 * contamination_train:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_val = X_val[\"outlier\"].value_counts()[-1]\n",
    "contamination_val = X_val[\"outlier\"].value_counts()[-1] / X_val[\"outlier\"].value_counts().sum()\n",
    "print(f\"Validation Data: Identified {n_outliers_val} rows ({100 * contamination_val:.1f}%) as multivariate outliers.\")\n",
    "\n",
    "n_outliers_test = X_test[\"outlier\"].value_counts()[-1]\n",
    "contamination_test = X_test[\"outlier\"].value_counts()[-1] / X_test[\"outlier\"].value_counts().sum()\n",
    "print(f\"Test Data: Identified {n_outliers_test} rows ({100 * contamination_test:.1f}%) as multivariate outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2798b5c-f489-4f07-8e5f-f2cd88dd5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix to visualize outliers for a subsample of the training data\n",
    "X_train_subsample = X_train[numerical_boolean_features + [\"outlier\"]].sample(n=5000, random_state=42)\n",
    "sns.pairplot(X_train_subsample, hue=\"outlier\", palette={1: \"#4F81BD\", -1: \"#D32F2F\"}, plot_kws={\"alpha\":0.6, \"s\":40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa2c6a-2555-4437-8694-ee777544fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_no_outliers = X_train[X_train[\"outlier\"] == 1]\n",
    "print(f\"Training Data: Removed {X_train[X_train['outlier'] == -1].shape[0]} rows ({X_train[X_train['outlier'] == -1].shape[0] / X_train.shape[0] * 100:.1f}%) with multivariate outliers.\") \n",
    "X_val_no_outliers = X_val[X_val[\"outlier\"] == 1]\n",
    "print(f\"Validation Data: Removed {X_val[X_val['outlier'] == -1].shape[0]} rows ({X_val[X_val['outlier'] == -1].shape[0] / X_val.shape[0] * 100:.1f}%) with multivariate outliers.\") \n",
    "X_test_no_outliers = df_test[X_test[\"outlier\"] == 1]\n",
    "print(f\"Test Data: Removed {X_test[X_test['outlier'] == -1].shape[0]} rows ({X_test[X_test['outlier'] == -1].shape[0] / X_test.shape[0] * 100:.1f}%) with multivariate outliers.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d2469-461b-4f5a-a335-8004df91637f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Feature Scaling and Encoding</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Use a <code>ColumnTransformer</code> to preprocess columns based on their semantic type. This allows the appropriate transformation to each semantic column type in a single step.\n",
    "    <ul>\n",
    "        <li>Scale numerical columns:</li>\n",
    "        <ul>\n",
    "            <li><code>StandardScaler</code>: Scales features to have mean = 0 and standard deviation = 1.</li>\n",
    "            <li><code>MinMaxScaler</code>: Scales features to a range between [0, 1].</li>\n",
    "        </ul>\n",
    "        <li>Encode categorical columns:</li>\n",
    "        <ul>\n",
    "            <li>Nominal columns: <code>OneHotEncoder</code> to convert string categories into binary (one-hot) encoded columns; use for unordered categories (e.g., red, green, blue).</li>\n",
    "            <li>Ordinal columns: <code>OrdinalEncoder</code> to convert string categories into integers; use for ordered categories (e.g., low, medium, high).</li>\n",
    "        </ul>\n",
    "        <li>Retain boolean columns: Pass through boolean columns unchanged using <code>remainder=\"passthrough\"</code>.</li>\n",
    "    </ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8131ac-71a4-48c7-bbf0-cecc33d66843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nominal and ordinal columns\n",
    "nominal_columns = [\"nominal_column_1\", \"nominal_column_2\", \"nominal_column_3\"]\n",
    "ordinal_columns = [\"ordinal_column_1\", \"ordinal_column_2\", \"ordinal_column_3\"]\n",
    "\n",
    "# Define the explicit order of categories for all ordinal columns\n",
    "ordinal_column_orders = [\n",
    "    [\"low\", \"medium\", \"high\"],  # Order for ordinal_column_1\n",
    "    [\"poor\", \"average\", \"good\", \"excellent\"],  # Order for ordinal_column_2\n",
    "    [\"small\", \"medium\", \"large\"]  # Order for ordinal_column_3\n",
    "]\n",
    "\n",
    "# Initialize a column transformer \n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), numerical_columns),  # Use MinMaxScaler() for min-max normalization\n",
    "        (\"nominal_encoder\", OneHotEncoder(drop=\"first\"), nominal_columns),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories=ordinal_column_orders), ordinal_columns)  \n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the column transformer to the training data and apply transformations\n",
    "X_train_transformed = column_transformer.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformations to the validation and test data\n",
    "X_val_transformed = column_transformer.transform(X_val)\n",
    "X_test_transformed = column_transformer.transform(X_test)\n",
    "\n",
    "# Get transformed column names\n",
    "nominal_encoded_columns = list(column_transformer.named_transformers_[\"nominal_encoder\"].get_feature_names_out())\n",
    "passthrough_columns = list(X_train.columns.difference(numerical_columns + nominal_columns + ordinal_columns, sort=False))\n",
    "transformed_columns = numerical_columns + nominal_encoded_columns + ordinal_columns + passthrough_columns\n",
    "\n",
    "# Convert transformed data from arrays to DataFrames with column names \n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=transformed_columns, index=X_train.index)\n",
    "X_val_transformed = pd.DataFrame(X_val_transformed, columns=transformed_columns, index=X_val.index)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=transformed_columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d499010-799d-4bec-9a7a-12e6812dbe17",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Polynomial Features</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Create polynomial features in combination with feature scaling and encoding. Use a <code>ColumnTransformer</code> to preprocess columns based on their semantic type. This allows the appropriate transformation to each semantic column type in a single step. \n",
    "    <ul>\n",
    "        <li>Numerical columns:</li>\n",
    "        <ul>\n",
    "            <li><code>Pipeline</code> to create polynomial features followed by feature scaling.</li>\n",
    "            <li><code>PolynomialFeatures</code> to create polynomial terms (e.g., squared terms and interaction terms).</li>\n",
    "            <li>Feature scaling:</li>\n",
    "            <ul>\n",
    "                <li><code>StandardScaler</code>: Scales features to have mean = 0 and standard deviation = 1.</li>\n",
    "                <li><code>MinMaxScaler</code>: Scales features to a range between [0, 1].</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "        <li>Categorical columns:</li>\n",
    "        <ul>\n",
    "            <li>Nominal columns: <code>OneHotEncoder</code> to convert string categories into binary (one-hot) encoded columns; use for unordered categories (e.g., red, green, blue).</li>\n",
    "            <li>Ordinal columns: <code>OrdinalEncoder</code> to convert string categories into integers; use for ordered categories (e.g., low, medium, high).</li>\n",
    "        </ul>\n",
    "        <li>Retain boolean columns: Pass through boolean columns unchanged using <code>remainder=\"passthrough\"</code>.</li>\n",
    "    </ul>\n",
    "</div> \n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Polynomial features are commonly used with models such as linear regression, logistic regression, and elastic net regression to capture non-linear relationships and interactions between features. Polynomial features are often combined with Elastic Net regularization (balancing the effects of both Lasso and Ridge regularization) to prevent overfitting.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb6c0f-9335-4bd1-a205-6900b5a5125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nominal and ordinal columns\n",
    "nominal_columns = [\"nominal_column_1\", \"nominal_column_2\", \"nominal_column_3\"]\n",
    "ordinal_columns = [\"ordinal_column_1\", \"ordinal_column_2\", \"ordinal_column_3\"]\n",
    "\n",
    "# Define the explicit order of categories for all ordinal columns\n",
    "ordinal_column_orders = [\n",
    "    [\"low\", \"medium\", \"high\"],  # Order for ordinal_column_1\n",
    "    [\"poor\", \"average\", \"good\", \"excellent\"],  # Order for ordinal_column_2\n",
    "    [\"small\", \"medium\", \"large\"]  # Order for ordinal_column_3\n",
    "]\n",
    "\n",
    "# Initialize a column transformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numerical columns: Use a pipeline to create polynomial features followed by feature scaling \n",
    "        (\"polynomial_scaler\", Pipeline([\n",
    "            (\"polynomial\", PolynomialFeatures(degree=2, include_bias=False)),  # degree hyperparameter tuning is recommended \n",
    "            (\"scaler\", StandardScaler())  # Use MinMaxScaler() for min-max normalization\n",
    "        ]), numerical_columns),\n",
    "        \n",
    "        # Nominal columns: One-hot encoding\n",
    "        (\"nominal_encoder\", OneHotEncoder(drop=\"first\"), nominal_columns),\n",
    "\n",
    "        # Ordinal Columns: Ordinal encoding with explicit category order\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories=ordinal_column_orders), ordinal_columns)  \n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Keep boolean columns as they are\n",
    ")\n",
    "\n",
    "# Fit the column transformer to the training data and apply transformations\n",
    "X_train_transformed = column_transformer.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformations to the validation and test data\n",
    "X_val_transformed = column_transformer.transform(X_val)\n",
    "X_test_transformed = column_transformer.transform(X_test)\n",
    "\n",
    "# Get transformed column names\n",
    "numerical_polynomial_columns = list(column_transformer.named_transformers_[\"polynomial_scaler\"].named_steps[\"polynomial\"].get_feature_names_out(numerical_columns))\n",
    "nominal_encoded_columns = list(column_transformer.named_transformers_[\"nominal_encoder\"].get_feature_names_out())\n",
    "passthrough_columns = list(X_train.columns.difference(numerical_columns + nominal_columns + ordinal_columns, sort=False))\n",
    "transformed_columns = numerical_polynomial_columns + nominal_encoded_columns + ordinal_columns + passthrough_columns\n",
    "\n",
    "# Convert transformed data from arrays to DataFrames with column names \n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=transformed_columns, index=X_train.index)\n",
    "X_val_transformed = pd.DataFrame(X_val_transformed, columns=transformed_columns, index=X_val.index)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=transformed_columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d48183-607c-4d2b-a462-7d22b0ee0fc4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Saving Data</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fa5b4-8784-4081-9de8-dc83de39533d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">CSV</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Save preprocessed data from a Pandas DataFrame to a <code>.csv</code> file in the <code>data</code> directory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d25a24-5dc0-40a4-a022-874a720e9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Merge transformed X features and y target variable\n",
    "df_train_transformed = pd.concat([X_train_transformed, y_train], axis=1)\n",
    "df_val_transformed = pd.concat([X_val_transformed, y_val], axis=1)\n",
    "df_test_transformed = pd.concat([X_test_transformed, y_test], axis=1)\n",
    "\n",
    "# Save as .csv  \n",
    "df_train_transformed.to_csv(\"data/df_train_preprocessed.csv\", index=False)\n",
    "df_val_transformed.to_csv(\"data/df_val_preprocessed.csv\", index=False)\n",
    "df_test_transformed.to_csv(\"data/df_test_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bad71b-409a-405e-945d-a0b3a30bbebf",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">MySQL</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Save preprocessed data from a Pandas DataFrame to a MySQL database table. <br><br>\n",
    "    üîí Make sure <code>sql_username</code> and <code>sql_password</code> were imported as environment variables.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f078ca-246c-4ce0-b00e-515db9e916e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database info\n",
    "mysql_host = \"localhost\"  # Default hostname for a MySQL server running locally\n",
    "mysql_port = 3306  # Default port for MySQL\n",
    "mysql_database_name = \"your_mysql_database_name_here\"\n",
    "mysql_table_name = \"preprocessed_data\"\n",
    "\n",
    "# Merge transformed X features and y target variable\n",
    "df_train_transformed = pd.concat([X_train_transformed, y_train], axis=1)\n",
    "df_val_transformed = pd.concat([X_val_transformed, y_val], axis=1)\n",
    "df_test_transformed = pd.concat([X_test_transformed, y_test], axis=1)\n",
    "\n",
    "try:\n",
    "    # Create an SQLAlchemy engine for interacting with the MySQL database\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{sql_username}:{sql_password}@{mysql_host}:{mysql_port}/{mysql_database_name}\")\n",
    "    \n",
    "    # Save data to MySQL \n",
    "    with engine.connect() as connection:\n",
    "        df_train_transformed.to_sql(name=\"df_train_preprocessed\", con=connection, if_exists=\"replace\", index=False)\n",
    "        df_val_transformed.to_sql(name=\"df_val_preprocessed\", con=connection, if_exists=\"replace\", index=False)\n",
    "        df_test_transformed.to_sql(name=\"df_test_preprocessed\", con=connection, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"Preprocessed data successfully saved to MySQL.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving preprocessed data to MySQL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca10b19-02a9-4a0f-b543-7374f677d139",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Exploratory Data Analysis (EDA)</h1>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Merge <code>X_train</code> and <code>y_train</code> to create a single DataFrame with both features and target, containing all partially preprocessed columns (not yet scaled or encoded) for EDA. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07919b-7bfa-483b-b4e8-1d31a618b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge X_train and y_train \n",
    "df_train = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b0e23-3001-433d-a0fe-9d3d0c040fe6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Univariate EDA</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Analyze the distribution of a single column using descriptive statistics and visualizations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb6cb0-f661-4be4-b0b5-bb6f0ba46fee",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical Columns</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Examine descriptive statistics (e.g., mean, median, standard deviation) and visualize the distributions (e.g., histograms) of numerical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe81ae6-559e-432b-8cf2-8643b13ef6dd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br>\n",
    "    üìå Examine descriptive statistics of numerical columns. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715a025-45ee-4ee8-bfff-37ff19ceddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of a single numerical column\n",
    "df_train[\"numerical_column\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ce7fc-350e-4624-949a-60d1386bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics of all numerical columns\n",
    "df_train[numerical_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c4705-ccfa-4b08-9789-9c5319d71e52",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Visualize Distributions</strong> <br> \n",
    "    üìå Histogram matrix that shows the distributions of all numerical columns. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fdf247-c1cb-4dd5-9fb2-1d55172ea758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "\n",
    "\n",
    "# Function to visualize distributions of all numerical columns using a histogram matrix\n",
    "def plot_numerical_distributions(df, numerical_columns, safe_to_file=False):\n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(numerical_columns)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Create subplot grid with figure size based on 4x3 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "    \n",
    "    # Flatten the axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over all numerical columns\n",
    "    for i, column in enumerate(numerical_columns):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create histogram for the current column\n",
    "        sns.histplot(data=df, x=column, ax=ax)\n",
    "        \n",
    "        # Customize histogram title, axes labels and tick labels\n",
    "        ax.set_title(column.title().replace(\"_\", \" \"), fontsize=14)\n",
    "        ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.tick_params(axis=\"both\", labelsize=10)\n",
    "\n",
    "    # Hide any unused subplots \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)  \n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Numerical distributions plot (histogram matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving numerical distributions plot (histogram matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving numerical distributions plot (histogram matrix) to file: '{image_path}' already exists.\")\n",
    "            \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize distributions of all numerical columns in the training data\n",
    "plot_numerical_distributions(df_train, numerical_columns, safe_to_file=\"numerical_distributions_histograms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357efeb-5ad1-4e15-875c-fce5e3a1b470",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Customize individual histograms for better interpretability. <br><br>  \n",
    "    üí° Example: Format x-axis in thousands (K) without decimals. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcf592-a21f-4155-8d27-b16cffd8cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Create histogram \n",
    "sns.histplot(df_train[\"numerical_column\"])\n",
    "\n",
    "# Customize title, axes labels and tick labels \n",
    "plt.title(\"numerical_column\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.tick_params(axis=\"both\", labelsize=10)  # apply fontsize 10 to tick labels of both axes\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x / 1000:.0f}K\"))   # Format x-axis tick labels in thousands (no decimals)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f9891-a8ab-4a10-9351-2be2e20a1e8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Categorical Columns</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Examine descriptive statistics (absolute and relative frequencies) and visualize the distributions (e.g., bar plots) of categorical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9fe5a-ecf0-4388-8a9d-88590fa48f89",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br> \n",
    "    üìå Examine frequencies of categorical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a462-d369-4340-97e6-51480f28e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Frequencies of a single categorical column ---\n",
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df_train[\"categorical_column\"].value_counts()\n",
    "relative_frequencies = df_train[\"categorical_column\"].value_counts(normalize=True) \n",
    "percent_frequencies = relative_frequencies.map(lambda x: f\"{x * 100:.2f}%\")  # formatted as percent with 2 decimals (string)\n",
    "\n",
    "# Show frequency table\n",
    "pd.concat([absolute_frequencies, percent_frequencies], axis=1, keys=[\"absolute_frequency\", \"percent_frequency\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9788d-1626-46a7-80dd-bbcaaf6d3ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Frequencies of all categorical columns ---\n",
    "# Function to create frequency tables for all categorical columns \n",
    "def calculate_frequencies(df, categorical_columns):\n",
    "    # Initialize dictionary to store all frequency tables \n",
    "    frequencies = {}\n",
    "\n",
    "    # Iterate over each categorical column\n",
    "    for column in categorical_columns:\n",
    "        # Calculate frequencies for current column\n",
    "        absolute_frequencies = df[column].value_counts()\n",
    "        relative_frequencies = df[column].value_counts(normalize=True) \n",
    "        percent_frequencies = relative_frequencies.map(lambda x: f\"{x * 100:.2f}%\")  # formatted as string with % sign\n",
    "\n",
    "        # Create frequency table\n",
    "        frequency_table = pd.concat([absolute_frequencies, relative_frequencies, percent_frequencies], axis=1).reset_index()\n",
    "        frequency_table.columns = [\"category\", \"absolute_frequency\", \"relative_frequency\", \"percent_frequency\"] \n",
    "        \n",
    "        # Add current frequency table to dictionary \n",
    "        frequencies[column] = frequency_table\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "\n",
    "# Use function to create frequency tables of all categorical and boolean columns in the training data\n",
    "frequencies = calculate_frequencies(df_train, categorical_columns + boolean_columns)\n",
    "\n",
    "# Display a single frequency table\n",
    "frequencies[\"categorical_column\"][[\"category\", \"absolute_frequency\", \"percent_frequency\"]]\n",
    "\n",
    "# Display all frequency tables\n",
    "for column, frequency_table in frequencies.items():\n",
    "    print(f\"{column.title().replace('_', ' ')} Frequencies:\")\n",
    "    display(frequency_table[[\"category\", \"absolute_frequency\", \"percent_frequency\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057fa22-fe17-4ea5-8558-cd8454ac77d6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Visualize Distributions</strong> <br> \n",
    "    üìå Bar plot matrix that shows the frequency distributions of all categorical columns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e4a00-f83a-438a-929c-72bba321fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "# Function to visualize categorical frequency distributions using a bar plot matrix \n",
    "def plot_categorical_distributions(df, categorical_columns, max_categories=5, safe_to_file=False):\n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(categorical_columns)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Create subplot grid with figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Flatten the axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over all categorical columns\n",
    "    for i, column in enumerate(categorical_columns):      \n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Calculate frequencies for the current column\n",
    "        column_frequencies = df[column].value_counts(normalize=True)  # False for absolute frequencies\n",
    "\n",
    "        # Retain inherent order of categories for ordinal columns\n",
    "        if column == \"ordinal_column\":\n",
    "            column_frequencies = column_frequencies.reindex([\"low\", \"medium\", \"high\"])  # define the explicit order of categories\n",
    "            \n",
    "        # Format plot title\n",
    "        plot_title = column.title().replace(\"_\", \" \")\n",
    "    \n",
    "        # Limit number of categories for better readability  \n",
    "        if len(column_frequencies) > max_categories:\n",
    "            column_frequencies = column_frequencies.head(max_categories)\n",
    "            plot_title += f\" (Top {max_categories})\"\n",
    "        \n",
    "        # Create bar plot for the current column\n",
    "        sns.barplot(x=column_frequencies.index, y=column_frequencies.values, ax=ax)\n",
    "        \n",
    "        # Add value labels\n",
    "        value_labels = [f\"{value * 100:.1f}%\" for value in column_frequencies.values]\n",
    "        ax.bar_label(ax.containers[0], labels=value_labels, padding=2, fontsize=10 if len(column_frequencies) <= 6 else 7)  \n",
    "        \n",
    "        # Customize title and axes labels \n",
    "        ax.set_title(plot_title, fontsize=14)\n",
    "        ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "        # Customize axes tick labels\n",
    "        xticks = range(len(column_frequencies.index))\n",
    "        xticklabels = [str(label).title().replace(\"_\", \" \") for label in column_frequencies.index]\n",
    "        if len(column_frequencies) >= 5:  # rotate x-tick labels if 5 or more categories \n",
    "            ax.set_xticks(xticks, labels=xticklabels, fontsize=11, rotation=45, ha=\"right\")\n",
    "        else:\n",
    "            ax.set_xticks(xticks, labels=xticklabels, fontsize=11)\n",
    "        ax.tick_params(axis=\"y\", labelsize=10)\n",
    "        ax.set_ylim(0, column_frequencies.max() * 1.1)  # set y-axis from 0 to 10% above maximum frequency\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))  # format y-axis as percent\n",
    "\n",
    "    # Hide any unused subplots \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Categorical distributions plot (bar plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving categorical distributions plot (bar plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving categorical distributions plot (bar plot matrix) to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize categorical frequencies of all boolean and categorical columns in training data\n",
    "plot_categorical_distributions(df_train, boolean_columns + categorical_columns, max_categories=8, safe_to_file=\"categorical_frequencies_barplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0be43f-5b60-4f40-af64-0e545aeeb513",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Customize individual bar plots for better interpretability. <br><br>  \n",
    "    üí° Example: Horizontal bar plot for high-cardinality column (large number of unique categories). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad25df1-89b1-455b-8c36-e19c85d25688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(6, 12))\n",
    "\n",
    "# Create horizontal bar plot\n",
    "ax = sns.barplot(x=frequencies[\"categorical_column\"][\"relative_frequency\"], \n",
    "                 y=frequencies[\"categorical_column\"][\"category\"].str.title().str.replace(\"_\", \" \"))\n",
    "\n",
    "# Add value labels inside the bars\n",
    "for i, value in enumerate(frequencies[\"categorical_column\"][\"relative_frequency\"]):\n",
    "    ax.text(value * 0.98,  # x position (slightly from right end)\n",
    "            i,    # y position\n",
    "            f\"{value:.1f}%\",  # text (frequency with 1 decimal place)\n",
    "            ha=\"right\",  # horizontal alignment\n",
    "            va=\"center\") # vertical alignment\n",
    "\n",
    "# Customize title, axes labels, and tick labels \n",
    "plt.title(\"categorical_column\", fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Frequency\", fontsize=12)\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=1))  # x-tick labels as percentages (1 decimal)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895c96c-daff-4873-852d-615627e17dcb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Bivariate EDA</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Analyze relationships between two columns using correlations and group-wise statistics and visualize relationships using scatter plots and bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539fa8d-ba5c-42e9-bf4c-7e178a87ff1c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical vs. Numerical</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Analyze relationships between two numerical columns using correlation coefficients and visualize relationships using scatter plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bed553-89be-4154-9775-d784adfc100f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Correlations</h4>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Correlation heatmap of all numerical and boolean columns. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947c5f9-8b59-46fa-aaae-a705aeb0e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot correlation heatmap \n",
    "def plot_correlation_heatmap(df, numerical_columns, safe_to_file=False):\n",
    "    # Create correlation matrix and round to 2 decimals\n",
    "    correlation_matrix = round(df[numerical_columns].corr(), 2) \n",
    "    \n",
    "    # Create upper triangle mask (k=1 excludes diagonal)\n",
    "    mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool) \n",
    "    \n",
    "    # Set upper triangle to NaN to avoid redundancy\n",
    "    correlation_matrix[mask] = np.nan\n",
    "\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(\n",
    "        correlation_matrix, \n",
    "        cmap=\"viridis\",  # Colorblind-friendly colormap (other options: \"cividis\", \"magma\", \"YlOrBr\", \"RdBu\") \n",
    "        annot=True,  # Show correlation values\n",
    "        fmt=\".2f\",  # Ensure uniform decimal formatting\n",
    "        linewidth=0.5  # Thin white lines between cells\n",
    "    )\n",
    "\n",
    "    # Customize title and axes tick labels\n",
    "    plt.title(\"Correlation Heatmap\", fontsize=14)\n",
    "    formatted_column_names = correlation_matrix.columns.str.title().str.replace(\"_\", \" \") \n",
    "    ax.set_xticklabels(formatted_column_names)\n",
    "    ax.set_yticklabels(formatted_column_names)\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save heatmap to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)  \n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:    \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Correlation heatmap saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving correlation heatmap: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving correlation heatmap: '{image_path}' already exists.\")\n",
    "        \n",
    "    # Show heatmap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to plot correlation heatmap of all numerical and boolean columns in training data\n",
    "plot_correlation_heatmap(df_train, numerical_columns + boolean_columns, safe_to_file=\"correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382adcc-1fdb-48b7-8f2a-6d27a559bbfc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Feature-target correlations by order of magnitude.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3e9d0-e31d-4d8a-a1be-66ab44bf5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-target correlations sorted by absolute values in descending order \n",
    "feature_target_correlations = df_train[numerical_columns + boolean_columns].corr()[\"numerical_target\"].sort_values(key=abs, ascending=False)\n",
    "feature_target_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbc9c5-2f2b-411f-afcd-ee50b30f8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize feature-target correlations using a bar plot\n",
    "def plot_feature_target_correlations(feature_target_correlations, y_min=-1, y_max=1, safe_to_file=False):\n",
    "    # Remove target variable self-correlation\n",
    "    feature_target_correlations = feature_target_correlations[1:]\n",
    "    \n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    ax = sns.barplot(x=feature_target_correlations.index, y=feature_target_correlations.values)  # for pandas Series\n",
    "\n",
    "    # Add horizontal line\n",
    "    ax.axhline(0, color=\"gray\", alpha=0.5, linewidth=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    ax.bar_label(ax.containers[0], fmt=\"%.2f\", padding=3, fontsize=10)\n",
    "    \n",
    "    # Customize title, axes labels and ticks\n",
    "    plt.title(\"Feature-Target Correlations\", fontsize=14)\n",
    "    plt.ylabel(\"Correlation\", fontsize=12)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.yticks(np.arange(y_min, y_max + 0.2, 0.2), fontsize=10)\n",
    "    formatted_xtick_labels = [label.title().replace(\"_\", \" \") for label in feature_target_correlations.index]\n",
    "    plt.xticks(ticks=range(len(formatted_xtick_labels)), labels=formatted_xtick_labels, rotation=45, ha=\"right\", fontsize=11)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Feature-target correlations bar plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving feature-target correlations bar plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving feature-target correlations bar plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Use function to plot feature-target correlations of training data\n",
    "plot_feature_target_correlations(feature_target_correlations, y_min=-0.5, y_max=0.5, safe_to_file=\"feature_target_correlations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54741433-1f70-4248-982e-318bba474e15",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Numerical-Numerical Relationships (Scatter Plot Matrix)</strong> <br>\n",
    "    üìå Visualize pairwise relationships between all numerical columns using a scatter plot matrix.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0316f71-9be2-4b48-b0b7-38b1bd53ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "# Function to visualize pairwise relationships between numerical columns using a scatter plot matrix\n",
    "def plot_numerical_relationships(df, numerical_columns, safe_to_file=False):\n",
    "    # Get all possible pairs of numerical columns\n",
    "    column_pairs = list(itertools.combinations(numerical_columns, 2))\n",
    "    \n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(column_pairs)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Create subplot grid with figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over each column pair\n",
    "    for i, (column_1, column_2) in enumerate(column_pairs):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        sns.scatterplot(data=df, x=column_1, y=column_2, ax=ax)\n",
    "        \n",
    "        # Customize title, axes labels, and axes tick labels\n",
    "        column_1_name = column_1.title().replace(\"_\", \" \")\n",
    "        column_2_name = column_2.title().replace(\"_\", \" \")\n",
    "        ax.set_title(f\"{column_1_name} vs. {column_2_name}\", fontsize=13)\n",
    "        ax.set_xlabel(column_1_name, fontsize=12)\n",
    "        ax.set_ylabel(column_2_name, fontsize=12)\n",
    "        ax.tick_params(axis=\"both\", labelsize=10)\n",
    "            \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Numerical relationships plot (scatter plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving numerical relationships plot (scatter plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving numerical relationships plot (scatter plot matrix) to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize relationships between all numerical columns in the training data\n",
    "plot_numerical_relationships(df_train, numerical_columns, safe_to_file=\"numerical_relationships_scatterplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ac8fa-6862-401a-a0be-5f59bdff803c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Customize individual scatter plots for better interpretability. <br><br>\n",
    "    üí° Example: Define the exact axes ticks for two columns representing years. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94309ab-686d-4390-bb41-1d43b728fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create scatter plot \n",
    "sns.scatterplot(data=df_train, x=\"numerical_column_1\", y=\"numerical_column_2\")\n",
    "\n",
    "# Customize title and axes labels\n",
    "plt.title(\"Relationship between numerical_column_1 and numerical_column_2\", fontsize=14)\n",
    "plt.xlabel(\"numerical_column_1\", fontsize=12)\n",
    "plt.ylabel(\"numerical_column_2\", fontsize=12)\n",
    "\n",
    "# Customize axes tick labels\n",
    "plt.xticks(range(0, 21, 1), fontsize=10)  # set x-axis ticks from 0 to 20 years in 1-year steps\n",
    "plt.yticks(range(5, 11, 1), fontsize=10)  # set y-axis ticks from 5 to 10 years in 1-year steps\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdbe81-6e4d-48b4-ae69-07336924ce94",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical vs. Categorical</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Analyze relationships between a numerical column and a categorical column using group-wise statistics (e.g., median or mean by category) and visualize relationships using bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c58bee-b56d-4f5c-9b3f-ccf5cd47203c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Group-Wise Statistics</h4>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Descriptive statistics of numerical columns grouped by a single categorical column. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca8415-b881-495b-8d09-2c6d354035e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group-wise statistics of all numerical columns by a single categorical column (focus on median, mean, and std for easier readability)\n",
    "stats_by_category = df_train[numerical_columns].groupby(df_train[\"categorical_column\"]).agg([\"median\", \"mean\", \"std\"]).transpose()\n",
    "stats_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5d314-45ef-4eee-9cfa-918a52081ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group-wise statistics of a single numerical column by a single categorical column \n",
    "stats_by_category = df_train[\"numerical_column\"].groupby(df_train[\"categorical_column\"]).describe()\n",
    "stats_by_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c168c-7350-4ea4-8248-593316e936e8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Effect Size</strong> <br>\n",
    "    üìå Quantify the magnitude of the difference between two groups using Cohen's d.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790655d2-8f58-468c-a2e9-7103734a998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Cohen's d\n",
    "def calculate_cohens_d(df, numerical_column, categorical_column, group_1, group_2):\n",
    "    x1 = df[df[categorical_column] == group_1][numerical_column]\n",
    "    x2 = df[df[categorical_column] == group_2][numerical_column]\n",
    "    \n",
    "    mean1, mean2 = np.mean(x1), np.mean(x2)\n",
    "    std1, std2 = np.std(x1, ddof=1), np.std(x2, ddof=1)  # Sample standard deviation using N‚àí1\n",
    "    n1, n2 = len(x1), len(x2)\n",
    "    \n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "    \n",
    "    return (mean1 - mean2) / pooled_std if pooled_std != 0 else 0\n",
    "\n",
    "\n",
    "# Use function to calculate Cohen's d for a single numerical column\n",
    "cohens_d_result = cohens_d(df_train, \"numerical_column\", \"categorical_column\", \"category_1\", \"category_2\")\n",
    "print(f\"Cohen's d: {cohens_d_result}\")\n",
    "\n",
    "# Use function to calculate Cohen's d for all numerical columns\n",
    "cohens_d_results = {column: calculate_cohens_d(df_train, column, \"categorical_column\", \"category_1\", \"category_2\") for column in numerical_columns}\n",
    "cohens_d_results = pd.DataFrame.from_dict(cohens_d_results, orient=\"index\", columns=[\"Cohen's d\"])\n",
    "cohens_d_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0cd54-3004-408b-a1b6-4874e034fd71",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Numerical-Categorical Relationships (Bar Plot Matrix)</strong> <br>\n",
    "    üìå Visualize pairwise relationships between all numerical columns and a single categorical column using a bar plot matrix.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e7c05-d27a-4604-88b5-4a053c8c48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize pairwise relationships between multiple numerical columns and a single categorical column using a bar plot matrix\n",
    "def plot_numerical_categorical_relationships(df, numerical_columns, categorical_column, estimator=np.median, safe_to_file=False):\n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(numerical_columns)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Create subplot grid with figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over all numerical columns\n",
    "    for i, numerical_column in enumerate(numerical_columns):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create bar plot \n",
    "        sns.barplot(data=df, x=categorical_column, y=numerical_column, estimator=estimator, errorbar=None, ax=ax)\n",
    "\n",
    "        # Add value labels \n",
    "        for bar in ax.patches:  # patches are the bars themselves\n",
    "            # Get the height of the bar (which is the value)\n",
    "            height = bar.get_height()\n",
    "            \n",
    "            # Format value labels            \n",
    "            value = f\"{height:.1f}\"\n",
    "                \n",
    "            # Get the x and y coordinates for the text label\n",
    "            x_pos = bar.get_x() + bar.get_width() / 2.\n",
    "            y_pos = height * 1.01\n",
    "\n",
    "            # Add the text label \n",
    "            ax.text(x_pos, y_pos, value, ha=\"center\", va=\"bottom\", fontsize=10) \n",
    "\n",
    "        # Extend the y-axis upper limit by 5% to make room for value labels\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        ax.set_ylim(y_min, y_max * 1.05)\n",
    "        \n",
    "        # Customize title and axes labels \n",
    "        numerical_column_name = numerical_column.title().replace(\"_\", \" \")\n",
    "        categorical_column_name = categorical_column.title().replace(\"_\", \" \")\n",
    "        ax.set_title(f\"{estimator.__name__.title()} {numerical_column_name} by {categorical_column_name}\", fontsize=13)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(numerical_column_name, fontsize=12)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Numerical-categorical relationships plot (bar plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving numerical-categorical relationships plot (bar plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving numerical-categorical relationships plot (bar plot matrix) to file: '{image_path}' already exists.\")\n",
    " \n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize relationships between all numerical columns and a single categorical column in the training data\n",
    "plot_numerical_categorical_relationships(df_train, numerical_columns, \"categorical_column\", safe_to_file=\"numerical_categorical_relationships_barplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6aa3d-ac29-44c0-92b1-117578f1bed5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Customize individual bar plots for better interpretability. <br><br>\n",
    "    üí° Example: Define exact order of categories and format y-axis in thousands. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76154aa8-528e-44d6-bdde-a00f53174340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Define exact order of categories\n",
    "ordered_categories = [\"category_1\", \"category_2\", \"category_3\"]\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Create bar plot \n",
    "sns.barplot(data=df_train, x=\"categorical_column\", order=ordered_categories, y=\"numerical_column\", estimator=np.median, errorbar=None, ax=ax)\n",
    "\n",
    "# Add value labels \n",
    "for bar in ax.patches:  \n",
    "    height = bar.get_height()\n",
    "    value = f\"{height:,.0f}K\"  # format in thousands with thousand separator (no decimals)\n",
    "    x_pos = bar.get_x() + bar.get_width() / 2.\n",
    "    y_pos = height * 1.01\n",
    "    ax.text(x_pos, y_pos, value, ha=\"center\", va=\"bottom\", fontsize=10) \n",
    "\n",
    "# Extend the y-axis upper limit by 5% \n",
    "y_min, y_max = ax.get_ylim()\n",
    "ax.set_ylim(y_min, y_max * 1.05)\n",
    "\n",
    "# Customize title and axes labels\n",
    "numerical_column_name = \"numerical_column\".title().replace(\"_\", \" \")\n",
    "categorical_column_name = \"categorical_column\".title().replace(\"_\", \" \")\n",
    "ax.set_title(f\"Median {numerical_column_name} by {categorical_column_name}\", fontsize=14)\n",
    "ax.set_xlabel(categorical_column_name, fontsize=12)\n",
    "ax.set_ylabel(numerical_column_name, fontsize=12)\n",
    "\n",
    "# Customize axes tick labels\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x / 1000:.0f}K\"))  # format in thousands (no decimals)\n",
    "ax.tick_params(axis=\"y\", labelsize=10)\n",
    "ax.set_xticks(range(len(ordered_categories)))\n",
    "ax.set_xticklabels([label.title().replace(\"_\", \" \") for label in ordered_categories], fontsize=10) \n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9ea87-8c72-4975-b86c-e3747dad9d51",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Categorical vs. Categorical</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Analyze relationships between two categorical columns using contingency tables and visualize relationships using grouped bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af880d65-3f35-4493-8c25-eadd5a65217f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Contingency Tables</h4>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Contingency tables between all categorical feature-target pairs for machine learning projects with a categorical target variable.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a65e47-bf27-4bce-8f23-192e09146b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all categorical and boolean columns with number of unique categories\n",
    "df_train[categorical_columns + boolean_columns].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79050d49-2e56-41e5-8b97-ccd2e19c297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create contingency tables between each categorical feature and a single categorical target variable \n",
    "def calculate_feature_target_crosstabs(df, categorical_columns, target_variable, normalize=\"index\"):\n",
    "    # Dictionary to store contingency tables\n",
    "    contingency_tables = {}\n",
    "    \n",
    "    # Get all possible pairs of the target variable with the categorical features\n",
    "    feature_target_pairs = [(feature, target_variable) for feature in categorical_columns if feature != target_variable]\n",
    "    \n",
    "    # Iterate over each feature-target pair\n",
    "    for feature, target in feature_target_pairs:\n",
    "        # Create contingency table (normalize=\"index\" gives percentage distribution of the target variable within each category of the feature)\n",
    "        contingency_tables[(feature, target)] = pd.crosstab(df[feature], df[target], normalize=normalize)\n",
    "\n",
    "    return contingency_tables\n",
    "\n",
    "\n",
    "# Use function to create contingency tables between the target variable and each categorical or boolean feature in the training data\n",
    "contingency_tables = calculate_feature_target_crosstabs(df_train, categorical_columns + boolean_columns, target_variable=\"categorical_target\")\n",
    "\n",
    "# Display a single feature-target contingency table (formatted as percent with 1 decimal)\n",
    "display(contingency_tables[(\"categorical_feature\", \"categorical_target\")].map(lambda x: f\"{x * 100:.1f}\"))\n",
    "\n",
    "# Display all feature-target contingency tables \n",
    "for (feature, target) in contingency_tables.keys():\n",
    "    display(contingency_tables[(feature, target)].map(lambda x: f\"{x * 100:.1f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce73bd-9ca4-4767-ab6b-ade2eaf2f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create heatmap of a contingency table\n",
    "def plot_crosstab_heatmap(contingency_table, safe_to_file=False):\n",
    "    # Get number of categories\n",
    "    n_categories_feature = contingency_table.index.nunique()\n",
    "    n_categories_target = contingency_table.columns.nunique()\n",
    "    \n",
    "    # Set the figure size based on number of categories\n",
    "    if n_categories_feature > 4:\n",
    "        plt.figure(figsize=(n_categories_target * 1.5, n_categories_feature * 0.4))\n",
    "    else:\n",
    "        plt.figure(figsize=(n_categories_target * 2, n_categories_feature * 0.8))\n",
    "\n",
    "    # Create heatmap (formatted as percent with 1 decimal)\n",
    "    ax = sns.heatmap(contingency_table, annot=True, fmt=\".1%\", cmap=\"viridis\", cbar=False, linewidth=0.5)\n",
    "\n",
    "    # Customize title and axes labels\n",
    "    feature_name = contingency_table.index.name.title().replace(\"_\", \" \")\n",
    "    target_name = contingency_table.columns.name.title().replace(\"_\", \" \")\n",
    "    ax.set_title(f\"{target_name} Distribution Within {feature_name}\", fontsize=14)\n",
    "    ax.set_xlabel(target_name, fontsize=12) \n",
    "    ax.set_ylabel(feature_name, fontsize=12) \n",
    "\n",
    "    # Customize axes tick labels\n",
    "    formatted_xticklabels = [label.get_text().title().replace(\"_\", \" \") for label in ax.get_xticklabels()]\n",
    "    formatted_yticklabels = [label.get_text().title().replace(\"_\", \" \") for label in ax.get_yticklabels()]\n",
    "    ax.set_xticklabels(formatted_xticklabels, fontsize=10)\n",
    "    ax.set_yticklabels(formatted_yticklabels, rotation=0, fontsize=10)\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Contingency table heatmap saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving contingency table heatmap: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving contingency table heatmap to file: '{image_path}' already exists.\")\n",
    "\n",
    "    # Show heatmap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to create a contingency table heatmap of a single categorical feature with the categorical target variable  \n",
    "plot_crosstab_heatmap(contingency_tables[\"categorical_feature\", \"categorical_target\"])\n",
    "\n",
    "# Use function to create contingency table heatmaps of all categorical features with the categorical target variable \n",
    "for (feature, target) in contingency_tables.keys():\n",
    "    plot_crosstab_heatmap(contingency_tables[feature, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb85a9-8b63-4527-8c7a-d52a965301dd",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Customize individual contingency table heatmaps for better interpretability. <br><br>\n",
    "    üí° Example: Display only the top and bottom 5 feature categories ranked by target frequency (useful for high-cardinality features).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45caec30-a1e0-43d4-be76-d6640c515b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contingency table with percentage distribution of the target variable within each category of the feature \n",
    "contingency_table = pd.crosstab(df_train[\"categorical_feature\"], df_train[\"categorical_target\"], normalize=\"index\")\n",
    "\n",
    "# Sort feature categories from highest to lowest target frequency (class 1 for binary target)\n",
    "contingency_table = contingency_table.sort_values(by=1, ascending=False)\n",
    "\n",
    "# Filter feature categories with 5 highest and 5 lowest target frequencies\n",
    "contingency_table = pd.concat([contingency_table.head(5), contingency_table.tail(5)])\n",
    "\n",
    "# Create heatmap of contingency table \n",
    "plot_crosstab_heatmap(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f0aa9-d935-47cb-934c-7c42249fc423",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Visualize Relationships</h4>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Categorical-Categorical Relationships (Grouped Bar Plot Matrix)</strong> <br>\n",
    "    üìå Visualize pairwise relationships between all categorical columns with low cardinality using a grouped bar plot matrix.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90913520-5399-4842-9476-3a71b2db052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Function to visualize pairwise relationships between all categorical columns using a grouped bar plot matrix\n",
    "def plot_categorical_relationships(df, categorical_columns, max_categories=5, safe_to_file=False):\n",
    "    # Filter columns with low cardinality (small number of unique categories) \n",
    "    low_cardinality_columns = [column for column in categorical_columns if df[column].nunique() <= max_categories]\n",
    "\n",
    "    # Get all possible pairs of categorical columns\n",
    "    column_pairs = tuple(itertools.combinations(low_cardinality_columns, 2))\n",
    "    \n",
    "    # Calculate number of rows and columns for subplot grid\n",
    "    n_plots = len(column_pairs)\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(n_plots / n_cols) \n",
    "    \n",
    "    # Create subplot grid with figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Flatten the axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over each column pair\n",
    "    for i, (column_1, column_2) in enumerate(column_pairs):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Ensure column_1 (plot x-axis) has higher cardinality (more categories) for plot readability\n",
    "        if df[column_1].nunique() < df[column_2].nunique():\n",
    "            column_1, column_2 = column_2, column_1\n",
    "        \n",
    "        # Create contingency table (normalize=\"index\" calculates the percentage distribution of column_2 within each category of column_1)\n",
    "        contingency_table = pd.crosstab(df[column_1], df[column_2], normalize=\"index\") \n",
    "\n",
    "        # Reshape data for easier plotting  \n",
    "        plot_df = contingency_table.stack().reset_index()\n",
    "        plot_df.columns = [column_1, column_2, \"Frequency\"]\n",
    "         \n",
    "        # Create grouped bar plot\n",
    "        sns.barplot(data=plot_df, x=column_1, y=\"Frequency\", hue=column_2, palette=\"viridis\", ax=ax)\n",
    "\n",
    "        # Add value labels\n",
    "        n_categories_col2 = plot_df[column_2].nunique()\n",
    "        value_label_size = {1: 10, 2: 10, 3: 8}.get(n_categories_col2, 7)  # dynamic fontsize based on number of categories (default fontsize 7)\n",
    "        for container in ax.containers:\n",
    "            value_labels = [f\"{value * 100:.0f}%\" for value in container.datavalues]\n",
    "            ax.bar_label(container, labels=value_labels, padding=2, fontsize=value_label_size) \n",
    "                    \n",
    "        # Extend the y-axis upper limit by 5% \n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        ax.set_ylim(y_min, y_max * 1.05)\n",
    "        \n",
    "        # Customize title and axes labels\n",
    "        column_1_name = column_1.title().replace(\"_\", \" \")\n",
    "        column_2_name = column_2.title().replace(\"_\", \" \")\n",
    "        ax.set_title(f\"{column_1_name} vs. {column_2_name}\", fontsize=14)\n",
    "        ax.set_xlabel(column_1_name, fontsize=12)\n",
    "        ax.set_ylabel(f\"% within {column_1_name}\", fontsize=12)\n",
    "        \n",
    "        # Customize axes ticks\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))  # format y-axis tick labels as percentages\n",
    "        xticks = range(plot_df[column_1].nunique())\n",
    "        xticklabels = [label.get_text().title().replace(\"_\", \" \") for label in ax.get_xticklabels()]\n",
    "        ax.set_xticks(xticks, labels=xticklabels, fontsize=10)\n",
    "\n",
    "        # Customize legend\n",
    "        legend_handles, legend_labels = ax.get_legend_handles_labels()\n",
    "        legend_labels = [str(label).title().replace(\"_\", \" \") for label in legend_labels]  # format legend labels \n",
    "        ax.legend(handles=legend_handles, labels=legend_labels, title=column_2_name)    \n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "        \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Categorical relationships plot (grouped bar plot matrix) saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving categorical relationships plot (grouped bar plot matrix): {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving categorical relationships plot (grouped bar plot matrix) to file: '{image_path}' already exists.\")\n",
    "  \n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to visualize relationships between all categorical columns in the training data\n",
    "plot_categorical_relationships(df_train, categorical_columns + boolean_columns, safe_to_file=\"categorical_relationships_groupedbarplots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916000d9-45d4-4a14-9b31-777bd511320c",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Customize individual grouped bar plots for better interpretability. <br><br>\n",
    "    üí° Example: Display only the top and bottom 5 categories (useful for high-cardinality columns).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bbb83-5b85-4825-89f0-23994b13b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contingency table (normalize=\"index\" calculates the percentage distribution of column_2 within each category of column_1)\n",
    "contingency_table = pd.crosstab(df_train[\"categorical_column_1\"], df_train[\"categorical_column_2\"], normalize=\"index\") \n",
    "\n",
    "# Sort categories from highest to lowest (by class 1 of categorical_column_2)\n",
    "contingency_table = contingency_table.sort_values(by=1, ascending=False)\n",
    "\n",
    "# Filter 5 highest and 5 lowest categories\n",
    "contingency_table = pd.concat([contingency_table.head(5), contingency_table.tail(5)])\n",
    "\n",
    "# Reshape data for easier plotting  \n",
    "plot_df = contingency_table.stack().reset_index()\n",
    "plot_df.columns = [\"categorical_column_1\", \"categorical_column_2\", \"Frequency\"]\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create grouped bar plot\n",
    "sns.barplot(data=plot_df, x=\"categorical_column_1\", y=\"Frequency\", hue=\"categorical_column_2\", palette=\"viridis\", ax=ax)\n",
    "\n",
    "# Add value labels\n",
    "for container in ax.containers:\n",
    "    value_labels = [f\"{value * 100:.1f}%\" for value in container.datavalues]\n",
    "    ax.bar_label(container, labels=value_labels, padding=2, fontsize=10) \n",
    "            \n",
    "# Extend the y-axis upper limit by 5% \n",
    "y_min, y_max = ax.get_ylim()\n",
    "ax.set_ylim(y_min, y_max * 1.05)\n",
    "\n",
    "# Customize title and axes labels\n",
    "column_1_name = \"categorical_column_1\".title().replace(\"_\", \" \")\n",
    "column_2_name = \"categorical_column_2\".title().replace(\"_\", \" \")\n",
    "ax.set_title(f\"{column_1_name} vs. {column_2_name}\", fontsize=14)\n",
    "ax.set_xlabel(column_1_name, fontsize=12)\n",
    "ax.set_ylabel(f\"% Within {column_1_name}\", fontsize=12)\n",
    "\n",
    "# Customize axes ticks\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))  # format y-axis tick labels as percentages\n",
    "column_1_categories = contingency_table.index.tolist()  # Get the specific order of categories\n",
    "ax.set_xticks(range(len(column_1_categories)))\n",
    "ax.set_xticklabels([label.title().replace(\"_\", \" \") for label in column_1_categories], rotation=45, ha=\"right\") \n",
    "\n",
    "# Customize legend\n",
    "legend_handles, legend_labels = ax.get_legend_handles_labels()\n",
    "legend_labels = [str(label).title().replace(\"_\", \" \") for label in legend_labels]\n",
    "ax.legend(handles=legend_handles, labels=legend_labels, title=column_2_name)    \n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc85084-35ba-46e8-82aa-db1dacb9f2f5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px\">\n",
    "    <h1 style=\"margin:0px\">Modeling (Regression)</h1>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è For a regression problem, where the task is to predict a numerical target variable. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a506f-0b4a-4aad-bfac-3e68f65f3dea",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Helper Function for Residual Plots</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Creates two plots for predicted vs. actual target and residuals vs. actual target to evaluate model performance and identify potential issues in the model assumptions graphically. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb007a-2d89-431e-ab84-60337886ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create residual plots\n",
    "def plot_residuals(y, y_pred):\n",
    "    # Calculate residuals\n",
    "    residuals = [actual_value - predicted_value for actual_value, predicted_value in zip(y, y_pred)]\n",
    "\n",
    "    # Create a 1x2 grid of subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=150)\n",
    "\n",
    "    # Plot 1: Predicted vs. Actual Target\n",
    "    axes[0].scatter(y, y_pred)\n",
    "    axes[0].plot([min(y), max(y)], \n",
    "                 [min(y), max(y)], \n",
    "                 color=\"red\", \n",
    "                 linestyle=\"--\", \n",
    "                 label=\"Perfect Prediction\")  # Add diagonal reference line\n",
    "    axes[0].set_title(\"Predicted vs. Actual Target\")\n",
    "    axes[0].set_xlabel(\"Actual Target\")\n",
    "    axes[0].set_ylabel(\"Predicted Target\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].legend() \n",
    "\n",
    "    # Plot 2: Residuals vs. Actual Target\n",
    "    axes[1].scatter(y, residuals)\n",
    "    axes[1].axhline(y=0, color=\"red\", linestyle=\"--\", label=\"Perfect Prediction\")  # Add horizontal reference line\n",
    "    axes[1].set_xlabel(\"Actual Target\")\n",
    "    axes[1].set_ylabel(\"Residuals\")\n",
    "    axes[1].set_title(\"Residuals vs. Actual Target\")\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend() \n",
    "\n",
    "    # Adjust layout and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b415b78-03cc-41e8-8040-7abc98cb60e1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Training Baseline Models (Individually)  </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa28f5b-8e46-4813-9920-8cf9f79328e2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Linear Regression</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li><code>fit_intercept=True</code>: Calculates the intercept; can be set to <code>False</code> if data is already centered.</li>\n",
    "        <li><code>n_jobs=None</code>: Number of CPU threads; use <code>-1</code> for all available processors.</li>\n",
    "        <li><code>positive=False</code>: Forces regression coefficients to be non-negative if set to <code>True</code>.</li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\" target=\"_blank\">scikit-learn LinearRegression documentation</a>.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e64a3-d589-4f67-9fc3-04d0fc6b78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "lr.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = lr.predict(X_train_transformed)\n",
    "y_val_pred = lr.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "lr_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(lr_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ee293-544b-4346-a5a9-39592799ee8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Elastic Net Regression</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>alpha=1.0</code>: Regularization strength. Higher values increase the penalty, reducing overfitting but possibly underfitting the data.</li>\n",
    "                <li><code>l1_ratio=0.5</code>: The mix between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "                    <ul>\n",
    "                        <li><code>l1_ratio=1.0</code> corresponds to pure Lasso.</li>\n",
    "                        <li><code>l1_ratio=0.0</code> corresponds to pure Ridge.</li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Solver Configuration:\n",
    "            <ul>\n",
    "                <li><code>fit_intercept=True</code>: Whether to calculate the intercept for the model. If <code>False</code>, the model assumes data is centered.</li>\n",
    "                <li><code>precompute=False</code>: Whether to use precomputed Gram matrices to speed up calculations. Set to <code>True</code> for small datasets.</li>\n",
    "                <li><code>max_iter=1000</code>: The maximum number of iterations allowed for convergence during optimization.</li>\n",
    "                <li><code>tol=1e-4</code>: Stopping criterion for optimization. If the change in the cost function is smaller than <code>tol</code>, training stops.</li>\n",
    "                <li><code>warm_start=False</code>: If <code>True</code>, reuse the solution of the previous fit as initialization for the next fit.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>selection=\"cyclic\"</code>: Determines the strategy for updating coefficients. <code>\"cyclic\"</code> updates coefficients sequentially, <code>\"random\"</code> in a random order.</li>\n",
    "                <li><code>random_state=None</code>: Seed for random number generation when <code>selection=\"random\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Configuration:\n",
    "            <ul>\n",
    "                <li><code>copy_X=True</code>: Whether to copy the input data <code>X</code>. If <code>False</code>, training modifies the original data, saving memory.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\" target=\"_blank\">scikit-learn Elastic Net Regression documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb0792-6d99-441e-ba7c-f4e728163955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model \n",
    "en = ElasticNet()\n",
    "\n",
    "# Train model\n",
    "en.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = en.predict(X_train_transformed)\n",
    "y_val_pred = en.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "en_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(en_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad21cad-c8f5-4a04-82de-e30226ce3032",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">K-Nearest Neighbors Regressor</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_neighbors=5</code>: The number of neighbors to use for prediction. A higher value makes the model more general, while a lower value may lead to overfitting.</li>\n",
    "                <li><code>weights=\"uniform\"</code>: Determines how neighbors are weighted during prediction. <code>\"uniform\"</code> gives equal weight to all neighbors, while <code>\"distance\"</code> gives closer neighbors more influence.</li>\n",
    "                <li><code>p=2</code>: The power parameter for the Minkowski distance. <code>p=2</code> corresponds to the Euclidean distance, commonly used in KNN regression.</li>\n",
    "                <li><code>algorithm=\"auto\"</code>: The algorithm used to compute nearest neighbors. <code>\"auto\"</code> selects the best algorithm based on the dataset (options include <code>\"ball_tree\"</code>, <code>\"kd_tree\"</code>, and <code>\"brute\"</code>).</li>\n",
    "                <li><code>leaf_size=30</code>: The size of the leaf in tree-based algorithms like Ball Tree and KD Tree. This parameter impacts the speed and memory usage during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Distance Metrics:\n",
    "            <ul>\n",
    "                <li><code>metric=\"minkowski\"</code>: The distance metric used to calculate the proximity between data points. The default is <code>\"minkowski\"</code>, but you can also use <code>\"euclidean\"</code>, <code>\"manhattan\"</code>, etc.</li>\n",
    "                <li><code>metric_params=None</code>: Additional parameters for the distance metric, usually left as <code>None</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>n_jobs=None</code>: The number of parallel jobs to run for neighbor search. Setting <code>n_jobs=-1</code> utilizes all available CPU cores for faster computation.</li>\n",
    "                <li><code>radius=1.0</code>: Defines the search radius for neighbors. Instead of a fixed number of neighbors, this parameter considers neighbors within a given radius. This can be more flexible than <code>n_neighbors</code>, but it should be used with care as it may return an inconsistent number of neighbors.</li>\n",
    "                <li><code>max_iter=None</code>: The maximum number of iterations for the neighbor search process. This is set to <code>None</code> to allow unlimited iterations.</li>\n",
    "                <li><code>verbose=False</code>: Whether or not to print progress messages during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\" target=\"_blank\">scikit-learn KNN Regressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b097c1-1784-43d7-8155-03c16a4c8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Train model\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = knn.predict(X_train_transformed)\n",
    "y_val_pred = knn.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "knn_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(knn_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000a9c5-aa40-4871-927e-cd5bfb4a4290",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Support Vector Regressor</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>C=1.0</code>: Regularization parameter balancing error reduction and model complexity.</li>\n",
    "                <li><code>epsilon=0.1</code>: Margin of tolerance for predictions without penalty.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Kernel Configuration:\n",
    "            <ul>\n",
    "                <li><code>kernel=\"rbf\"</code>: Kernel function for mapping data to higher dimensions (default is radial basis function or <code>\"rbf\"</code>).</li>\n",
    "                <li><code>degree=3</code>: Degree of the polynomial kernel function (ignored by the rbf kernel).</li>\n",
    "                <li><code>gamma=\"scale\"</code>: Influence range of a single training example. <code>\"scale\"</code> means <code>1 / (n_features * X.var())</code>.</li>\n",
    "                <li><code>coef0=0.0</code>: Independent term in polynomial and sigmoid kernel function (ignored by the rbf kernel).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>tol=1e-3</code>: Stopping criterion for optimization. If the change in the cost function is less than this tolerance, training stops.</li>\n",
    "                <li><code>cache_size=200</code>: Memory (MB) allocated for kernel computation. Larger values speed up training.</li>\n",
    "                <li><code>shrinking=True</code>: Enables the shrinking heuristic, which can speed up training by eliminating unnecessary steps during optimization.</li>\n",
    "                <li><code>verbose=False</code>: Whether to print progress messages during training.</li>\n",
    "                <li><code>max_iter=-1</code>: Maximum number of iterations during training (<code>-1</code> for no limit).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\" target=\"_blank\">scikit-learn SVR documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5ef70-af0b-4cc5-8892-21df67f8ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "svr = SVR()\n",
    "\n",
    "# Train model\n",
    "svr.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = svr.predict(X_train_transformed)\n",
    "y_val_pred = svr.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "svr_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(svr_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0d369-7b24-4dc9-b452-b372a822d0be",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Decision Tree Regressor</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>max_depth=None</code>: Maximum depth of the tree. <code>None</code> allows nodes to expand until all leaves are pure or contain fewer samples than <code>min_samples_split</code>.</li>\n",
    "                <li><code>min_samples_split=2</code>: Minimum number of samples required to split an internal node.</li>\n",
    "                <li><code>min_samples_leaf=1</code>: Minimum number of samples required to be at a leaf node.</li>\n",
    "                <li><code>criterion=\"squared_error\"</code>: Function to measure the quality of a split. Options include <code>\"squared_error\"</code> (mean squared error) and <code>\"friedman_mse\"</code> (Friedman‚Äôs mean squared error).</li>\n",
    "                <li><code>splitter=\"best\"</code>: Strategy to choose the split at each node. Options are <code>\"best\"</code> (best split) and <code>\"random\"</code> (random split).</li>\n",
    "                <li><code>max_features=None</code>: Number of features to consider when looking for the best split. If <code>None</code>, all features are considered.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization:\n",
    "            <ul>\n",
    "                <li><code>ccp_alpha=0.0</code>: Complexity parameter for pruning. A higher value encourages pruning by penalizing tree complexity.</li>\n",
    "                <li><code>min_impurity_decrease=0.0</code>: A node will split only if the impurity decrease exceeds this threshold.</li>\n",
    "                <li><code>max_leaf_nodes=None</code>: Maximum number of leaf nodes in the tree.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>min_weight_fraction_leaf=0.0</code>: Minimum weighted fraction of the sum of weights required at a leaf node.</li>\n",
    "                <li><code>max_samples=None</code>: (Only relevant for certain ensemble methods; ignored in standalone <code>DecisionTreeRegressor</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>presort=\"deprecated\"</code>: Pre-sorting data for faster splits has been deprecated in recent versions.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\" target=\"_blank\">scikit-learn DecisionTreeRegressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786f108-9a6c-4428-b390-872f81a89feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train model\n",
    "tree.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = tree.predict(X_train_transformed)\n",
    "y_val_pred = tree.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "tree_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(tree_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2e272-3a4e-40c0-9d7d-c45ce19b43dc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Random Forest Regressor</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_estimators=100</code>: Number of trees in the forest.</li>\n",
    "                <li><code>max_depth=None</code>: Maximum depth of each tree; <code>None</code> allows trees to grow until all leaves are pure or minimum samples are reached.</li>\n",
    "                <li><code>min_samples_split=2</code>: Minimum number of samples required to split a node.</li>\n",
    "                <li><code>min_samples_leaf=1</code>: Minimum number of samples required at a leaf node.</li>\n",
    "                <li><code>max_features=\"auto\"</code>: Number of features considered for the best split; default <code>auto</code> uses the square root of all features.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization:\n",
    "            <ul>\n",
    "                <li><code>max_leaf_nodes=None</code>: Maximum number of leaf nodes per tree.</li>\n",
    "                <li><code>min_impurity_decrease=0.0</code>: Splits a node only if it decreases impurity by this threshold.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>bootstrap=True</code>: Whether to use bootstrap samples for training each tree.</li>\n",
    "                <li><code>oob_score=False</code>: Whether to use out-of-bag samples to estimate prediction accuracy.</li>\n",
    "                <li><code>n_jobs=None</code>: Number of CPU threads used (<code>-1</code> for all processors).</li>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>verbose=0</code>: Controls the verbosity of output during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>max_samples=None</code>: Maximum number of samples used to train each tree, useful for subsampling large datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\" target=\"_blank\">scikit-learn RandomForestRegressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106850a-59f4-4aad-aecb-1b0097c94381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = rf.predict(X_train_transformed)\n",
    "y_val_pred = rf.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "rf_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(rf_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab373eef-06bf-498b-b9f2-518db59a9522",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Multi-Layer Perceptron Regressor</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Architecture:\n",
    "            <ul>\n",
    "                <li><code>hidden_layer_sizes=(100,)</code>: Defines the size and number of hidden layers; <code>(100,)</code> indicates one layer with 100 neurons.</li>\n",
    "                <li><code>activation=\"relu\"</code>: Activation function for the hidden layers; options include <code>\"relu\"</code>, <code>\"tanh\"</code>, <code>\"logistic\"</code>, or <code>\"identity\"</code>.</li>\n",
    "                <li><code>solver=\"adam\"</code>: Optimization algorithm; options are <code>\"adam\"</code> (default), <code>\"lbfgs\"</code>, or <code>\"sgd\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization and Learning:\n",
    "            <ul>\n",
    "                <li><code>alpha=0.0001</code>: L2 regularization term to prevent overfitting.</li>\n",
    "                <li><code>learning_rate=\"constant\"</code>: Strategy for learning rate adjustment; options are <code>\"constant\"</code>, <code>\"invscaling\"</code>, or <code>\"adaptive\"</code>.</li>\n",
    "                <li><code>learning_rate_init=0.001</code>: Initial learning rate for weight updates.</li>\n",
    "                <li><code>power_t=0.5</code>: Exponent for inverse scaling of learning rate (used when <code>learning_rate=\"invscaling\"</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>max_iter=200</code>: Maximum number of iterations for training.</li>\n",
    "                <li><code>tol=1e-4</code>: Tolerance for stopping criteria; training stops if loss improvement is below this value.</li>\n",
    "                <li><code>momentum=0.9</code>: Momentum parameter for gradient descent updates (used when <code>solver=\"sgd\"</code>).</li>\n",
    "                <li><code>n_iter_no_change=10</code>: Number of iterations with no improvement to stop early.</li>\n",
    "                <li><code>early_stopping=False</code>: Enables early stopping when validation score doesn‚Äôt improve.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>batch_size=\"auto\"</code>: Number of samples per batch for training; <code>\"auto\"</code> uses <code>min(200, n_samples)</code>.</li>\n",
    "                <li><code>shuffle=True</code>: Whether to shuffle training data before each epoch.</li>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>verbose=False</code>: Controls verbosity of output during training.</li>\n",
    "                <li><code>warm_start=False</code>: Reuses previous solution to initialize weights for additional fitting.</li>\n",
    "                <li><code>beta_1=0.9</code>, <code>beta_2=0.999</code>: Exponential decay rates for moving averages of gradients and squared gradients (used in <code>solver=\"adam\"</code>).</li>\n",
    "                <li><code>epsilon=1e-8</code>: Small value to prevent division by zero in <code>solver=\"adam\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\" target=\"_blank\">scikit-learn MLPRegressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a704b2-b16d-4ad6-84f2-5e0014394140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "# Train model\n",
    "mlp.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = mlp.predict(X_train_transformed)\n",
    "y_val_pred = mlp.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "mlp_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(mlp_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c8bbc-f07a-41e9-a29b-2f410b518c90",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">XGBoost Regressor</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_estimators=100</code>: Number of trees.</li>\n",
    "                <li><code>max_depth=6</code>: Maximum depth of each tree.</li>\n",
    "                <li><code>learning_rate=0.3</code>: Step size shrinking to prevent overfitting.</li>\n",
    "                <li><code>subsample=1.0</code>: Fraction of training samples used for each tree.</li>\n",
    "                <li><code>colsample_bytree=1.0</code>: Fraction of features used for each tree.</li>\n",
    "                <li><code>colsample_bylevel=1.0</code>: Fraction of features used at each tree level.</li>\n",
    "                <li><code>colsample_bynode=1.0</code>: Fraction of features used at each node.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization and Learning:\n",
    "            <ul>\n",
    "                <li><code>gamma=0</code>: Minimum loss reduction required to make a further partition on a leaf node.</li>\n",
    "                <li><code>min_child_weight=1</code>: Minimum sum of instance weight (hessian) in a child.</li>\n",
    "                <li><code>scale_pos_weight=1</code>: Controls the balance of positive and negative weights; used for imbalanced datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>objective=\"reg:squarederror\"</code>: Objective function for regression; default is for squared error.</li>\n",
    "                <li><code>booster=\"gbtree\"</code>: Booster type; options include <code>\"gbtree\"</code> (default), <code>\"gblinear\"</code>, and <code>\"dart\"</code>.</li>\n",
    "                <li><code>tree_method=\"auto\"</code>: Tree construction algorithm; <code>\"auto\"</code> chooses based on system configuration. Options include <code>\"exact\"</code>, <code>\"approx\"</code>, and <code>\"hist\"</code>.</li>\n",
    "                <li><code>eval_metric=\"rmse\"</code>: Metric used for validation during training; default is root mean square error (<code>rmse</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>early_stopping_rounds=None</code>: Stops training if validation metric does not improve after specified rounds.</li>\n",
    "                <li><code>n_jobs=1</code>: Number of threads used for parallel computation (<code>-1</code> for all processors).</li>\n",
    "                <li><code>random_state=None</code>: Seed for reproducibility.</li>\n",
    "                <li><code>verbose=1</code>: Verbosity level for training output; <code>0</code> for silent, higher values show more details.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Advanced Parameters:\n",
    "            <ul>\n",
    "                <li><code>lambda=1</code>: L2 regularization term on weights.</li>\n",
    "                <li><code>alpha=0</code>: L1 regularization term on weights.</li>\n",
    "                <li><code>max_delta_step=0</code>: Used to help with convergence in highly imbalanced datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fea17-acb8-4e88-8c1a-ce1d3f37758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Train model\n",
    "xgb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = xgb.predict(X_train_transformed)\n",
    "y_val_pred = xgb.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "xgb_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(xgb_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b85e39-0770-44c4-b67c-85896b0c82ee",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Training Baseline Models (Pipeline)</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Train 8 baseline models:  \n",
    "    <ul>\n",
    "        <li>Linear Regression</li>\n",
    "        <li>Elastic Net Regression</li>\n",
    "        <li>K-Nearest Neighbors Regressor</li>\n",
    "        <li>Support Vector Regressor</li>\n",
    "        <li>Decision Tree Regressor</li>\n",
    "        <li>Random Forest Regressor</li>\n",
    "        <li>Multi-Layer Perceptron Regressor</li>\n",
    "        <li>XGBoost Regressor</li>\n",
    "    </ul>\n",
    "    üéØ Model performance will be evaluated using the following metrics:  \n",
    "    <ul>\n",
    "        <li>Root Mean Squared Error (RMSE)</li>\n",
    "        <li>Mean Absolute Percentage Error (MAPE)</li>\n",
    "        <li>R-squared (R2)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca118a-6a69-4b27-adbc-4bdc77027ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with baseline configurations\n",
    "models = [\n",
    "    LinearRegression(), \n",
    "    ElasticNet(),\n",
    "    KNeighborsRegressor(),\n",
    "    SVR(), \n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    RandomForestRegressor(random_state=42), \n",
    "    MLPRegressor(random_state=42), \n",
    "    XGBRegressor(random_state=42)\n",
    "]\n",
    "\n",
    "# Create lists for storing the evaluation metrics (RMSE, MAPE, R2) of each model \n",
    "rmse_ls = []\n",
    "mape_ls = []\n",
    "r2_ls = []\n",
    "\n",
    "# Loop through each model\n",
    "for model in models:\n",
    "    # Show model\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Model: {model}\")\n",
    "\n",
    "    # Scale numerical columns and encode categorical columns \n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"scaler\", StandardScaler(), numerical_columns),  # Use MinMaxScaler() for min-max normalization\n",
    "            (\"nominal_encoder\", OneHotEncoder(drop=\"first\"), nominal_columns),\n",
    "            (\"ordinal_encoder\", OrdinalEncoder(categories=ordinal_column_orders), ordinal_columns)  \n",
    "        ],\n",
    "        remainder=\"passthrough\"  # Include the boolean columns without transformation\n",
    "    )\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"column_transformer\", column_transformer),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "    # Calculate evaluation metrics: RMSE, MAPE, R2\n",
    "    rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    # Show evaluation metrics\n",
    "    print(f\"RMSE: {round(rmse, 2)}\")\n",
    "    print(f\"MAPE: {round(mape, 2)}\")\n",
    "    print(f\"R-squared (R¬≤): {round(r2, 2)}\")\n",
    "\n",
    "    # Add evaluation metrics to their respective lists\n",
    "    rmse_ls.append(rmse)\n",
    "    mape_ls.append(mape)\n",
    "    r2_ls.append(r2)\n",
    "\n",
    "    # Create residual plots\n",
    "    plot_residuals(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caccfbd5-1687-4cd8-8b6a-df2e146c94d3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Hyperparameter Tuning</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Based on baseline model training, select the best performing models for hyperparameter tuning. <br><br> \n",
    "    üí° Example: The following models outperformed the other candidates across evaluation metrics (RMSE, MAPE, R-squared) on the validation data and were selected for hyperparameter tuning:  \n",
    "    <ul>\n",
    "        <li><b>Model 1: Random Forest Regressor</b>  \n",
    "            <br><i>Justification:</i> Delivered low RMSE and MAPE scores during baseline evaluation, showing strong predictive performance with minimal error.</li>\n",
    "        <li><b>Model 2: XGBoost Regressor</b>  \n",
    "            <br><i>Justification:</i> Achieved high R-squared and consistently low error metrics, indicating its ability to capture underlying patterns in the data effectively.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5167a-77c1-4572-b26e-343a60ac01e9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Grid Search</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">  \n",
    "    ‚ÑπÔ∏è Hyperparameters:  \n",
    "    <ul>  \n",
    "        <li>Core Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>estimator</code>: The machine learning model you want to optimize (e.g., <code>RandomForestRegressor</code>).</li>  \n",
    "                <li><code>param_grid</code>: A dictionary where keys are hyperparameter names, and values are lists of possible values to try.</li>  \n",
    "                <li><code>cv=5</code>: Number of folds for cross-validation.</li>  \n",
    "                <li><code>scoring=None</code>: The metric to evaluate model performance (e.g., <code>\"neg_mean_squared_error\"</code>, <code>\"neg_mean_absolute_error\"</code>, <code>\"r2\"</code>).</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "        <li>Optional Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>verbose=0</code>: Controls the verbosity of the output. Higher values provide more detailed logs.</li>  \n",
    "                <li><code>n_jobs=None</code>: Number of jobs to run in parallel. <code>-1</code> uses all available CPU cores.</li>  \n",
    "                <li><code>pre_dispatch=\"2*n_jobs\"</code>: Controls the number of jobs that get dispatched during parallel execution.</li>  \n",
    "                <li><code>refit=True</code>: If <code>True</code>, the estimator is refit on the entire training set using the best parameters.</li>  \n",
    "                <li><code>random_state=None</code>: Ensures reproducibility of results when estimators or scoring functions involve randomness.</li>  \n",
    "                <li><code>error_score=np.nan</code>: Value to assign to the score if a parameter combination results in a failure.</li>  \n",
    "                <li><code>return_train_score=False</code>: If <code>True</code>, training scores will be returned along with validation scores.</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "    </ul>  \n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0291e-5f7a-43c4-a1d3-b409fa056362",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 1</h4>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <p>üí° Example: Random Forest Regressor</p>\n",
    "    <p>The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code></li>\n",
    "        <li><code>max_depth</code></li>\n",
    "        <li><code>min_samples_split</code></li>\n",
    "        <li><code>min_samples_leaf</code></li>\n",
    "        <li><code>max_features</code></li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\" target=\"_blank\">scikit-learn RandomForestRegressor documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785cd9ad-ec73-41db-81fe-b2b6f4793297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500],               \n",
    "    \"max_depth\": [None, 10, 20],              \n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [0.33, 0.66, 1]                \n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf, \n",
    "    param_grid=rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"neg_root_mean_squared_error\"  # use \"r2\" for R-squared or a custom function for MAPE\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "rf_grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601be53-7d15-48ca-9321-6915b90511e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of grid search results \n",
    "rf_grid_search_results = pd.DataFrame({\n",
    "    \"validation_rmse\": -1 * rf_grid_search.cv_results_[\"mean_test_score\"],  # RMSE on validation data\n",
    "    \"parameters\": rf_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "rf_grid_search_results[\"n_estimators\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "rf_grid_search_results[\"max_depth\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "rf_grid_search_results[\"min_samples_split\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_split\"])\n",
    "rf_grid_search_results[\"min_samples_leaf\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_leaf\"])\n",
    "rf_grid_search_results[\"max_features\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_features\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_grid_search_results = rf_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models\n",
    "print(rf_grid_search_results.sort_values(\"validation_rmse\").head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7a298-1e53-4efa-80b1-5152884a3131",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 2</h4>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <p>üí° Example: XGBoost Regressor</p>\n",
    "    <p>The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code></li>\n",
    "        <li><code>max_depth</code></li>\n",
    "        <li><code>learning_rate</code></li>\n",
    "        <li><code>subsample</code></li>\n",
    "        <li><code>colsample_bytree</code></li>\n",
    "        <li><code>gamma</code></li>\n",
    "        <li><code>min_child_weight</code></li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f58f8-0c05-4a19-a851-ed03d87bc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],               \n",
    "    \"max_depth\": [3, 6, 10], \n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],  \n",
    "    \"gamma\": [0, 0.1, 0.2],           \n",
    "    \"min_child_weight\": [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=xgb_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"neg_root_mean_squared_error\"  # use \"r2\" for R-squared or a custom function for MAPE\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "xgb_grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358dfa5-5366-495d-89fe-f76611299e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of grid search results \n",
    "xgb_grid_search_results = pd.DataFrame({\n",
    "    \"validation_rmse\": -1 * xgb_grid_search.cv_results_[\"mean_test_score\"],  # RMSE on validation data\n",
    "    \"parameters\": xgb_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "xgb_grid_search_results[\"n_estimators\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "xgb_grid_search_results[\"max_depth\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "xgb_grid_search_results[\"learning_rate\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"learning_rate\"])\n",
    "xgb_grid_search_results[\"subsample\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"subsample\"])\n",
    "xgb_grid_search_results[\"colsample_bytree\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"colsample_bytree\"])\n",
    "xgb_grid_search_results[\"gamma\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"gamma\"])\n",
    "xgb_grid_search_results[\"min_child_weight\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_child_weight\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_grid_search_results = xgb_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models\n",
    "print(xgb_grid_search_results.sort_values(\"validation_rmse\").head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c22068-57b1-4207-b8b2-2e414ed051ef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Randomized Search</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">  \n",
    "    ‚ÑπÔ∏è Hyperparameters:  \n",
    "    <ul>  \n",
    "        <li>Core Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>estimator</code>: The machine learning model you want to optimize (e.g., <code>RandomForestRegressor</code>).</li>  \n",
    "                <li><code>param_distributions</code>: A dictionary where keys are hyperparameter names, and values are the distributions or lists of possible values.</li>  \n",
    "                <li><code>n_iter=10</code>: The number of random combinations to sample from the parameter grid.</li>  \n",
    "                <li><code>cv=5</code>: Number of folds for cross-validation.</li>  \n",
    "                <li><code>scoring=None</code>: The metric to evaluate model performance (e.g., <code>\"neg_mean_squared_error\"</code>, <code>\"neg_mean_absolute_error\"</code>, <code>\"r2\"</code>).</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "        <li>Optional Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>verbose=0</code>: Controls the verbosity of the output. Higher values provide more detailed logs.</li>  \n",
    "                <li><code>random_state=None</code>: Ensures reproducibility of results.</li>  \n",
    "                <li><code>n_jobs=None</code>: Number of jobs to run in parallel. <code>-1</code> uses all available CPU cores.</li>  \n",
    "                <li><code>pre_dispatch=\"2*n_jobs\"</code>: Controls the number of jobs that get dispatched during parallel execution.</li>  \n",
    "                <li><code>refit=True</code>: If <code>True</code>, the estimator is refit on the entire training set using the best parameters.</li>  \n",
    "                <li><code>error_score=np.nan</code>: Value to assign to the score if a parameter combination results in a failure.</li>  \n",
    "                <li><code>return_train_score=False</code>: If <code>True</code>, training scores will be returned along with validation scores.</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "    </ul>  \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be264892-bc77-4101-ad36-adf0ef4c525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy to define the hyperparameter distributions (randint for int, uniform for float)\n",
    "from scipy.stats import randint, uniform  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2d6e8-48d4-4e73-a0e2-578d5c38a8cd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 1</h4>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <p>üí° Example: Random Forest Regressor</p>\n",
    "    <p>The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code></li>\n",
    "        <li><code>max_depth</code></li>\n",
    "        <li><code>min_samples_split</code></li>\n",
    "        <li><code>min_samples_leaf</code></li>\n",
    "        <li><code>max_features</code></li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\" target=\"_blank\">scikit-learn RandomForestRegressor documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b5b8b-5c1c-4d08-9c52-955f165ba8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "rf_param_distributions = {\n",
    "    \"n_estimators\": randint(50, 1000),               \n",
    "    \"max_depth\": randint(5, 50),              \n",
    "    \"min_samples_split\": randint(2, 20),\n",
    "    \"min_samples_leaf\": randint(1, 10),\n",
    "    \"max_features\": uniform(0.1, 0.9)                \n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=rf_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"neg_root_mean_squared_error\",  # use \"r2\" for R-squared or a custom function for MAPE\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "rf_random_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78774557-07ff-40dd-8899-5e5cfdeb0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of randomized search results\n",
    "rf_random_search_results = pd.DataFrame({\n",
    "    \"validation_rmse\": -1 * rf_random_search.cv_results_[\"mean_test_score\"],  # RMSE on validation data\n",
    "    \"parameters\": rf_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "rf_random_search_results[\"n_estimators\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "rf_random_search_results[\"max_depth\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "rf_random_search_results[\"min_samples_split\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_split\"])\n",
    "rf_random_search_results[\"min_samples_leaf\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_leaf\"])\n",
    "rf_random_search_results[\"max_features\"] = rf_random_search_results[\"parameters\"].apply(lambda x: x[\"max_features\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_random_search_results = rf_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models \n",
    "print(rf_random_search_results.sort_values(\"validation_rmse\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37163d1-945e-4a2b-a26d-8584862bd9bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 2</h4>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <p>üí° Example: XGBoost Regressor</p>\n",
    "    <p>The following hyperparameters are typically the most impactful:</p>\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code></li>\n",
    "        <li><code>max_depth</code></li>\n",
    "        <li><code>learning_rate</code></li>\n",
    "        <li><code>subsample</code></li>\n",
    "        <li><code>colsample_bytree</code></li>\n",
    "        <li><code>gamma</code></li>\n",
    "        <li><code>min_child_weight</code></li>\n",
    "    </ul>\n",
    "    <p>For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbbfb4-1435-430d-bce7-421044c7a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "xgb_param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700],  \n",
    "    'max_depth': randint(3, 11),                \n",
    "    'learning_rate': uniform(0.01, 0.4),       \n",
    "    'subsample': uniform(0.5, 0.5),             \n",
    "    'colsample_bytree': uniform(0.5, 0.5),      \n",
    "    'gamma': uniform(0, 0.5),                     \n",
    "    'min_child_weight': [1, 3, 5, 7],           \n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=xgb_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"neg_root_mean_squared_error\",  # use \"r2\" for R-squared or a custom function for MAPE\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "xgb_random_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01728c4-d5d3-4d2d-adf3-3460c72b7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of randomized search results\n",
    "xgb_random_search_results = pd.DataFrame({\n",
    "    \"validation_rmse\": -1 * xgb_random_search.cv_results_[\"mean_test_score\"],  # RMSE on validation data\n",
    "    \"parameters\": xgb_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "xgb_random_search_results[\"n_estimators\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "xgb_random_search_results[\"max_depth\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "xgb_random_search_results[\"learning_rate\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"learning_rate\"])\n",
    "xgb_random_search_results[\"subsample\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"subsample\"])\n",
    "xgb_random_search_results[\"colsample_bytree\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"colsample_bytree\"])\n",
    "xgb_random_search_results[\"gamma\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"gamma\"])\n",
    "xgb_random_search_results[\"min_child_weight\"] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[\"min_child_weight\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_random_search_results = xgb_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models \n",
    "print(xgb_random_search_results.sort_values(\"validation_rmse\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fcc93-aa1f-48a2-b11d-399ddf8807f8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Final Model</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Select the best model based on hyperparameter tuning. <br><br> \n",
    "    üí° Example: The XGBoost Regressor model achieved the best performance (e.g., RMSE = 12.34) on the validation data compared to other candidates, making it the optimal choice for the final model. This model will be further evaluated on the test data to confirm its generalizability.  \n",
    "    <br><br>\n",
    "    Hyperparameter Values:  \n",
    "    <ul>\n",
    "        <li><code>n_estimators=300</code></li>\n",
    "        <li><code>max_depth=4</code></li>\n",
    "        <li><code>learning_rate=0.1</code></li>\n",
    "        <li><code>subsample=0.8</code></li>\n",
    "        <li><code>colsample_bytree=0.8</code></li>\n",
    "        <li><code>gamma=0.1</code></li>\n",
    "        <li><code>min_child_weight=3</code></li>\n",
    "        <li><code>random_state=42</code></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb70149-6fda-49df-a20b-10130e193888",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Training Final Model</h3>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50760a6-e090-4481-8eea-223a0c5d1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "final_model = XGBRegressor(\n",
    "    n_estimators=300, \n",
    "    max_depth=4, \n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=0.8, \n",
    "    gamma=0.1,\n",
    "    min_child_weight=3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "final_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5419d06-3048-4ae6-a9a0-f92eea024c99",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Model Evaluation</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Metrics</strong> <br> \n",
    "    üìå Evaluate model performance using metrics (RMSE, MAPE, R-squared) for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d565f-4c82-430f-89e3-cf03f326696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training, validation and test data\n",
    "y_train_pred = final_model.predict(X_train_transformed)\n",
    "y_val_pred = final_model.predict(X_val_transformed)\n",
    "y_test_pred = final_model.predict(X_test_transformed)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "final_model_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2],\n",
    "    \"Test\": [test_rmse, test_mape, test_r2]\n",
    "})\n",
    "\n",
    "# Show table\n",
    "print(final_model_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc71535-72da-4e7c-aef3-804f312466c9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Error Analysis: Residual Plots</strong> <br> \n",
    "    üìå Plot predicted vs. actual values and residuals vs. actual values for validation and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b15d30-b95b-4499-ac84-2124b938669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resiudal plots for validation data\n",
    "plot_residuals(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec2f02-1f58-4004-8a56-82e43e7e460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resiudal plots for tets data\n",
    "plot_residuals(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d08d64-d355-40aa-9ae2-fd982b3f2b5f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Error Analysis: Descriptive Statistics and Visualizations</strong> <br> \n",
    "    üìå Analyze errors on test data with descriptive statistics (mean, median) and visualize error distributions using histograms. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a7ae0-8f44-466c-b308-1d9e26ecc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_test, y_test, and y_test_pred into one DataFrame\n",
    "df_test = X_test.copy()  \n",
    "df_test[\"Actual\"] = y_test\n",
    "df_test[\"Predicted\"] = y_test_pred\n",
    "\n",
    "# Calculate errors \n",
    "df_test[\"Error\"] = df_test[\"Predicted\"] - df_test[\"Actual\"]\n",
    "df_test[\"Absolute Error\"] = np.abs(df_test[\"Actual\"] - df_test[\"Predicted\"])\n",
    "df_test[\"Error (%)\"] = (df_test[\"Actual\"] - df_test[\"Predicted\"]) / df_test[\"Actual\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7d3bf-16fc-4b55-9e78-a0c4e588e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and median error\n",
    "print(f\"Mean Absolute Error: {df_test['Absolute Error'].mean():.2f}\")\n",
    "print(f\"Median Absolute Error: {df_test['Absolute Error'].median():.2f}\")\n",
    "print(f\"Mean Error (%): {df_test['Error (%)'].mean():.2f}%\")\n",
    "print(f\"Median Error (%): {df_test['Error (%)'].median():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d8f2b-9c82-4599-9db0-3c33301923c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error histograms \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create subplot for absolute errors\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=df_test, x=\"Absolute Error\", bins=30)\n",
    "plt.title(\"Distribution of Absolute Errors\")\n",
    "plt.xlabel(\"Absolute Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create subplot for percentage errors\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data=df_test, x=\"Error (%)\", bins=30)\n",
    "plt.title(\"Distribution of Percentage Errors\")\n",
    "plt.xlabel(\"Error (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb21ce-254f-4208-82e9-0de1e2a61295",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Error Analysis: Feature Relationships with Errors</strong> <br> \n",
    "    üìå Explore relationships between the features and the errors through correlations and scatterplots. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7db79-f061-429d-80d0-5a8fd46b3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between features and absolute error\n",
    "# Note: Ensure numerical_columns and boolean_columns were defined in exploratory data analysis section \n",
    "df_test[numerical_columns + boolean_columns].corr()[\"Absolute Error\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a9479-8898-47be-9161-a8330f1ab5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot matrix between (numerical or boolean) features and absolute error\n",
    "import math\n",
    "\n",
    "# Features to plot: All numerical and boolean columns excluding the target variable\n",
    "features_to_plot = [col for col in numerical_columns + boolean_columns if col != \"numerical_target\"]\n",
    "\n",
    "# Define the number of columns for the grid\n",
    "num_cols = 4  \n",
    "\n",
    "# Calculate the number of rows needed\n",
    "num_rows = math.ceil(len(features_to_plot) / num_cols)\n",
    "\n",
    "# Set the figure size dynamically based on the grid size\n",
    "plt.figure(figsize=(num_cols * 4, num_rows * 4))\n",
    "\n",
    "# Iterate over the features to plot\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    # Create a subplot in the grid \n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    \n",
    "    # Create a scatterplot between the current feature and the absolute error\n",
    "    sns.scatterplot(data=df_test, x=feature, y=\"Absolute Error\")\n",
    "    \n",
    "    # Add title and axis labels\n",
    "    plt.title(f\"Absolute Error by {feature}\")\n",
    "    plt.xlabel(f\"{feature}\")\n",
    "    plt.ylabel(\"Absolute Error\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cd43e-206f-464e-a2f8-06f3c641a06f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Importance</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå <strong>Feature Importance Plot</strong>: Linear Regression or Elastic Net Regression.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb70cf5-e06d-4ec0-9d00-a582a3f5b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients\n",
    "# Note: final_model must be LinearRegression or ElasticNet\n",
    "coefficients = final_model.coef_\n",
    "\n",
    "# Get the feature names \n",
    "feature_names = X_train_transformed.columns  \n",
    "\n",
    "# Create a DataFrame to make it easier for Seaborn to plot\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Create feature importance plot of the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.head(10), palette=\"colorblind\")\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.xlabel(\"Importance (Absolute Coefficients)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5accb07a-2b83-4083-9f5a-09a26c776ab1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå <strong>Feature Importance Plot</strong>: Decision Tree or Random Forest. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35c105-7fc4-46ac-9fdd-02c110a1d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "# Note: final_model must be a decision tree or a random forest\n",
    "importances = final_model.feature_importances_\n",
    "\n",
    "# Get the feature names \n",
    "feature_names = X_train_transformed.columns  \n",
    "\n",
    "# Create a DataFrame to make it easier for Seaborn to plot\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Create feature importance plot of the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.head(10), palette=\"colorblind\")\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c656ef-62d4-4836-b919-c85e8549c92d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå <strong>Feature Importance Plot</strong>: XGBoost.  \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b47507-6281-4a28-961c-5dc8db8e01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "# Note: final_model must be XGBoost model \n",
    "importances = final_model.get_score(importance_type=\"gain\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": list(importances.keys()),\n",
    "    \"importance\": list(importances.values())\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Create feature importance plot of the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.head(10), palette=\"colorblind\")\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f0226-7417-46b1-a294-b712ab15c544",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Model Prediction Examples</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Show illustrative examples of model predictions from test data to demonstrate performance on unseen data.\n",
    "    <ul>\n",
    "        <li>Goal: Give stakeholders a clear picture of when the model performs well and when it struggles.</li>\n",
    "        <li>Recommendations:\n",
    "            <ul>\n",
    "                <li>Show 5-10 diverse examples: Best cases, worst cases, and typical cases.</li>\n",
    "                <li>Show 2-5 most important features, actual vs. predicted values, and errors.</li>\n",
    "                <li>Add notes about any interesting patterns or edge cases observed.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Example table:</li>\n",
    "    </ul>\n",
    "    <table style=\"border-collapse: collapse; margin-top: 10px;\">\n",
    "        <tr style=\"background-color:#e8f4fd; border-bottom: 2px solid #a6c8e9;\">\n",
    "            <th style=\"padding: 8px;\">Size (m¬≤)</th>\n",
    "            <th style=\"padding: 8px;\">Bedrooms</th>\n",
    "            <th style=\"padding: 8px;\">Neighborhood</th>\n",
    "            <th style=\"padding: 8px;\">Built Year</th>\n",
    "            <th style=\"padding: 8px;\">Actual Price (‚Ç¨)</th>\n",
    "            <th style=\"padding: 8px;\">Predicted Price (‚Ç¨)</th>\n",
    "            <th style=\"padding: 8px;\">Error (%)</th>\n",
    "        </tr>\n",
    "        <tr style=\"background-color:#e8f4fd;\">\n",
    "            <td style=\"padding: 8px;\">80</td>\n",
    "            <td style=\"padding: 8px;\">2</td>\n",
    "            <td style=\"padding: 8px;\">Prenzlauer Berg</td>\n",
    "            <td style=\"padding: 8px;\">1990</td>\n",
    "            <td style=\"padding: 8px;\">1200</td>\n",
    "            <td style=\"padding: 8px;\">1205</td>\n",
    "            <td style=\"padding: 8px; color:green;\">+0.4</td>\n",
    "        </tr>\n",
    "        <tr style=\"background-color:#d0e7fa;\">\n",
    "            <td style=\"padding: 8px;\">120</td>\n",
    "            <td style=\"padding: 8px;\">3</td>\n",
    "            <td style=\"padding: 8px;\">Charlottenburg</td>\n",
    "            <td style=\"padding: 8px;\">2005</td>\n",
    "            <td style=\"padding: 8px;\">2500</td>\n",
    "            <td style=\"padding: 8px;\">2100</td>\n",
    "            <td style=\"padding: 8px; color:red;\">-16.0</td>\n",
    "        </tr>\n",
    "        <tr style=\"background-color:#e8f4fd;\">\n",
    "            <td style=\"padding: 8px;\">100</td>\n",
    "            <td style=\"padding: 8px;\">3</td>\n",
    "            <td style=\"padding: 8px;\">Kreuzberg</td>\n",
    "            <td style=\"padding: 8px;\">2010</td>\n",
    "            <td style=\"padding: 8px;\">2000</td>\n",
    "            <td style=\"padding: 8px;\">2060</td>\n",
    "            <td style=\"padding: 8px; color:green;\">+3.0</td>\n",
    "        </tr>\n",
    "        <tr style=\"background-color:#d0e7fa;\">\n",
    "            <td style=\"padding: 8px;\">150</td>\n",
    "            <td style=\"padding: 8px;\">4</td>\n",
    "            <td style=\"padding: 8px;\">Mitte</td>\n",
    "            <td style=\"padding: 8px;\">2018</td>\n",
    "            <td style=\"padding: 8px;\">3500</td>\n",
    "            <td style=\"padding: 8px;\">4000</td>\n",
    "            <td style=\"padding: 8px; color:red;\">+14.3</td>\n",
    "        </tr>\n",
    "        <tr style=\"background-color:#e8f4fd;\">\n",
    "            <td style=\"padding: 8px;\">75</td>\n",
    "            <td style=\"padding: 8px;\">1</td>\n",
    "            <td style=\"padding: 8px;\">Neuk√∂lln</td>\n",
    "            <td style=\"padding: 8px;\">1995</td>\n",
    "            <td style=\"padding: 8px;\">1000</td>\n",
    "            <td style=\"padding: 8px;\">1030</td>\n",
    "            <td style=\"padding: 8px; color:green;\">+3.0</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb9e4a-e3f6-4107-ae0e-5ffe1d4fa3ac",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå <strong>Identify Best, Worst, and Typical Cases.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a0530-63a1-4ea9-939c-c5788fa4486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 most important features\n",
    "# Note: Ensure feature_importance_df was created in feature importance section \n",
    "top5_features = feature_importance_df.sort_values(\"importance\", ascending=False).head(5)[\"feature\"]\n",
    "\n",
    "# Keep only top 5 features and error metrics\n",
    "# Note: Ensure df_test was created in model evaluation section \n",
    "columns_to_keep = list(top5_features) + [\"Actual\", \"Predicted\", \"Error\", \"Absolute Error\", \"Error (%)\"]\n",
    "df_test = df_test[columns_to_keep].copy()\n",
    "\n",
    "# Best cases: Top 10 cases with smallest errors\n",
    "print(\"Best cases:\")\n",
    "print(df_test.sort_values(\"Absolute Error\").head(10))\n",
    "\n",
    "# Worst cases: Top 10 cases with largest errors\n",
    "print(\"Worst cases:\")\n",
    "print(df_test.sort_values(\"Absolute Error\", ascending=False).head(10))\n",
    "\n",
    "# Typical cases: 10 cases closest to the mean error\n",
    "mean_error = df_test[\"Absolute Error\"].mean()\n",
    "df_test[\"Difference from Mean Error\"] = np.abs(df_test[\"Absolute Error\"] - mean_error)\n",
    "print(\"Typical cases:\")\n",
    "print(df_test.sort_values(\"Difference from Mean Error\").head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ea3f4-6f7f-4500-b24e-9c83c2b9e572",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå <strong>Plot: Actual vs. Predicted Values.</strong> <br> \n",
    "    While identifying the best, worst, and typical cases offers insights into individual scenarios, this plot provides context for understanding overall model performance and reveals trends like consistent underprediction or overprediction.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca4c0f-80ee-4f41-b594-73c0ee288d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Actual vs. predicted values\n",
    "plt.figure(figsize=(7, 5), dpi=150)\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "plt.plot([min(y_test), max(y_test)], \n",
    "         [min(y_test), max(y_test)], \n",
    "         color=\"red\", \n",
    "         linestyle=\"--\", \n",
    "         label=\"Perfect Prediction\")  # Diagonal reference line\n",
    "plt.title(\"Actual vs. Predicted Values\", fontsize=14)\n",
    "plt.xlabel(\"Actual Values\", fontsize=12)\n",
    "plt.ylabel(\"Predicted Values\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend() \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661472f1-6c53-4270-a381-f96641708735",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Saving Model</h3>\n",
    "</div>   \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Save both the column transformer and the final model as pickle files in the <code>models</code> directory for later use.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bf3c1-3bc1-47a4-8c78-9ecf3052c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save column transformer \n",
    "try:\n",
    "    with open(\"models/column_transformer.pkl\", \"wb\") as file:\n",
    "        pickle.dump(column_transformer, file)\n",
    "    print(\"Column transformer saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the column transformer: {e}}\")\n",
    "    \n",
    "# Save final model\n",
    "try:\n",
    "    with open(\"models/final_model.pkl\", \"wb\") as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    print(\"Final model saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the final model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482eca8-6853-44dc-ad83-7f4b4e5a31f8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Modeling (Classification)</h1>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è For a classification problem, where the task is to predict a categorical target variable. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22993a78-d509-4a34-bc86-0f4ca6df9bf7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Helper functions to save and load models using <code>pickle</code>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1440a84-e00a-420c-9f37-2bbd93ca2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save model as .pkl file\n",
    "def save_model(model, filename):\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    # Save model as .pkl file \n",
    "    try:\n",
    "        with open(f\"models/{filename}\", \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "        print(f\"Model saved successfully under 'models/{filename}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "\n",
    "\n",
    "# Function to load model from .pkl file \n",
    "def load_model(filename):\n",
    "    try:\n",
    "        with open(f\"models/{filename}\", \"rb\") as file:  # ensure model is stored in \"models\" directory\n",
    "            model = pickle.load(file)\n",
    "        print(f\"{filename} loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb40770-9329-4b98-a938-6668e3a50907",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Baseline Models</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Train 8 baseline models (default hyperparameter values):  \n",
    "    <ul>\n",
    "        <li>Logistic Regression</li>\n",
    "        <li>Elastic Net Logistic Regression</li>\n",
    "        <li>K-Nearest Neighbors Classifier</li>\n",
    "        <li>Support Vector Classifier</li>\n",
    "        <li>Decision Tree Classifier</li>\n",
    "        <li>Random Forest Classifier</li>\n",
    "        <li>Multi-Layer Perceptron Classifier</li>\n",
    "        <li>XGBoost Classifier</li>\n",
    "    </ul>\n",
    "    üéØ Evaluate model performance:  \n",
    "    <ul>\n",
    "        <li>Metrics:\n",
    "            <ul>\n",
    "                <li>Accuracy</li>\n",
    "                <li>Recall</li>\n",
    "                <li>Precision</li>\n",
    "                <li>F1-score</li>\n",
    "                <li>ROC-AUC (Area Under the Receiver Operating Characteristic Curve)</li>\n",
    "                <li>AUC-PR (Area Under the Precision-Recall Curve)</li>\n",
    "            </ul>     \n",
    "        </li>\n",
    "        <li>Additional Diagnostics:\n",
    "            <ul>\n",
    "                <li>Metrics Comparison Table</li>\n",
    "                <li>Precision-Recall Curves</li>\n",
    "                <li>Classification Report</li>\n",
    "                <li>Confusion Matrix</li>                \n",
    "                <li>Overfitting</li>\n",
    "                <li>Feature Misclassification Analysis</li> \n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340453d-3c06-4e94-aec3-3febf9e26621",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Training</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Train all baseline models and store fitted models, predicted values, and evaluation metrics in a results dictionary.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb538f25-abb7-4ae9-935b-f3272740867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "\n",
    "# Define features to be used for model training\n",
    "columns_to_keep = [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\", \"feature_5\"]\n",
    "X_train_transformed = X_train_transformed[columns_to_keep].copy()\n",
    "X_val_transformed = X_val_transformed[columns_to_keep].copy()\n",
    "X_test_transformed = X_test_transformed[columns_to_keep].copy()\n",
    "\n",
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Elastic Net\": LogisticRegression(penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.5),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),  # get predicted probabilities for ROC-AUC and AUC-PR\n",
    "    \"Neural Network\": MLPClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "# Function to train and evaluate a single model\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    # Fit model on the training data and measure training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()    \n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "    auc_pr = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    # Return fitted model, predicted values, and evaluation metrics\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"training_time\": end_time - start_time,\n",
    "        \"y_val_pred\": y_val_pred,\n",
    "        \"y_val_proba\": y_val_proba,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"AUC-PR\": auc_pr\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# tree = evaluate_model(baseline_models[\"Decision Tree\"], X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "# knn = evaluate_model(baseline_models[\"K-Nearest Neighbors\"], X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "\n",
    "# Function to train and evaluate all models \n",
    "def evaluate_all_models(models, X_train, y_train, X_val, y_val):\n",
    "    results = {}   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        result = evaluate_model(model, X_train, y_train, X_val, y_val)\n",
    "        results[model_name] = result\n",
    "        print(f\"Training Time: {round(result['training_time'], 1)} sec\")\n",
    "    return results\n",
    "\n",
    "    \n",
    "# Use function to train all baseline models\n",
    "baseline_model_results = evaluate_all_models(baseline_models, X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "# Save baseline model results as .pkl file (using helper function)  \n",
    "save_model(baseline_model_results, \"baseline_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47831491-9019-4061-916a-3637f381efc5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Metrics</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Compare the evaluation metrics of all baseline models on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae74e6-3fc2-485b-9e85-3a0bf73492aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model results (using helper function)\n",
    "baseline_model_results = load_model(\"baseline_models.pkl\")\n",
    "\n",
    "# Extract evaluation metrics\n",
    "baseline_model_comparison = {\n",
    "    model_name: {\n",
    "        metric: baseline_model_results[model_name][metric]\n",
    "        for metric in [\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"]\n",
    "    }\n",
    "    for model_name in baseline_model_results\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame \n",
    "baseline_model_comparison = pd.DataFrame(baseline_model_comparison).transpose()\n",
    "\n",
    "# Show model comparison table\n",
    "round(baseline_model_comparison, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ac0d9-0fe3-48b2-98a9-29e646856fa6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Precision-Recall Curves</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Plot precision-recall curves of all baseline models on the validation data in a single graph.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdd453-f4fc-4ecb-859d-5747845560f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot precision-recall curve of one or more models\n",
    "def plot_precision_recall_curve(y_true, model_results, title=\"Precision-Recall Curves\", safe_to_file=False):\n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Get colors for different models (colormap \"viridis\" is colorblind-friendly)\n",
    "    cmap = plt.get_cmap(\"viridis\", len(model_results))\n",
    "    \n",
    "    # Plot baseline performance of random classifier\n",
    "    baseline = np.sum(y_true) / len(y_true)\n",
    "    ax.axhline(y=baseline, color=\"black\", linestyle=\"--\", alpha=0.5, label=f\"Baseline = {baseline:.2f}\")\n",
    "    \n",
    "    # Iterate over each model in the results dictionary\n",
    "    for i, (model_name, model_result) in enumerate(model_results.items()):\n",
    "        # Plot precision-recall curve for the current model\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_true, model_result[\"y_val_proba\"])\n",
    "        auc_pr = auc(recall_curve, precision_curve)\n",
    "        ax.plot(recall_curve, precision_curve, color=cmap(i), label=f\"{model_name} AUC-PR={auc_pr:.2f}\")\n",
    "    \n",
    "    # Customize title, axes labels, axes ticks, legend, and grid\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel(\"Precision\", fontsize=12)\n",
    "    ax.set_xlabel(\"Recall\", fontsize=12)\n",
    "    ax.set_ylim(0, 1.02)  # slightly extend y-axis for visibility\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.legend(loc=\"best\", fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save the plot to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Precision-recall curve plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving precision-recall curve plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving precision-recall curve plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to plot precision-recall curves of all baseline models on the validation data \n",
    "plot_precision_recall_curve(\n",
    "    y_val, \n",
    "    baseline_model_results, \n",
    "    title=\"Precision-Recall Curves: Baseline Models\", \n",
    "    safe_to_file=\"precision_recall_curves_baseline.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1157f8-e28e-49a6-b46e-a1d9d84e6f57",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Classification Report</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Create classification report for all baseline models on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb3dad-97bc-487d-af17-b21cb26ec53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for all baseline models\n",
    "for model_name, model_result in baseline_model_results.items():\n",
    "    print(f\"\\n{model_name}: Classification Report\")\n",
    "    print(classification_report(y_val, model_result[\"y_val_pred\"], zero_division=0))  # disable zero-division warning if no predictions for a given class (sets metric to 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8de60-8716-4688-9133-377dc322a39e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Confusion Matrix</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Plot confusion matrix for all baseline models on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6369504-0f81-4487-806d-b50ab4964d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix (y, y_pred, title=\"\", display_labels=None, safe_to_file=False, axes=None):\n",
    "    # Create axis if not provided\n",
    "    if axes is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = axes\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "    cm_disp.plot(cmap=\"viridis\", values_format=\"d\", colorbar=False, ax=ax)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title(f\"{title}\", fontsize=14)\n",
    "    ax.set_xlabel(\"Predicted\", fontsize=12)\n",
    "    ax.set_ylabel(\"True\", fontsize=12)\n",
    "\n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Confusion matrix saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving confusion matrix: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving confusion matrix to file: '{image_path}' already exists.\")\n",
    "\n",
    "    # Show the plot\n",
    "    if axes is None:\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Use function to plot confusion matrix for all baseline models ---\n",
    "# Calculate number of rows and columns for subplot grid\n",
    "n_plots = len(baseline_model_results)\n",
    "n_cols = 3  \n",
    "n_rows = math.ceil(n_plots / n_cols) \n",
    "\n",
    "# Create subplot grid with figure size based on 5x5 inches per subplot\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flat\n",
    "\n",
    "# Iterate over each model\n",
    "for i, (model_name, model_result) in enumerate(baseline_model_results.items()):\n",
    "    # Plot confusion matrix for current model\n",
    "    plot_confusion_matrix(y_val, model_result[\"y_val_pred\"], title=f\"{model_name}\", axes=axes[i])\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465106b-f78f-4daf-934d-52237756293d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Overfitting</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Diagnose overfitting for all baseline models by comparing evaluation metrics between training and validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04bf40-f491-4be1-ba3f-bdc1b8218c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze overfitting\n",
    "def analyze_overfitting(X_train, y_train, model_results):\n",
    "    # Store overfitting results as a list of dictionaries\n",
    "    overfitting_results = [] \n",
    "    \n",
    "    # Iterate over each model\n",
    "    for model_name, model_result in model_results.items():\n",
    "        model = model_result[\"model\"]\n",
    "    \n",
    "        # Predict on training data\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        if \"best_threshold\" in model_result:\n",
    "            y_train_pred = (y_train_proba >= model_result[\"best_threshold\"]).astype(int)\n",
    "        else:\n",
    "            y_train_pred = model.predict(X_train)\n",
    "\n",
    "        # Calculate training metrics\n",
    "        accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "        recall_train = recall_score(y_train, y_train_pred)\n",
    "        precision_train = precision_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_proba)\n",
    "        precision_curve_train, recall_curve_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "        auc_pr_train = auc(recall_curve_train, precision_curve_train)\n",
    "\n",
    "        # Get validation metrics\n",
    "        accuracy_val = model_result[\"Accuracy\"]\n",
    "        recall_val = model_result[\"Recall\"]\n",
    "        precision_val = model_result[\"Precision\"]\n",
    "        f1_val = model_result[\"F1-Score\"] \n",
    "        roc_auc_val = model_result[\"ROC-AUC\"]\n",
    "        auc_pr_val = model_result[\"AUC-PR\"]\n",
    "        \n",
    "        # Create results dictionary for current model\n",
    "        model_metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy (Train)\": accuracy_train,\n",
    "            \"Accuracy (Val)\": accuracy_val,\n",
    "            \"Accuracy (Diff)\": accuracy_train - accuracy_val,\n",
    "            \"Recall (Train)\": recall_train,\n",
    "            \"Recall (Val)\": recall_val,\n",
    "            \"Recall (Diff)\": recall_train - recall_val,\n",
    "            \"Precision (Train)\": precision_train,\n",
    "            \"Precision (Val)\": precision_val,\n",
    "            \"Precision (Diff)\": precision_train - precision_val,\n",
    "            \"F1-Score (Train)\": f1_train,\n",
    "            \"F1-Score (Val)\": f1_val, \n",
    "            \"F1-Score (Diff)\": f1_train - f1_val,\n",
    "            \"ROC-AUC (Train)\": roc_auc_train,\n",
    "            \"ROC-AUC (Val)\": roc_auc_val,\n",
    "            \"ROC-AUC (Diff)\": roc_auc_train - roc_auc_val,\n",
    "            \"AUC-PR (Train)\": auc_pr_train,\n",
    "            \"AUC-PR (Val)\": auc_pr_val,\n",
    "            \"AUC-PR (Diff)\": auc_pr_train - auc_pr_val,\n",
    "        }\n",
    "\n",
    "        # Append current model dictionary\n",
    "        overfitting_results.append(model_metrics)\n",
    "    \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    overfitting_results = pd.DataFrame(overfitting_results)\n",
    "    \n",
    "    # Set model as index\n",
    "    overfitting_results = overfitting_results.set_index(\"Model\")\n",
    "    \n",
    "    return overfitting_results\n",
    "\n",
    "\n",
    "# Use function to analyze overfitting for all baseline models \n",
    "baseline_models_overfitting = analyze_overfitting(X_train_transformed, y_train, model_results=baseline_model_results)\n",
    "\n",
    "# Display overfitting results (with 2 decimals)\n",
    "round(baseline_models_overfitting, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3cd1c6-4874-4467-8fd0-36c066566609",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Plot training and validation metrics of all baseline models side-by-side using grouped bar plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df26b0-87a8-477c-9cac-e4451024452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot train vs. validation metrics of multiple models \n",
    "def plot_train_val_metrics(metrics, overfitting_results, safe_to_file=False):    \n",
    "    # Ensure metrics is a list, even if a single metric is provided\n",
    "    if not isinstance(metrics, list):\n",
    "        metrics = [metrics]\n",
    "\n",
    "    # Get number of metrics\n",
    "    n = len(metrics)\n",
    "    \n",
    "    # Set up the subplot layout based on the number of metrics\n",
    "    if n == 1:\n",
    "        fig, ax = plt.subplots(figsize=(9, 6))\n",
    "        axes = [ax]\n",
    "    else:\n",
    "        # Create a grid with 2 columns and enough rows to accommodate all metrics\n",
    "        rows = int(np.ceil(n / 2))\n",
    "        fig, axes = plt.subplots(rows, 2, figsize=(16, 6 * rows))\n",
    "        # Flatten the axes for easier iteration\n",
    "        axes = axes.flat\n",
    "\n",
    "    # Add overall figure title only if there are multiple metrics\n",
    "    if n > 1:\n",
    "        fig.suptitle(\"Overfitting: Train vs. Validation Metrics\", fontsize=16, y=0.98)\n",
    "    \n",
    "    for ax, metric in zip(axes, metrics):        \n",
    "        # Create DataFrame with only training and validation metric\n",
    "        metric_df = overfitting_results[[f\"{metric} (Train)\", f\"{metric} (Val)\"]].reset_index()\n",
    "    \n",
    "        # Rename columns for clarity\n",
    "        metric_df = metric_df.rename(columns={f\"{metric} (Train)\": \"Training\", f\"{metric} (Val)\": \"Validation\"})\n",
    "        \n",
    "        # Melt the DataFrame for easier plotting \n",
    "        metric_df = pd.melt(\n",
    "            metric_df,\n",
    "            id_vars=[\"Model\"], \n",
    "            value_vars=[\"Training\", \"Validation\"],\n",
    "            var_name=\"Data\", \n",
    "            value_name=metric\n",
    "        )\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        sns.barplot(data=metric_df, x=\"Model\", y=metric, hue=\"Data\", palette=\"viridis\", ax=ax)\n",
    "    \n",
    "        # Add value labels \n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt=\"%.2f\", padding=3, fontsize=10)\n",
    "       \n",
    "        # Customize plot \n",
    "        if n > 1:\n",
    "            ax.set_title(f\"{metric}\", fontsize=14, pad=12)\n",
    "            ax.set_ylabel(\"\")\n",
    "        else:\n",
    "            ax.set_title(f\"Overfitting: Train vs. Validation {metric}\", fontsize=14, pad=12)\n",
    "            ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "        ax.tick_params(axis=\"x\", labelrotation=45 if len(overfitting_results.index) > 5 else 0, labelsize=12)  # rotate xticks if more than 5 models\n",
    "        ax.tick_params(axis=\"y\", labelsize=10)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "    \n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Overfitting plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving overfitting plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving overfitting plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to plot train vs. validation AUC-PR of all baseline models  \n",
    "plot_train_val_metrics(\"AUC-PR\", baseline_models_overfitting)\n",
    "\n",
    "# Use function to plot train vs. validation comparison of all metrics for all baseline models \n",
    "plot_train_val_metrics([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"], baseline_models_overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ea02d-8488-41cd-9f8b-a9b8b98f4aa1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Plot train-validation difference scores of all metrics and all baseline models in a single grouped bar plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dff9c5-e39a-4352-9ad0-c0f07ae8a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot train-validation difference scores of multiple models across multiple metrics in a single grouped bar plot\n",
    "def plot_train_val_difference(metrics, overfitting_results, safe_to_file=False):\n",
    "    # Extract difference scores from overfitting results\n",
    "    diff_metrics = [metric + \" (Diff)\" for metric in metrics]\n",
    "    metric_df = overfitting_results[diff_metrics].reset_index()\n",
    "        \n",
    "    # Rename columns for better reabability\n",
    "    metric_df.columns = metric_df.columns.str.replace(\" (Diff)\", \"\")\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting\n",
    "    metric_df = pd.melt(\n",
    "        metric_df,\n",
    "        id_vars=[\"Model\"], \n",
    "        var_name=\"Metric\", \n",
    "        value_name=\"Value\"\n",
    "    )\n",
    "    \n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "    # Create grouped bar plot\n",
    "    sns.barplot(data=metric_df, x=\"Model\", y=\"Value\", hue=\"Metric\", palette=\"viridis\", ax=ax)\n",
    "    \n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", padding=3, fontsize=7)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title(\"Overfitting: Train-Validation Difference Scores\", fontsize=14, pad=12)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Difference (Train - Val)\", fontsize=12)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12, labelrotation=45 if len(overfitting_results.index) > 5 else 0)  # rotate xticks if more than 5 models\n",
    "    ax.tick_params(axis=\"y\", labelsize=10)  \n",
    "    ax.set_ylim(metric_df[\"Value\"].min() - 0.05, metric_df[\"Value\"].max() + 0.05)  # slightly extend y-axis for visibility \n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Overfitting plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving overfitting plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving overfitting plot to file: '{image_path}' already exists.\")\n",
    "    \n",
    "    # Adjust layout and show plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Use function to plot train-validation difference scores of all metrics and all baseline models\n",
    "plot_train_val_difference([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"], baseline_models_overfitting)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c4866-533b-4021-9bef-7d8f80d94fbc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Misclassification Analysis</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Analyze relationships between the features and misclassifications on the validation data through correlations and box plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f331115-6314-43d0-9750-0913f5844286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze feature correlations with misclassifications\n",
    "def analyze_feature_misclassification(X, y, y_pred, numerical_features=None):  \n",
    "    # Combine features with actual and predicted target values into a single DataFrame\n",
    "    df = X.copy()  \n",
    "    df[\"Actual\"] = y\n",
    "    df[\"Predicted\"] = y_pred\n",
    "\n",
    "    # Create misclassification column\n",
    "    df[\"Misclassification\"] = (df[\"Predicted\"] != df[\"Actual\"]).astype(int)\n",
    "    \n",
    "    # Create correlations between features and misclassifications\n",
    "    correlations = df.drop(columns=[\"Actual\", \"Predicted\"]).corr()[\"Misclassification\"]\n",
    "    \n",
    "    # --- Create box plot matrix ---\n",
    "    # Ensure only numerical features\n",
    "    if numerical_features is None:\n",
    "        numerical_features = X.select_dtypes(include=[\"number\"]).columns  \n",
    "        # Include continuous numerical features, exclude binary and ordinal features with less than 5 categories\n",
    "        numerical_features = [column for column in numerical_features if X[column].nunique() > 4]\n",
    "        \n",
    "    # Number of columns and rows for matrix grid\n",
    "    n_cols = 3  \n",
    "    n_rows = math.ceil(len(numerical_features) / n_cols)\n",
    "    \n",
    "    # Create subplot grid with figure size based on 4x4 inches per subplot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flat\n",
    "    \n",
    "    # Iterate over the numerical features \n",
    "    for i, feature in enumerate(numerical_features):\n",
    "        # Get the current axes object\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create a box plot of the current feature grouped by misclassification\n",
    "        sns.boxplot(data=df, x=\"Misclassification\", y=feature, ax=ax)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_title(f\"{feature.title().replace('_', ' ')} by Misclassification\")\n",
    "        ax.set_xlabel(\"Misclassification\")\n",
    "        ax.set_ylabel(f\"{feature.title().replace('_', ' ')}\")\n",
    "        ax.set_xticks(ticks=[0, 1], labels=[\"Correct\", \"Misclassified\"])\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return the misclassification correlations\n",
    "    return correlations\n",
    "\n",
    "\n",
    "# Example usage for a single model  \n",
    "# rf_misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, baseline_model_results[\"Random Forest\"][\"y_val_pred\"])\n",
    "\n",
    "# --- Use function to analyze feature misclassification relationships of all baseline models (no outlier handling) ---\n",
    "# Initialize results dictionary\n",
    "baseline_misclassification_correlations = {}\n",
    "# Iterate over each model\n",
    "for model_name, model_result in baseline_model_results.items():\n",
    "    print(f\"{model_name}: Feature Misclassification Analysis\")\n",
    "    # Analyze feature misclassification relationships for current model\n",
    "    misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, model_result[\"y_val_pred\"])\n",
    "    # Add current model results to dictionary\n",
    "    baseline_misclassification_correlations[model_name] = misclassification_correlations\n",
    "    print(\"=\" * 145)\n",
    "# Convert results dictionary into a DataFrame    \n",
    "baseline_misclassification_correlations = pd.DataFrame(baseline_misclassification_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90368c-8b9b-4b7a-bcfa-a8b9608f3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature misclassification correlations of all baseline models (with 2 decimals)\n",
    "round(baseline_misclassification_correlations, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe872c0e-d238-4b4f-a946-2d12929e311a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Hyperparameter Tuning</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Model Selection</strong> <br>\n",
    "    üí°  Select the best performing baseline models for hyperparameter tuning. <br><br>\n",
    "    Example: The following models outperformed the other candidates on the evaluation metrics (accuracy, recall, precision, F1-score, ROC-AUC, AUC-PR) and additional diagnostics (precision-recall curves, classification report, confusion matrix, overfitting, feature misclassification analysis) and were selected for hyperparameter tuning:  \n",
    "    <ul>\n",
    "        <li><b>Random Forest</b>: Showed balanced, high performance across key metrics (notably F1-score and AUC-PR) and minimal overfitting.</li>\n",
    "        <li><b>XGBoost</b>: Achieved leading performance on key metrics, especially ROC-AUC and F1-score.</li>\n",
    "    </ul>\n",
    "    <strong>Next steps</strong> <br>\n",
    "    <ul>\n",
    "        <li>Tune the hyperparameters of each model using grid search or randomized search.</li>\n",
    "        <li>Retrain the best-performing model from each algorithm and plot precision-recall curves.</li>\n",
    "        <li>Optimize decision thresholds.</li>\n",
    "        <li>Evaluate the best hyperparameter-tuned models with both default and optimized thresholds using:\n",
    "            <ul>\n",
    "                <li>Evaluation metrics (accuracy, recall, precision, F1-score, ROC-AUC, AUC-PR).</li>\n",
    "                <li>Additional diagnostics (classification report, confusion matrix, overfitting, feature misclassification analysis).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d815b8d-fa8a-43bd-86a3-82f43e18968d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Grid Search</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">  \n",
    "    ‚ÑπÔ∏è Hyperparameters:  \n",
    "    <ul>  \n",
    "        <li>Core Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>estimator</code>: The machine learning model you want to optimize (e.g., <code>RandomForestClassifier</code>).</li>  \n",
    "                <li><code>param_grid</code>: A dictionary where keys are hyperparameter names, and values are lists of possible values to try.</li>  \n",
    "                <li><code>cv=5</code>: Number of folds for cross-validation.</li>  \n",
    "                <li><code>scoring=None</code>: The metric to evaluate model performance (e.g., <code>\"accuracy\"</code>, <code>\"precision\"</code>, <code>\"recall\"</code>, <code>\"f1\"</code>, <code>\"roc_auc\"</code>).</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "        <li>Optional Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>verbose=0</code>: Controls the verbosity of the output. Higher values provide more detailed logs.</li>  \n",
    "                <li><code>n_jobs=None</code>: Number of jobs to run in parallel. <code>-1</code> uses all available CPU cores.</li>  \n",
    "                <li><code>pre_dispatch=\"2*n_jobs\"</code>: Controls the number of jobs that get dispatched during parallel execution.</li>  \n",
    "                <li><code>refit=True</code>: If <code>True</code>, the estimator is refit on the entire training set using the best parameters.</li>  \n",
    "                <li><code>random_state=None</code>: Ensures reproducibility of results when estimators or scoring functions involve randomness.</li>  \n",
    "                <li><code>error_score=np.nan</code>: Value to assign to the score if a parameter combination results in a failure.</li>  \n",
    "                <li><code>return_train_score=False</code>: If <code>True</code>, training scores will be returned along with validation scores.</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "    </ul>  \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58eb45-fc5c-4072-a8af-e61a5dc9c140",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 1</h4>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    üí° Example: Random Forest Classifier <br><br>\n",
    "    ‚ÑπÔ∏è The following hyperparameters are typically the most impactful:\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees in the forest.</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree; <code>None</code> allows trees to grow until all leaves are pure or minimum samples are reached.</li>\n",
    "        <li><code>min_samples_split</code>: Minimum number of samples required to split a node.</li>\n",
    "        <li><code>min_samples_leaf</code>: Minimum number of samples required at a leaf node.</li>\n",
    "        <li><code>max_features</code>: Number of features considered for the best split; default <code>\"auto\"</code> uses the square root of all features.</li>\n",
    "        <li><code>class_weight</code>: Weights associated with classes. If <code>None</code>, all classes are supposed to have weight one. Use <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies in the input data.</li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\">scikit-learn RandomForestClassifier documentation</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1feb0-3ff7-40c7-a766-c491b0321ac4",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Fit grid search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae75e06-4ef6-469b-856e-e8f219f6b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500],               \n",
    "    \"max_depth\": [None, 10, 20],              \n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [0.33, 0.66, 1],                \n",
    "    \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf, \n",
    "    param_grid=rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"accuracy\"  # use \"f1\", \"precision\", \"recall\" or \"average_precision\" optionally\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "rf_grid_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Save fitted grid search as .pkl file using helper function  \n",
    "save_model(rf_grid_search, \"rf_grid_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad87cd-51d1-4830-8c53-412eebedfacb",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Load grid search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2528d8-f322-454c-b7dd-d100bfea0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grid search using helper function\n",
    "rf_grid_search = load_model(\"rf_grid_search.pkl\")\n",
    "\n",
    "# Create DataFrame of grid search results \n",
    "rf_grid_search_results = pd.DataFrame({\n",
    "    \"validation_accuracy\": rf_grid_search.cv_results_[\"mean_test_score\"],  # accuracy on validation data\n",
    "    \"parameters\": rf_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in rf_param_distributions:\n",
    "    rf_grid_search_results[parameter] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_grid_search_results = rf_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models\n",
    "rf_grid_search_results.sort_values(\"validation_accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b358e0-bfbd-4a25-87d2-2ea75536525b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 2</h4>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    üí° Example: XGBoost Classifier <br><br>\n",
    "    ‚ÑπÔ∏è The following hyperparameters are typically the most impactful:\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees (boosting rounds).</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree.</li>\n",
    "        <li><code>learning_rate</code>: Step size shrinkage to prevent overfitting.</li>\n",
    "        <li><code>subsample</code>: Fraction of training samples used per tree.</li>\n",
    "        <li><code>colsample_bytree</code>: Fraction of features used per tree.</li>\n",
    "        <li><code>gamma</code>: Minimum loss reduction required to split a leaf node.</li>\n",
    "        <li><code>min_child_weight</code>: Minimum sum of instance weights (hessian) in a child.</li>\n",
    "        <li><code>scale_pos_weight</code>: Balances positive and negative class weights for imbalanced datasets.</li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf82a48-f290-4ce6-9e50-82eb5d19d790",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Fit grid search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fb3cb-e92a-41ef-aa6d-5ca2bffe0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],               \n",
    "    \"max_depth\": [3, 6, 10], \n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],  \n",
    "    \"gamma\": [0, 0.1, 0.2],           \n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"scale_pos_weight\": [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=xgb_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"accuracy\"  # use \"f1\", \"precision\", \"recall\" or \"average_precision\" optionally\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "xgb_grid_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Save fitted grid search as .pkl file using helper function  \n",
    "save_model(xgb_grid_search, \"xgb_grid_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045823d-6fc9-4926-9957-92fc54e705c6",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Load grid search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7e442-ad81-4970-8400-203268267790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grid search using helper function \n",
    "xgb_grid_search = load_model(\"xgb_grid_search.pkl\")\n",
    "\n",
    "# Create DataFrame of grid search results \n",
    "xgb_grid_search_results = pd.DataFrame({\n",
    "    \"validation_accuracy\": xgb_grid_search.cv_results_[\"mean_test_score\"],  # accuracy on validation data\n",
    "    \"parameters\": xgb_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in xgb_param_grid:\n",
    "    xgb_grid_search_results[parameter] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_grid_search_results = xgb_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models\n",
    "xgb_grid_search_results.sort_values(\"validation_accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74d32e-a558-4314-9099-2804be9c4c79",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Randomized Search</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">  \n",
    "    ‚ÑπÔ∏è Hyperparameters:  \n",
    "    <ul>  \n",
    "        <li>Core Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>estimator</code>: The machine learning model you want to optimize (e.g., <code>RandomForestClassifier</code>).</li>  \n",
    "                <li><code>param_distributions</code>: A dictionary where keys are hyperparameter names, and values are the distributions or lists of possible values.</li>  \n",
    "                <li><code>n_iter=10</code>: The number of random combinations to sample from the parameter grid.</li>  \n",
    "                <li><code>cv=5</code>: Number of folds for cross-validation.</li>  \n",
    "                <li><code>scoring=None</code>: The metric to evaluate model performance (e.g., <code>\"accuracy\"</code>, <code>\"precision\"</code>, <code>\"recall\"</code>, <code>\"f1\"</code>, <code>\"roc_auc\"</code>).</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "        <li>Optional Parameters:  \n",
    "            <ul>  \n",
    "                <li><code>verbose=0</code>: Controls the verbosity of the output. Higher values provide more detailed logs.</li>  \n",
    "                <li><code>random_state=None</code>: Ensures reproducibility of results.</li>  \n",
    "                <li><code>n_jobs=None</code>: Number of jobs to run in parallel. <code>-1</code> uses all available CPU cores.</li>  \n",
    "                <li><code>pre_dispatch=\"2*n_jobs\"</code>: Controls the number of jobs that get dispatched during parallel execution.</li>  \n",
    "                <li><code>refit=True</code>: If <code>True</code>, the estimator is refit on the entire training set using the best parameters.</li>  \n",
    "                <li><code>error_score=np.nan</code>: Value to assign to the score if a parameter combination results in a failure.</li>  \n",
    "                <li><code>return_train_score=False</code>: If <code>True</code>, training scores will be returned along with validation scores.</li>  \n",
    "            </ul>  \n",
    "        </li>  \n",
    "    </ul>  \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41a01e-ddf1-4fef-aa2e-55d465fdb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy to define the hyperparameter distributions (randint for int, uniform for float)\n",
    "from scipy.stats import randint, uniform  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e384e-18bb-4b4f-87fc-f7517900521f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 1</h4>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    üí° Example: Random Forest Classifier <br><br>\n",
    "    ‚ÑπÔ∏è The following hyperparameters are typically the most impactful:\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees in the forest.</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree; <code>None</code> allows trees to grow until all leaves are pure or minimum samples are reached.</li>\n",
    "        <li><code>min_samples_split</code>: Minimum number of samples required to split a node.</li>\n",
    "        <li><code>min_samples_leaf</code>: Minimum number of samples required at a leaf node.</li>\n",
    "        <li><code>max_features</code>: Number of features considered for the best split; default <code>\"auto\"</code> uses the square root of all features.</li>\n",
    "        <li><code>class_weight</code>: Weights associated with classes. If <code>None</code>, all classes are supposed to have weight one. Use <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies in the input data.</li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\">scikit-learn RandomForestClassifier documentation</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fb1d8-e50a-4732-a4d7-35965e159681",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1395f-d2e1-4ff6-8987-652d8aa05867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "rf_param_distributions = {\n",
    "    \"n_estimators\": randint(100, 501),  # Random integers between 100 and 500             \n",
    "    \"max_depth\": randint(5, 31),  # Random integers between 5 and 30            \n",
    "    \"min_samples_split\": randint(2, 21),  # Random integers between 2 and 20\n",
    "    \"min_samples_leaf\": randint(1, 11),  # Random integers between 1 and 10\n",
    "    \"max_features\": uniform(0.1, 0.9),  # Random floats between 0.1 and 1.0  \n",
    "    \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=rf_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"accuracy\",  # use \"f1\", \"precision\", \"recall\" or \"average_precision\" optionally\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2,  # print training progress messages \n",
    "    refit=False  # Prevent storing \"best_estimator_\" to save storage\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "rf_random_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Save fitted random search as .pkl file using helper function \n",
    "save_model(rf_random_search, \"rf_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16c2ae-96f3-48d0-90d3-91d124d0f568",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Load random search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f52f2a-3390-4b2f-a0d2-8483a069b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search using helper function \n",
    "rf_random_search = load_model(\"rf_random_search.pkl\")\n",
    "\n",
    "# Create DataFrame of randomized search results\n",
    "rf_random_search_results = pd.DataFrame({\n",
    "    \"validation_accuracy\": rf_random_search.cv_results_[\"mean_test_score\"],  # accuracy on validation data\n",
    "    \"parameters\": rf_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in rf_param_distributions:\n",
    "    rf_random_search_results[parameter] = rf_random_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_random_search_results = rf_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models \n",
    "rf_random_search_results.sort_values(\"validation_accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abd5ea-a197-49d2-85e4-c546ea4923e4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#5f9ade; color:white; padding:8px; border-radius:6px;\">\n",
    "    <h4 style=\"margin:0px\">Model 2</h4>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    üí° Example: XGBoost Classifier <br><br>\n",
    "    ‚ÑπÔ∏è The following hyperparameters are typically the most impactful:\n",
    "    <ul>\n",
    "        <li><code>n_estimators</code>: Number of trees (boosting rounds).</li>\n",
    "        <li><code>max_depth</code>: Maximum depth of each tree.</li>\n",
    "        <li><code>learning_rate</code>: Step size shrinkage to prevent overfitting.</li>\n",
    "        <li><code>subsample</code>: Fraction of training samples used per tree.</li>\n",
    "        <li><code>colsample_bytree</code>: Fraction of features used per tree.</li>\n",
    "        <li><code>gamma</code>: Minimum loss reduction required to split a leaf node.</li>\n",
    "        <li><code>min_child_weight</code>: Minimum sum of instance weights (hessian) in a child.</li>\n",
    "        <li><code>scale_pos_weight</code>: Balances positive and negative class weights for imbalanced datasets.</li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e55af7-650b-4502-9b8e-9c214e4880ca",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Fit random search and save as <code>.pkl</code> file.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f056b5a-43af-4c92-9adb-1e165ac291b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter distributions \n",
    "xgb_param_distributions = {\n",
    "    \"n_estimators\": randint(100, 501),  # Random integers between 100 and 500             \n",
    "    \"max_depth\": randint(3, 11),  # Random integers between 3 and 10            \n",
    "    \"learning_rate\": uniform(0.01, 0.29),  # Random floats between 0.01 and 0.30\n",
    "    \"subsample\": uniform(0.5, 0.5),  # Random floats between 0.5 and 1.0\n",
    "    \"colsample_bytree\": uniform(0.5, 0.5),  # Random floats between 0.5 and 1.0\n",
    "    \"gamma\": uniform(0, 0.5),  # Random floats between 0.0 and 0.5  \n",
    "    \"min_child_weight\": randint(1, 10),  # Random integers between 1 and 9\n",
    "    \"scale_pos_weight\": randint(1, 16)  # Random integers between 1 and 15\n",
    "}\n",
    "\n",
    "# Initialize randomized search object\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=xgb_param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=5, \n",
    "    scoring=\"accuracy\",  # use \"f1\", \"precision\", \"recall\" or \"average_precision\" optionally\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # utilize all available CPU cores for parallel processing\n",
    "    verbose=2  # print training progress messages \n",
    ")\n",
    "\n",
    "# Fit the random search to the training data\n",
    "xgb_random_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Save fitted random search as .pkl file using helper function \n",
    "save_model(xgb_random_search, \"xgb_random_search.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5811353-a37f-4b1e-b517-a2d3932ce658",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Load random search from <code>.pkl</code> file and show Top 10 models.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5411d-1f7e-4026-a7c3-9af17259fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random search using helper function \n",
    "xgb_random_search = load_model(\"xgb_random_search.pkl\")\n",
    "\n",
    "# Create DataFrame of randomized search results\n",
    "xgb_random_search_results = pd.DataFrame({\n",
    "    \"validation_accuracy\": xgb_random_search.cv_results_[\"mean_test_score\"],  # accuracy on validation data\n",
    "    \"parameters\": xgb_random_search.cv_results_[\"params\"]  # parameter values\n",
    "})\n",
    "\n",
    "# Extract each hyperparameter as a separate column\n",
    "for parameter in xgb_param_distributions:\n",
    "    xgb_random_search_results[parameter] = xgb_random_search_results[\"parameters\"].apply(lambda x: x[parameter])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_random_search_results = xgb_random_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show top 10 best performing models \n",
    "xgb_random_search_results.sort_values(\"validation_accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac7f4a-bf45-4c35-9bae-bb03f634a675",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Retraining</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Retrain the best hyperparameter-tuned model from each algorithm on the full training dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec29123-bf13-4f89-a93d-8d45c440fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define best hyperparameter-tuned model from each algorithm (continue example with Random Forest and XGBoost)\n",
    "tuned_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(**rf_random_search.best_params_, random_state=42),  # or rf_grid_search.best_params_\n",
    "    \"XGBoost\": XGBClassifier(**xgb_random_search.best_params_, random_state=42)  # or xgb_grid_search.best_params_\n",
    "}\n",
    "\n",
    "# Retrain models on the full training data\n",
    "tuned_model_results = evaluate_all_models(tuned_models, X_train_transformed, y_train, X_val_transformed, y_val)\n",
    "\n",
    "# Save hyperparameter-tuned model results as .pkl file \n",
    "save_model(tuned_model_results, \"tuned_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f3b91-5bb3-48e8-985f-c73bad25667b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Precision-Recall Curves</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Plot precision-recall curves of the tuned models on the validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76e020-39c4-4eb8-b481-2c20eca93610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameter-tuned model results\n",
    "tuned_model_results = load_model(\"tuned_models.pkl\")\n",
    "\n",
    "# Plot precision-recall curves of hyperparameter-tuned models\n",
    "plot_precision_recall_curve(\n",
    "    y_val, \n",
    "    tuned_model_results, \n",
    "    title=\"Precision-Recall Curves: Hyperparameter-Tuned Models\",\n",
    "    safe_to_file=\"precision_recall_curves_tuned.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102035fb-63e2-46e2-a2f0-548008a96afb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Threshold Optimization</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è The default decision threshold (typically 0.5) may not be ideal to achieve the business goals, especially when certain performance targets are non-negotiable or misclassification has quantifiable costs. There are two common strategies to optimize the threshold based on business needs: \n",
    "    <ol>\n",
    "        <li>Set a minimum requirement for a primary metric (e.g., recall), and then optimize a secondary metric (e.g., precision).</li>\n",
    "        <li>Assign costs to false positives (FP) and false negatives (FN), and select the threshold that minimizes total cost.</li>\n",
    "    </ol> \n",
    "    Examples:\n",
    "    <ul>\n",
    "        <li>Spam Filtering: Enforce a minimum precision of 0.99 to avoid misclassifying legitimate emails as spam (users are highly sensitive to missing important emails), then maximize recall to catch as much spam as possible.</li>\n",
    "        <li>Medical Screening: Ensure a recall of at least 0.97 for the disease class to avoid missed diagnoses (which could delay treatment and increase mortality risk), then optimize precision to reduce unnecessary follow-up tests for healthy people.</li>\n",
    "        <li>Fraud Detection: Assign costs to missed fraud (FN cost of 200‚Ç¨) and blocked legitimate transactions (FP cost of 5‚Ç¨) and select the threshold that minimizes total cost.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e4d14-b05d-4900-ba67-3a0ef15f0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a single model across multiple decision thresholds and determine the best threshold \n",
    "def optimize_threshold(y_true, y_pred_proba, thresholds=None, optimize=\"Accuracy\", min_accuracy=0, min_recall=0, min_precision=0, min_f1=0,\n",
    "                       cost_fp=0, cost_fn=0, title=\"Metrics by Threshold\", safe_to_file=False):\n",
    "    # Use 1% to 99% in 1%-steps in the absence of custom thresholds\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "    # --- Calculate metrics for each threshold ---\n",
    "    # Store threshold evaluation results as list of dictionaries\n",
    "    threshold_results = []   \n",
    "    \n",
    "    # Iterate over each threshold\n",
    "    for threshold in thresholds:\n",
    "        # Get class predictions for current threshold\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate evaluation metrics for current threshold\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "        # Calculate cost for current threshold\n",
    "        if optimize == \"Cost\" and cost_fp == 0 and cost_fn == 0:\n",
    "            print(\"Warning: Cannot optimize for 'Cost' when cost_fn and cost_fp are both 0. Defaulting to 'Accuracy'.\")\n",
    "            optimize=\"Accuracy\"\n",
    "            total_cost = None\n",
    "        elif optimize == \"Cost\" and (cost_fp > 0 or cost_fn > 0):\n",
    "            # Calculate number of true negatives (tn), false positives (fp), false negatives (fn) and true positives (tp)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            # Calculate total cost\n",
    "            total_cost = cost_fp * fp + cost_fn * fn\n",
    "        else:\n",
    "            total_cost = None\n",
    "        \n",
    "        # Add evaluation metrics dictionary to list\n",
    "        threshold_results.append({\n",
    "            \"threshold\": threshold,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"F1-Score\": f1,\n",
    "            \"Cost\": total_cost\n",
    "        })    \n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    threshold_results = pd.DataFrame(threshold_results)\n",
    "\n",
    "    # --- Determine the best threshold --- \n",
    "    # Filter thresholds that satisfy minimum accuracy, recall, precision, and F1-score\n",
    "    filtered_thresholds = threshold_results[\n",
    "        (threshold_results[\"Accuracy\"] >= min_accuracy) & \n",
    "        (threshold_results[\"Recall\"] >= min_recall) & \n",
    "        (threshold_results[\"Precision\"] >= min_precision) & \n",
    "        (threshold_results[\"F1-Score\"] >= min_f1)\n",
    "    ]\n",
    "    \n",
    "    # Fallback to no minimum criteria if not a single threshold satisfies all of them\n",
    "    if filtered_thresholds.empty:\n",
    "        print(\"Warning: No threshold satisfies all minimum criteria.\")\n",
    "        print(\"Defaulting to optimization without any minimum criteria.\")\n",
    "        filtered_thresholds = threshold_results.copy()\n",
    "    \n",
    "    # Optimize accuracy\n",
    "    if optimize == \"Accuracy\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Accuracy\"].idxmax(), \"threshold\"] \n",
    "    # Optimize recall\n",
    "    elif optimize == \"Recall\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Recall\"].idxmax(), \"threshold\"]  \n",
    "    # Optimize precision\n",
    "    elif optimize == \"Precision\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Precision\"].idxmax(), \"threshold\"]  \n",
    "    # Optimize F1-score\n",
    "    elif optimize == \"F1-Score\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"F1-Score\"].idxmax(), \"threshold\"]  \n",
    "    # Optimize cost\n",
    "    elif optimize == \"Cost\":\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Cost\"].idxmin(), \"threshold\"]  \n",
    "    # Fallback to accuracy if metric unkown \n",
    "    else:\n",
    "        print(f\"Warning: Unknown optimize metric '{optimize}'. Defaulting to accuracy.\")\n",
    "        optimize = \"Accuracy\"\n",
    "        best_threshold = filtered_thresholds.loc[filtered_thresholds[\"Accuracy\"].idxmax(), \"threshold\"] \n",
    "    \n",
    "    # --- Plot metrics by threshold --- \n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Define metrics to use in plot starting with the optimization metric\n",
    "    metrics = [optimize]\n",
    "    # When plotting anything other than cost, also plot all metrics with minimum criteria\n",
    "    if optimize != \"Cost\":\n",
    "        if min_accuracy > 0 and \"Accuracy\" not in metrics:\n",
    "            metrics.append(\"Accuracy\")\n",
    "        if min_recall > 0 and \"Recall\" not in metrics:\n",
    "            metrics.append(\"Recall\")\n",
    "        if min_precision > 0 and \"Precision\" not in metrics:\n",
    "            metrics.append(\"Precision\")\n",
    "        if min_f1 > 0 and \"F1-Score\" not in metrics:\n",
    "            metrics.append(\"F1-Score\")\n",
    "\n",
    "    # Order metrics\n",
    "    ordered_metrics = [\"Cost\", \"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\"]\n",
    "    ordered_metrics = [metric for metric in ordered_metrics if metric in metrics]\n",
    "        \n",
    "    # Get a color from viridis colormap for each metric\n",
    "    n_metrics = len(ordered_metrics)\n",
    "    cmap = plt.get_cmap(\"viridis\", n_metrics)\n",
    "\n",
    "    # Iterate over each metric\n",
    "    for i, metric in enumerate(ordered_metrics):\n",
    "        # Create line plot of current metric by threshold\n",
    "        ax.plot(threshold_results[\"threshold\"], threshold_results[metric], label=metric, color=cmap(i))\n",
    "    \n",
    "    # Format cost plots\n",
    "    if optimize == \"Cost\":\n",
    "        title += f\" (FN Cost: {cost_fn}, FP Cost: {cost_fp})\"  # add FN and FP costs to title \n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{x:,.0f}\"))  # format cost y-axis with thousand separator and no decimals\n",
    "\n",
    "    # Format plots with all other metrics\n",
    "    else:\n",
    "        ax.set_ylim(-0.02, 1.02)  # value range of metrics is 0 to 1, slightly extend y-axis for better visibility\n",
    "        ax.set_yticks(np.arange(0, 1.1, 0.1))  # set y-axis ticks from 0 to 1 in 0.1 steps        \n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel(\"Threshold\", fontsize=12)\n",
    "    ax.set_ylabel(\"Metric Value\" if n_metrics > 1 else metric, fontsize=12)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.legend(fontsize=11).set_visible(True if n_metrics > 1 else False)\n",
    "    ax.grid(True, alpha=0.3)\n",
    " \n",
    "    # Add dashed line for best threshold \n",
    "    ax.axvline(x=best_threshold, color=\"gray\", linestyle=\"--\")\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.text(best_threshold+0.01, y_min+(y_max-y_min)*0.05, f\"Best Threshold: {best_threshold:.2f}\", rotation=90, fontsize=11)\n",
    "    \n",
    "    # Adjust layout\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save to file\n",
    "    if safe_to_file:\n",
    "        os.makedirs(\"images\", exist_ok=True)\n",
    "        image_path = os.path.join(\"images\", f\"{safe_to_file}\")  \n",
    "        if not os.path.exists(image_path):\n",
    "            try:        \n",
    "                plt.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "                print(f\"Metrics by threshold plot saved successfully to '{image_path}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving metrics by threshold plot: {e}\")\n",
    "        else:\n",
    "            print(f\"Skip saving metrics by threshold plot to file: '{image_path}' already exists.\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold, threshold_results\n",
    "\n",
    "\n",
    "# Example usage: Optimize accuracy (default) \n",
    "rf_best_threshold, rf_threshold_results = optimize_threshold(\n",
    "    y_true=y_val, \n",
    "    y_pred_proba=tuned_model_results[\"Random Forest\"][\"y_val_proba\"],\n",
    "    title=\"Accuracy by Threshold\"\n",
    ")\n",
    "\n",
    "# Example usage: Optimize precision while ensuring minimum recall  \n",
    "rf_best_threshold, rf_threshold_results = optimize_threshold(\n",
    "    y_true=y_val, \n",
    "    y_pred_proba=tuned_model_results[\"Random Forest\"][\"y_val_proba\"],\n",
    "    optimize=\"Precision\",\n",
    "    min_recall=0.95\n",
    ")\n",
    "\n",
    "# Example usage: Optimize cost  \n",
    "rf_best_threshold, rf_threshold_results = optimize_threshold(\n",
    "    y_true=y_val, \n",
    "    y_pred_proba=tuned_model_results[\"Random Forest\"][\"y_val_proba\"],\n",
    "    optimize=\"Cost\",\n",
    "    cost_fn=8500,\n",
    "    cost_fp=1250,\n",
    "    title=\"Cost by Threshold\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Use function to optimize thresholds for all tuned models --- \n",
    "# Store tuned threshold model results as dictionary\n",
    "tuned_threshold_model_results = {}\n",
    "\n",
    "# Iterate over each tuned model\n",
    "for model_name, model_results in tuned_model_results.items():\n",
    "    # Optimize threshold for current model on the validation data \n",
    "    best_threshold, threshold_results = optimize_threshold(\n",
    "        y_true=y_val, \n",
    "        y_pred_proba=model_results[\"y_val_proba\"], \n",
    "        optimize=\"Precision\",  # maximize precision\n",
    "        min_recall=0.95,  # ensure minimum recall of 0.95 \n",
    "        title=f\"{model_name}: Metrics by Threshold\"\n",
    "    )\n",
    "\n",
    "    # Add best threshold and resulting class predictions to tuned threshold model results dictionary\n",
    "    tuned_threshold_model_results[model_name] = {}\n",
    "    tuned_threshold_model_results[model_name][\"best_threshold\"] = best_threshold\n",
    "    tuned_threshold_model_results[model_name][\"y_val_pred\"] = (tuned_model_results[model_name][\"y_val_proba\"] >= best_threshold).astype(int)\n",
    "\n",
    "    # Add evaluation metrics of optimized threshold model to results dictionary\n",
    "    tuned_threshold_model_results[model_name][\"ROC-AUC\"] = model_results[\"ROC-AUC\"]\n",
    "    tuned_threshold_model_results[model_name][\"AUC-PR\"] = model_results[\"AUC-PR\"]\n",
    "    tuned_threshold_model_results[model_name][\"Accuracy\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"Accuracy\"].squeeze()\n",
    "    tuned_threshold_model_results[model_name][\"Recall\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"Recall\"].squeeze()\n",
    "    tuned_threshold_model_results[model_name][\"Precision\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"Precision\"].squeeze()\n",
    "    tuned_threshold_model_results[model_name][\"F1-Score\"] = threshold_results.loc[threshold_results[\"threshold\"] == best_threshold, \"F1-Score\"].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774835c2-4e02-48ad-8c62-89fc827fe843",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Metrics</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Compare evaluation metrics of hyperparameter-tuned models with default and optimized thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be02b09-49e9-45c2-907b-3d5155c781f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics with default thresholds\n",
    "metrics_default_thresholds = {\n",
    "    model_name: {\n",
    "        metric: tuned_model_results[model_name][metric]\n",
    "        for metric in [\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"]\n",
    "    }\n",
    "    for model_name in tuned_model_results\n",
    "}\n",
    "\n",
    "# Extract metrics with optimized thresholds\n",
    "metrics_optimized_thresholds = {\n",
    "    model_name: {\n",
    "        metric: tuned_threshold_model_results[model_name][metric]\n",
    "        for metric in [\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"]\n",
    "    }\n",
    "    for model_name in tuned_threshold_model_results\n",
    "}\n",
    "\n",
    "# Create dictionary with tuned model comparison tables \n",
    "tuned_model_comparison = {\n",
    "    \"default_thresholds\": pd.DataFrame(metrics_default_thresholds).transpose(),\n",
    "    \"optimized_thresholds\": pd.DataFrame(metrics_optimized_thresholds).transpose()\n",
    "}\n",
    "\n",
    "# Display comparison table for default thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Default Thresholds)\")\n",
    "display(round(tuned_model_comparison[\"default_thresholds\"], 2))\n",
    "\n",
    "# Display comparison table for optimized thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Optimized Thresholds)\")\n",
    "display(round(tuned_model_comparison[\"optimized_thresholds\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdc8f0-c031-46cf-8902-bc419434c3fa",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Classification Report</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    üìå Show classification report of hyperparameter-tuned models with default thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474358e-d8d7-4cbe-881d-354100d5441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for all tuned models with default thresholds\n",
    "for model_name, model_result in tuned_model_results.items():\n",
    "    print(f\"\\n{model_name} (Default Threshold): Classification Report\")\n",
    "    print(classification_report(y_val, model_result[\"y_val_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c3fae-855b-435c-9bd8-02ff5d1dfd9e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    üìå Show classification report of hyperparameter-tuned models with optimized thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e4057-80e1-4a76-865f-23949d6131d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for all tuned models with optimized thresholds\n",
    "for model_name, model_result in tuned_threshold_model_results.items():\n",
    "    print(f\"\\n{model_name} (Optimized Threshold): Classification Report\")\n",
    "    print(classification_report(y_val, model_result[\"y_val_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc384eb9-cc1d-4638-babe-7986b307ab91",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Confusion Matrix</h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    üìå Plot confusion matrix of hyperparameter-tuned models with default thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdd5f1-d9b0-446a-b7e4-dac7fbc62878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot confusion matrix for all tuned models with default thresholds ---\n",
    "# Calculate number of rows and columns for subplot grid\n",
    "n_plots = len(tuned_model_results)\n",
    "n_cols = 2  \n",
    "n_rows = math.ceil(n_plots / n_cols) \n",
    "\n",
    "# Create subplot grid with figure size based on 6x6 inches per subplot\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flat\n",
    "\n",
    "# Iterate over each model\n",
    "for i, (model_name, model_result) in enumerate(tuned_model_results.items()):\n",
    "    # Plot confusion matrix for current model\n",
    "    plot_confusion_matrix(y_val, model_result[\"y_val_pred\"], title=f\"{model_name} (Default Threshold)\", axes=axes[i])\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9328761-c020-405d-9af0-cc295bb51d0e",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    üìå Plot confusion matrix of hyperparameter-tuned models with optimized thresholds on the validation data.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b6f9f-4aa5-424e-b14c-5e981eece1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot confusion matrix for all tuned models with optimized thresholds ---\n",
    "# Calculate number of rows and columns for subplot grid\n",
    "n_plots = len(tuned_threshold_model_results)\n",
    "n_cols = 2  \n",
    "n_rows = math.ceil(n_plots / n_cols) \n",
    "\n",
    "# Create subplot grid with figure size based on 6x6 inches per subplot\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flat\n",
    "\n",
    "# Iterate over each model\n",
    "for i, (model_name, model_result) in enumerate(tuned_threshold_model_results.items()):\n",
    "    # Plot confusion matrix for current model\n",
    "    plot_confusion_matrix(y_val, model_result[\"y_val_pred\"], title=f\"{model_name} (Optimized Threshold)\", axes=axes[i])\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01102512-f7e6-4c56-b152-d16d3e865a6a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Overfitting</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    üìå Diagnose overfitting of hyperparameter-tuned models with default thresholds by comparing evaluation metrics between training and validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a47cf-064e-4fd8-b5e0-718257619200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overfitting for tuned models with default thresholds \n",
    "tuned_models_overfitting = analyze_overfitting(X_train_transformed, y_train, model_results=tuned_model_results)\n",
    "print(\"Hyperparameter-Tuned Models (Default Thresholds)\")\n",
    "round(tuned_models_overfitting, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2751432-dc39-4811-9253-752313474d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train vs. validation comparison of all metrics for tuned models with default thresholds  \n",
    "plot_train_val_metrics([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"], tuned_models_overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a7e46-bfed-4ac7-9165-b4025fbc07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-validation difference scores of all metrics for tuned models with default thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Default Thresholds)\")\n",
    "plot_train_val_difference([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"], tuned_models_overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32fb9e-118d-45d6-ae20-590807ee11ce",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    üìå Diagnose overfitting of hyperparameter-tuned models with optimized thresholds by comparing evaluation metrics between training and validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537752a0-a088-4dd5-9ea9-a34ac34f44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overfitting for tuned models with optimized thresholds \n",
    "tuned_threshold_models_overfitting = analyze_overfitting(X_train_transformed, y_train, model_results=tuned_threshold_model_results)\n",
    "print(\"Hyperparameter-Tuned Models (Optimized Thresholds)\")\n",
    "round(tuned_threshold_models_overfitting, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b48036-43c0-47a9-80dd-1b8671d8cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train vs. validation comparison of all metrics for tuned models with optimized thresholds  \n",
    "plot_train_val_metrics([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"], tuned_threshold_models_overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e7983-ffce-46c6-9530-07a01219c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-validation difference scores of all metrics for tuned models with optimized thresholds\n",
    "print(\"Hyperparameter-Tuned Models (Optimized Thresholds)\")\n",
    "plot_train_val_difference([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\", \"ROC-AUC\", \"AUC-PR\"], tuned_threshold_models_overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3570e30-8942-4634-8770-9454c6d81d50",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Misclassification Analysis</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Default Thresholds</strong> <br>\n",
    "    üìå Analyze feature misclassification relationships of hyperparameter-tuned models with default thresholds on the validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a338a9f-87e3-4938-863c-dcf4a1c1b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze feature misclassification relationships of tuned models with default thresholds ---\n",
    "# Initialize results dictionary\n",
    "tuned_misclassification_correlations = {}\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name, model_result in tuned_model_results.items():\n",
    "    print(f\"{model_name}: Feature Misclassification Analysis\")\n",
    "    \n",
    "    # Analyze feature misclassification relationships for current model\n",
    "    misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, model_result[\"y_val_pred\"])\n",
    "    \n",
    "    # Add current model results to dictionary\n",
    "    tuned_misclassification_correlations[model_name] = misclassification_correlations\n",
    "    print(\"=\" * 145)\n",
    "    \n",
    "# Convert results dictionary into a DataFrame    \n",
    "tuned_misclassification_correlations = pd.DataFrame(tuned_misclassification_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42b5ee-b312-4d69-8aa2-d86963f2ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature misclassification correlations of tuned models with default thresholds\n",
    "round(tuned_misclassification_correlations, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684d337-5c8d-434f-a391-07fd8012c034",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Optimized Thresholds</strong> <br>\n",
    "    üìå Analyze feature misclassification relationships of hyperparameter-tuned models with optimized thresholds on the validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f966c6-dc3c-442d-98b0-dad16ef7ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyze feature misclassification relationships of tuned models with default thresholds ---\n",
    "# Initialize results dictionary\n",
    "tuned_threshold_misclassification_correlations = {}\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name, model_result in tuned_threshold_model_results.items():\n",
    "    print(f\"{model_name}: Feature Misclassification Analysis\")\n",
    "    \n",
    "    # Analyze feature misclassification relationships for current model\n",
    "    misclassification_correlations = analyze_feature_misclassification(X_val_transformed, y_val, model_result[\"y_val_pred\"])\n",
    "    \n",
    "    # Add current model results to dictionary\n",
    "    tuned_threshold_misclassification_correlations[model_name] = misclassification_correlations\n",
    "    print(\"=\" * 145)\n",
    "    \n",
    "# Convert results dictionary into a DataFrame    \n",
    "tuned_threshold_misclassification_correlations = pd.DataFrame(tuned_threshold_misclassification_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab2687-c22b-4bfb-afb4-719173d9f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature misclassification correlations of tuned models with optimized thresholds\n",
    "round(tuned_threshold_misclassification_correlations, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da82187-7ced-4000-8eb7-8b8a5468ff49",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Final Model</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Model Selection</strong> <br>\n",
    "    üí° Example: The Random Forest model achieved the best performance (e.g., accuracy = 0.92) on the validation data compared to other candidates. Therefore, the final model is a Random Forest Classifier with the following hyperparameters:  \n",
    "    <ul>\n",
    "        <li><code>n_estimators=225</code></li>\n",
    "        <li><code>max_depth=26</code></li>\n",
    "        <li><code>min_samples_split=2</code></li>\n",
    "        <li><code>min_samples_leaf=1</code></li>\n",
    "        <li><code>max_features=0.13</code></li>\n",
    "        <li><code>class_weight=\"balanced\"</code></li>\n",
    "    </ul>\n",
    "    <strong>Next steps</strong> <br>\n",
    "    <ul>\n",
    "        <li>Retrain the final model, save it to a file, and apply the optimized threshold.</li>\n",
    "        <li>Evaluate the final model on the training, validation, and test sets to confirm its generalizability.</li>\n",
    "        <li>Use the same performance metrics as for the baseline and hyperparameter-tuned models \n",
    "            (accuracy, recall, precision, F1-score, ROC-AUC, AUC-PR) along with additional diagnostics \n",
    "            (classification report, confusion matrix, overfitting, feature misclassification analysis).\n",
    "        </li>\n",
    "        <li>In addition, conduct a feature importance analysis and review model prediction examples \n",
    "            to further interpret and validate model behavior.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3ea9b-9d33-4bb8-851c-645471bb427d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Retraining</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Retrain the final model with optimized hyperparameters and save it to a <code>.pkl</code> file in the <code>model</code> directory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd443a0-a947-4464-a094-b396b19c7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with tuned hyperparameters (Random Forest example)\n",
    "final_model = RandomForestClassifier(**rf_random_search.best_params_, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "final_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Save final model as .pkl file \n",
    "save_model(final_model, \"final_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ef7e4-37d0-44da-a98f-e5717847665b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Apply optimized threshold to obtain predicted values on the training, validation, and test sets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c822855-1ebf-440e-8844-9e51cbfb6e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final model\n",
    "final_model = load_model(\"final_model.pkl\")\n",
    "\n",
    "# Predict probabilities for class-1 on the training, validation and test data\n",
    "y_train_proba = final_model.predict_proba(X_train_transformed)[:, 1]\n",
    "y_val_proba = final_model.predict_proba(X_val_transformed)[:, 1]\n",
    "y_test_proba = final_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "# Apply optimized threshold to convert probabilities to binary predictions\n",
    "threshold = tuned_threshold_model_results[\"Random Forest\"][\"best_threshold\"]  # Random Forest example\n",
    "y_train_pred = (y_train_proba >= threshold).astype(int)\n",
    "y_val_pred = (y_val_proba >= threshold).astype(int)\n",
    "y_test_pred = (y_test_proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4906c4-3902-4059-9613-e273e834f4b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Metrics</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Compare evaluation metrics of the final model on the training, validation, and test sets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1d89c-e111-48b7-977f-46ff991467f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate evaluation metrics ---\n",
    "# Accuracy\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Recall \n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "recall_val = recall_score(y_val, y_val_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "\n",
    "# Precision \n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "precision_val = precision_score(y_val, y_val_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "\n",
    "# F1-score\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "f1_val = f1_score(y_val, y_val_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc_train = roc_auc_score(y_train, y_train_proba)\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# AUC-PR\n",
    "precision_curve_train, recall_curve_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "auc_pr_train = auc(recall_curve_train, precision_curve_train)\n",
    "precision_curve_val, recall_curve_val, _ = precision_recall_curve(y_val, y_val_proba)\n",
    "auc_pr_val = auc(recall_curve_val, precision_curve_val)\n",
    "precision_curve_test, recall_curve_test, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "auc_pr_test = auc(recall_curve_test, precision_curve_test)\n",
    "\n",
    "# --- Comparison table of evaluation metrics ---\n",
    "# Create comparison table\n",
    "final_model_comparison = pd.DataFrame({\n",
    "    \"Data\": [\"Training\", \"Validation\", \"Test\"],\n",
    "    \"Accuracy\": [accuracy_train, accuracy_val, accuracy_test],\n",
    "    \"Recall\": [recall_train, recall_val, recall_test],\n",
    "    \"Precision\": [precision_train, precision_val, precision_test],\n",
    "    \"F1-Score\": [f1_train, f1_val, f1_test],\n",
    "    \"ROC-AUC\": [roc_auc_train, roc_auc_val, roc_auc_test],\n",
    "    \"AUC-PR\": [auc_pr_train, auc_pr_val, auc_pr_test]\n",
    "})\n",
    "\n",
    "# Display comparison table (with 2 decimals)\n",
    "round(final_model_comparison, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f902d1-6c7b-4c9f-b9be-c29dcad03396",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Classification Report</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Show classification report for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de11214-4afc-41dd-96e7-42b124d8b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report: Training\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Classification Report: Validation\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"Classification Report: Test\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc0330-3259-4f0c-a5e2-9a054e2e986f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Confusion Matrix</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Plot confusion matrix for training, validation, and test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067a723-f53d-42e6-a342-690a67eefa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot confusion matrix for training, validation, and test data ---\n",
    "# Create subplot grid \n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(\"Final Model: Confusion Matrix\", fontsize=16, y=1.03)\n",
    "\n",
    "# Create confusion matrix subplots for training, validation, and test data (using helper function)\n",
    "plot_confusion_matrix(y_train, y_train_pred, title=\"Training\", axes=axes[0])\n",
    "plot_confusion_matrix(y_val, y_val_pred, title=\"Validation\", axes=axes[1])\n",
    "plot_confusion_matrix(y_test, y_test_pred, title=\"Test\", axes=axes[2])\n",
    "    \n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554a275-9163-4b84-ac6c-7f358cdba06d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Overfitting</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Diagnose overfitting of final model by comparing evaluation metrics between training, validation, and test data using a grouped bar plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5531a-ab46-480a-b005-d6637ec55668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Overfitting grouped bar plot ---\n",
    "# Melt the final_model_comparison DataFrame (from the \"Metrics\" section) for easier plotting \n",
    "metric_df = pd.melt(final_model_comparison, id_vars=[\"Data\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create grouped bar plot\n",
    "sns.barplot(data=metric_df, x=\"Metric\", y=\"Value\", hue=\"Data\", palette=\"viridis\", ax=ax)\n",
    "\n",
    "# Add value labels \n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", padding=3, fontsize=10)\n",
    "\n",
    "# Customize plot \n",
    "ax.set_title(\"Final Model: Overfitting\", fontsize=14)\n",
    "ax.set_ylabel(\"Metric Value\", fontsize=12)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylim(0, 1.05)  # slightly extend y-axis upper limit for better visibility of value labels\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.1))  # y-axis ticks from 0 to 1 in 0.1 steps\n",
    "ax.tick_params(axis=\"x\", labelsize=12) \n",
    "ax.tick_params(axis=\"y\", labelsize=10)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Adjust the layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ef4e8-8eeb-4a74-aa21-33a4b7cd6dca",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Misclassification Analysis</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Analyze relationships between the features and misclassifications on the training, validation, and test sets through correlations and box plots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2e290-7482-431b-bec8-f19447a64f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature misclassification relationships on the training data (using helper function)\n",
    "print(\"Feature Misclassification Relationships: Training\")\n",
    "misclassification_correlations_train = analyze_feature_misclassification(X_train_transformed, y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06266c3a-992f-4fa4-8d32-58f524c7e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature misclassification relationships on the validation data\n",
    "print(\"Feature Misclassification Relationships: Validation\")\n",
    "misclassification_correlations_val = analyze_feature_misclassification(X_val_transformed, y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee41c0-0fd6-4b07-930e-50507a2d0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature misclassification relationships on the test data\n",
    "print(\"Feature Misclassification Relationships: Test\")\n",
    "misclassification_correlations_test = analyze_feature_misclassification(X_test_transformed, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806017d-d6a9-4592-9a62-155f43a39eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Misclassification Correlations ---\n",
    "# Merge training, validation, and test correlations into a single DataFrame\n",
    "misclassification_correlations = pd.concat(\n",
    "    [misclassification_correlations_train, misclassification_correlations_val, misclassification_correlations_test], \n",
    "    axis=1,\n",
    "    keys=[\"Training\", \"Validation\", \"Test\"]\n",
    ")\n",
    "\n",
    "# Show misclassification correlations (rounded to 2 decimals and sorted by absolute correlation values in test data)\n",
    "round(misclassification_correlations.sort_values(\"Test\", key=abs, ascending=False), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63402e-85d3-445c-be3c-074f350a97fc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Feature Importance</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Visualize feature importances of Logistic Regression or Elastic Net Logistic Regression with a bar plot.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359019a-e069-4637-9df6-659467b197fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature importance plot for logistic regression or elastic net logistic regression ---\n",
    "# Get the coefficients \n",
    "coefficients = final_model.coef_  # final_model must be LogisticRegression \n",
    "\n",
    "# Get feature names in proper format \n",
    "feature_names = X_train_transformed.columns.str.title().str.replace(\"_\", \" \") \n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create bar plot of top 10 features\n",
    "sns.barplot(data=feature_importance_df.head(10), x=\"importance\", y=\"feature\", hue=\"feature\", palette=\"viridis\", ax=ax)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title(\"Final Model: Top 10 Most Important Features\", fontsize=14)\n",
    "ax.set_xlabel(\"Feature Importance (Absolute Coefficients)\", fontsize=12)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(feature_importance_df[\"importance\"].head(10)):\n",
    "    ax.text(value + 0.001, i, f\"{value:.2f}\", va=\"center\", fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save plot to file\n",
    "os.makedirs(\"images\", exist_ok=True)  \n",
    "image_path = os.path.join(\"images\", \"feature_importance_final.png\")  \n",
    "if not os.path.exists(image_path):\n",
    "    try:        \n",
    "        fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "        print(f\"Feature importance plot saved successfully to '{image_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature importance plot: {e}\")\n",
    "else:\n",
    "    print(f\"Skip saving feature importance plot: '{image_path}' already exists.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e810d-595c-42bb-a0d6-29284c45feaa",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Visualize feature importances of Decision Tree or Random Forest with a bar plot.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f861923-6bc6-4423-b56d-2a3557a6d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature importance plot for decision tree or a random forest ---\n",
    "# Get feature importances\n",
    "importances = final_model.feature_importances_  # final_model must be DecisionTree or RandomForest\n",
    "\n",
    "# Get feature names in proper format \n",
    "feature_names = X_train_transformed.columns.str.title().str.replace(\"_\", \" \")\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create bar plot of top 10 features\n",
    "sns.barplot(data=feature_importance_df.head(10), x=\"importance\", y=\"feature\", hue=\"feature\", palette=\"viridis\", ax=ax)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title(\"Final Model: Top 10 Most Important Features\", fontsize=14)\n",
    "ax.set_xlabel(\"Feature Importance\", fontsize=12)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(feature_importance_df[\"importance\"].head(10)):\n",
    "    ax.text(value + 0.001, i, f\"{value:.2f}\", va=\"center\", fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save plot to file\n",
    "os.makedirs(\"images\", exist_ok=True)  \n",
    "image_path = os.path.join(\"images\", \"feature_importance_final.png\")  \n",
    "if not os.path.exists(image_path):\n",
    "    try:        \n",
    "        fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "        print(f\"Feature importance plot saved successfully to '{image_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature importance plot: {e}\")\n",
    "else:\n",
    "    print(f\"Skip saving feature importance plot: '{image_path}' already exists.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65833257-d9e7-4162-99aa-135fadddf6d8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Visualize feature importances of XGBoost with a bar plot.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd242e0-bd0a-422e-ac3b-4ccda65901a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature importance plot for XGBoost ---\n",
    "# Get the feature importances\n",
    "importances = final_model.get_score(importance_type=\"gain\")  # final_model must be XGBoost \n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": list(importances.keys()),\n",
    "    \"importance\": list(importances.values())\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create bar plot of top 10 features\n",
    "sns.barplot(data=feature_importance_df.head(10), x=\"importance\", y=\"feature\", hue=\"feature\", palette=\"viridis\", ax=ax)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title(\"Final Model: Top 10 Most Important Features\", fontsize=14)\n",
    "ax.set_xlabel(\"Feature Importance\", fontsize=12)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels \n",
    "for i, value in enumerate(feature_importance_df[\"importance\"].head(10)):\n",
    "    ax.text(value + 0.001, i, f\"{value:.2f}\", va=\"center\", fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save plot to file\n",
    "os.makedirs(\"images\", exist_ok=True)  \n",
    "image_path = os.path.join(\"images\", \"feature_importance_final.png\")  \n",
    "if not os.path.exists(image_path):\n",
    "    try:        \n",
    "        fig.savefig(image_path, bbox_inches=\"tight\", dpi=144)\n",
    "        print(f\"Feature importance plot saved successfully to '{image_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving feature importance plot: {e}\")\n",
    "else:\n",
    "    print(f\"Skip saving feature importance plot: '{image_path}' already exists.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903a6f7-21f1-43b5-a254-592cd418b0b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Model Prediction Examples</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Show illustrative examples of model predictions from test data to showcase performance on unseen data.\n",
    "    <ul>\n",
    "        <li>Goal: Give stakeholders a clear picture of when the model performs well and when it struggles.</li>\n",
    "        <li>Recommendations:\n",
    "            <ul>\n",
    "                <li>Show 5-10 diverse examples: Best cases, worst cases, and typical cases.</li>\n",
    "                <li>Show 2-5 most important features, actual vs. predicted values, prediction confidence, and whether the example was misclassified.</li>\n",
    "                <li>Add notes about any interesting patterns or edge cases observed.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify best examples (correct with high confidence), worst examples (incorrect with high confidence), and typical examples (average confidence).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74288008-dad0-4b12-8a30-bd98512d9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create DataFrame with top 5 features, actual and predicted values, prediction confidence, and misclassifications from test data ---\n",
    "# Combine test features with actual and predicted target values into a single DataFrame\n",
    "prediction_examples = X_test.copy()  # raw features (before transformation) for easier interpretability\n",
    "prediction_examples.columns = prediction_examples.columns.str.title().str.replace(\"_\", \" \")  # format feature names\n",
    "prediction_examples[\"Actual\"] = y_test\n",
    "prediction_examples[\"Predicted\"] = y_test_pred\n",
    "\n",
    "# Calculate prediction confidence (ensure final_model supports predict_proba and X_test_transformed is correctly preprocessed)\n",
    "prediction_examples[\"Confidence Score\"] = final_model.predict_proba(X_test_transformed).max(axis=1)\n",
    "prediction_examples[\"Confidence\"] = prediction_examples[\"Confidence Score\"].apply(lambda x: f\"{x:.0%}\")  # format as percentages\n",
    "\n",
    "# Calculate misclassification (and format for better readability)\n",
    "prediction_examples[\"Misclassification\"] = prediction_examples[\"Predicted\"] != prediction_examples[\"Actual\"]\n",
    "prediction_examples[\"Misclassification\"] = prediction_examples[\"Misclassification\"].map({True: \"‚ùå Yes\", False: \"‚úÖ No\"})  \n",
    "\n",
    "# Get top 5 most important features (ensure feature_importance_df was created in feature importance section) \n",
    "top5_features = feature_importance_df.sort_values(\"importance\", ascending=False).head(5)[\"feature\"]\n",
    "\n",
    "# Filter DataFrame columns\n",
    "columns_to_keep = list(top5_features) + [\"Actual\", \"Predicted\", \"Confidence Score\", \"Confidence\", \"Misclassification\"]\n",
    "prediction_examples = prediction_examples[columns_to_keep].copy()\n",
    "\n",
    "# --- Identify best examples ---\n",
    "# Get the top 5 correctly classified cases with highest confidence scores for each class \n",
    "best_class_1 = prediction_examples[(prediction_examples[\"Actual\"] == 1) & (prediction_examples[\"Misclassification\"] == \"‚úÖ No\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "best_class_0 = prediction_examples[(prediction_examples[\"Actual\"] == 0) & (prediction_examples[\"Misclassification\"] == \"‚úÖ No\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "\n",
    "# Combine and show best prediction examples from each class\n",
    "best_examples = pd.concat([best_class_1, best_class_0]).drop(columns=[\"Confidence Score\"])\n",
    "print(\"Best examples (correct with high confidence):\")\n",
    "display(best_examples)\n",
    "\n",
    "# --- Identify worst examples ---\n",
    "# Get the top 5 misclassified cases despite high confidence scores for each class\n",
    "worst_class_1 = prediction_examples[(prediction_examples[\"Actual\"] == 1) & (prediction_examples[\"Misclassification\"] == \"‚ùå Yes\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "worst_class_0 = prediction_examples[(prediction_examples[\"Actual\"] == 0) & (prediction_examples[\"Misclassification\"] == \"‚ùå Yes\")].sort_values(\"Confidence Score\", ascending=False).head(5)\n",
    "\n",
    "# Combine and show worst prediction examples from each class\n",
    "worst_examples = pd.concat([worst_class_1, worst_class_0]).drop(columns=[\"Confidence Score\"])\n",
    "print(\"Worst examples (incorrect with high confidence):\")\n",
    "display(worst_examples)\n",
    "\n",
    "# --- Identify typical examples ---\n",
    "# Calculate average confidence score\n",
    "mean_confidence = prediction_examples[\"Confidence Score\"].mean()\n",
    "\n",
    "# Calculate difference from average confidence for each case\n",
    "prediction_examples[\"Difference from Mean Confidence\"] = np.abs(prediction_examples[\"Confidence Score\"] - mean_confidence)\n",
    "\n",
    "# Get the top 5 cases with confidence scores closest to the average confidence for each class\n",
    "typical_class_1 = prediction_examples[prediction_examples[\"Actual\"] == 1].sort_values(\"Difference from Mean Confidence\").head(5)\n",
    "typical_class_0 = prediction_examples[prediction_examples[\"Actual\"] == 0].sort_values(\"Difference from Mean Confidence\").head(5)\n",
    "\n",
    "# Combine and show typical prediction examples from each class\n",
    "typical_examples = pd.concat([typical_class_1, typical_class_0]).drop(columns=[\"Confidence Score\", \"Difference from Mean Confidence\"])\n",
    "print(\"Typical examples (average confidence):\")\n",
    "display(typical_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b668e-2e12-4350-8ca3-3750c5642883",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Display the best, worst, and typical prediction examples for each class in the test data, including the top five most important features, actual vs. predicted values, prediction confidence, and whether the example was misclassified.\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
    "    üí° Option 1: Single Table (Loan Default Example)\n",
    "</p>\n",
    "\n",
    "| Example | Income (‚Ç¨) | Age | Credit Score | Loan Amount (‚Ç¨) | Work Years | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|---------|------------|-----|--------------|-----------------|------------|----------------|-------------------|------------|---------------|\n",
    "| Best    | 48,000     | 26  | 740          | 12,000          | 1          | Yes            | Yes               | 99%        | ‚úÖ No        |\n",
    "| Best    | 72,000     | 56  | 770          | 25,000          | 2          | No             | No                | 100%       | ‚úÖ No        |\n",
    "| Worst   | 55,000     | 42  | 630          | 40,000          | 3          | Yes            | No                | 95%        | ‚ùå Yes       |\n",
    "| Worst   | 32,000     | 24  | 620          | 18,000          | 1          | No             | Yes               | 98%        | ‚ùå Yes       |\n",
    "| Typical | 60,000     | 47  | 710          | 30,000          | 3          | Yes            | Yes               | 94%        | ‚úÖ No        |\n",
    "| Typical | 38,000     | 24  | 750          | 10,000          | 4          | No             | No                | 94%        | ‚úÖ No        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e97024-a4e0-4bbd-80c1-ab49bc2b32d7",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
    "    üí° Option 2: Separate Tables (Loan Default Example)\n",
    "</p>\n",
    "\n",
    "üèÜ Best Predictions (Correct with High Confidence)\n",
    "| Income (‚Ç¨) | Age | Credit Score | Loan Amount (‚Ç¨) | Work Years | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|------------|-----|--------------|-----------------|------------|----------------|-------------------|------------|---------------|\n",
    "| 48,000     | 26  | 740          | 12,000          | 1          | Yes            | Yes               | 99%        | ‚úÖ No         |\n",
    "| 72,000     | 56  | 770          | 25,000          | 2          | No             | No                | 100%       | ‚úÖ No         |\n",
    "\n",
    "‚ö†Ô∏è Worst Predictions (Incorrect with High Confidence)\n",
    "| Income (‚Ç¨) | Age | Credit Score | Loan Amount (‚Ç¨) | Work Years | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|------------|-----|--------------|-----------------|------------|----------------|-------------------|------------|---------------|\n",
    "| 55,000     | 42  | 630          | 40,000          | 3          | Yes            | No                | 95%        | ‚ùå Yes        |\n",
    "| 32,000     | 24  | 620          | 18,000          | 1          | No             | Yes               | 98%        | ‚ùå Yes        |\n",
    "\n",
    "‚ûñ Typical Predictions (Average Confidence)\n",
    "| Income (‚Ç¨) | Age | Credit Score | Loan Amount (‚Ç¨) | Work Years | Actual Default | Predicted Default | Confidence | Misclassified |\n",
    "|------------|-----|--------------|-----------------|------------|----------------|-------------------|------------|---------------|\n",
    "| 60,000     | 47  | 710          | 30,000          | 3          | Yes            | Yes               | 94%        | ‚úÖ No         |\n",
    "| 38,000     | 24  | 750          | 10,000          | 4          | No             | No                | 94%        | ‚úÖ No         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8612fd-291f-4038-a180-35ed168eaca5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Summary</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ae0de-8df6-4a3a-8c2e-404e167fe1ab",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px; margin-bottom:8px;\">\n",
    "    <h2 style=\"margin:0px\">üßπ Data Preprocessing</h2>\n",
    "</div> \n",
    "\n",
    "Used `pandas` and `sklearn` for data loading, cleaning, transformation, and saving.\n",
    "- **Loaded data**:\n",
    "    - From a .csv file using `pandas` `read_csv`.\n",
    "    - From a MySQL database table using `sqlalchemy`, `mysql-connector-python`, and `pandas` `read_sql`.\n",
    "- **Standardized column names and labels**:\n",
    "    - To `snake_case` using `pandas` string methods and `apply` with custom functions.\n",
    "- **Handled duplicates**:\n",
    "    - Removed duplicate rows (e.g., based on the ID column) using `pandas` `drop_duplicates`.\n",
    "- **Handled data types**:\n",
    "    - Converted string columns to numerical types (`pandas` `astype`) and datetime types (`pandas` `to_datetime`).\n",
    "    - Converted string columns with two categories to boolean columns using `pandas` `map`.\n",
    "- **Train-validation-test split**:\n",
    "    - Split data into training (e.g., 70%), validation (15%), and test (15%) sets using `sklearn` `train_test_split`.\n",
    "- **Engineered new features**:\n",
    "    - Derived categorical, numerical, and boolean features from raw text columns using `pandas` `apply` with custom functions.\n",
    "    - Derived categorical and numerical features from categorical text columns using tiering and target encoding.\n",
    "- **Defined semantic type** for each column (numerical, categorical, boolean).\n",
    "- **Handled missing values**:\n",
    "    - Deleted rows with missing values using `pandas` `dropna`.\n",
    "    - Imputed missing values: Filled in the median for numerical columns or the mode for categorical columns using `pandas` `fillna`.\n",
    "- **Handled outliers**:\n",
    "    - Identified univariate outliers using statistical methods (e.g., 3SD or 1.5 IQR) with custom transformer classes that inherit from `sklearn` `BaseEstimator` and `TransformerMixin`.\n",
    "    - Identified multivariate outliers using `sklearn` `IsolationForest`. \n",
    "- **Feature scaling and encoding**:\n",
    "    - Scaled numerical features using standard scaling with `sklearn` `StandardScaler` or min-max normalization with `MinMaxScaler`.\n",
    "    - Encoded categorical features:\n",
    "        - Nominal features: Used one-hot encoding with `sklearn` `OneHotEncoder`.\n",
    "        - Ordinal features: Used ordinal encoding with `sklearn` `OrdinalEncoder`.\n",
    "    - Applied scaling and encoding together using `sklearn` `ColumnTransformer`.\n",
    "- **Polynomial features**:\n",
    "    - Created polynomial features using `sklearn` `PolynomialFeatures`.\n",
    "- **Saved the preprocessed data**:\n",
    "    - For training, validation, and test sets as .csv files using `pandas` `to_csv`.\n",
    "    - In a MySQL database table using `sqlalchemy`, `mysql-connector-python`, and `pandas` `to_sql`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a430b-44ae-42af-8c34-39eb7c4fc068",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px; margin-bottom:8px;\">\n",
    "    <h2 style=\"margin:0px\">üîç Exploratory Data Analysis (EDA)</h2>\n",
    "</div> \n",
    "\n",
    "Used `pandas`, `numpy`, `seaborn`, and `matplotlib` for statistical analysis and visualizations.\n",
    "- **Univariate EDA**:\n",
    "    - **Numerical columns**:\n",
    "        - Analyzed descriptive statistics (e.g., mean, median, standard deviation) using `pandas` `describe`.\n",
    "        - Visualized distributions with histograms using `seaborn` `histplot` and `matplotlib`.\n",
    "    - **Categorical columns**:\n",
    "        - Examined frequencies using `pandas` `value_counts`.\n",
    "        - Visualized frequency distributions with bar plots using `seaborn` `barplot` and `matplotlib`. \n",
    "- **Bivariate EDA**:\n",
    "    - **Numerical vs. numerical**:\n",
    "        - Analyzed pairwise relationships with a correlation matrix (`pandas` `corr` and `numpy`) and visualized them with a heatmap (`seaborn` `heatmap`).\n",
    "        - Visualized relationships with scatterplots using `seaborn` `scatterplot` and `matplotlib`.\n",
    "    - **Numerical vs. categorical**:\n",
    "        - Explored relationships with group-wise statistics (e.g., mean or median by category) using `pandas` `groupby` and `agg`.\n",
    "        - Quantified the magnitude of group differences with Cohen's d using a custom function.\n",
    "        - Visualized results with bar plots using `seaborn` `barplot` and `matplotlib`.\n",
    "    - **Categorical vs. categorical**:\n",
    "        - Analyzed relationships with contingency tables using `pandas` `crosstab`.\n",
    "        - Visualized relationships with grouped bar plots using `pandas` `crosstab` `plot` and `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed3fb7-2f25-4660-8e83-ec6fcd88120c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px; margin-bottom:8px;\">\n",
    "    <h2 style=\"margin:0px\">üèóÔ∏è Modeling</h2>\n",
    "</div> \n",
    "\n",
    "Used `sklearn` and `xgboost` for model training, evaluation, and optimization.\n",
    "- **Baseline models**:\n",
    "    - Trained baseline models with default hyperparameter values.\n",
    "    - Evaluated regression model performance using metrics and diagnostics:\n",
    "        - Calculated RMSE, MAPE, and R-squared with `sklearn` `mean_squared_error`, `mean_absolute_percentage_error`, and `r2_score`.\n",
    "        - Analyzed errors with residual plots and error distributions using `pandas` and `matplotlib`.\n",
    "        - Explored feature-error relationships using scatter plots with `seaborn` `scatterplot` and `matplotlib`.\n",
    "    - Evaluated classification model performance using metrics and diagnostics:\n",
    "        - Calculated accuracy, recall, precision, F1-score, ROC-AUC, and AUC-PR with `sklearn` `accuracy_score`, `recall_score`, `precision_score`, `f1_score`, `roc_auc_score`, `precision_recall_curve`, and `auc`.\n",
    "        - Compared metrics using tables with `pandas`.\n",
    "        - Plotted precision-recall curves using `matplotlib`.\n",
    "        - Created classification reports with `sklearn` `classification_report`.\n",
    "        - Plotted confusion matrices with `sklearn` `confusion_matrix` and `ConfusionMatrixDisplay`.\n",
    "        - Analyzed overfitting using custom tables (`pandas`) and plots (`seaborn` `barplot` and `matplotlib`).\n",
    "        - Explored feature-misclassification relationships through correlations (`pandas` `corr`) and grouped box plots (`seaborn` `boxplot` and `matplotlib`).\n",
    "- **Hyperparameter tuning**:\n",
    "    - Performed grid search (`sklearn` `GridSearchCV`) or random search (`RandomizedSearchCV`) with 5-fold cross-validation on the most promising baseline models.\n",
    "    - Retrained the best-performing model from each algorithm.\n",
    "    - Classification models: Plotted precision-recall curves and optimized decision thresholds.\n",
    "    - Evaluated all tuned models using the above metrics and diagnostics.\n",
    "- **Final model**:\n",
    "    - Selected Random Forest for its good performance (highest AUC-PR), low overfitting (lowest AUC-PR difference), and interpretability.\n",
    "    - Retrained and saved the final model as a .pkl file using `pickle`.\n",
    "    - Compared training, validation, and test performance using the above metrics and diagnostics.\n",
    "    - Visualized feature importances using `seaborn` `barplot` and `matplotlib` or `xgboost` `plot_importance`.\n",
    "    - Showed model prediction examples of the best, worst, and typical cases using `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf5748-f921-40ca-96b6-ae04d03c33b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px; margin-bottom:8px;\">\n",
    "    <h2 style=\"margin:0px\">Next Steps</h2>\n",
    "</div> \n",
    "\n",
    "- Add unsupervised learning section with algorithms like K-Means Clustering, DBSCAN, and Principal Component Analysis (PCA).\n",
    "- Add model deployment as a web application with an interactive UI and API (e.g., using Gradio or Flask)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be26fa-81d6-4e6a-84a0-1558eb0d2bdf",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Appendix</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73fca8-de43-459f-bd34-755392003ab3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Model Hyperparameters</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Overview of hyperparameters and their default values for both regression and classification models. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd661a79-ea34-48af-9bda-bea90113223f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Regression</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Linear Regression</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li><code>fit_intercept=True</code>: Calculates the intercept; can be set to <code>False</code> if data is already centered.</li>\n",
    "        <li><code>n_jobs=None</code>: Number of CPU threads; use <code>-1</code> for all available processors.</li>\n",
    "        <li><code>positive=False</code>: Forces regression coefficients to be non-negative if set to <code>True</code>.</li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\" target=\"_blank\">scikit-learn LinearRegression documentation</a>.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68737fb-326a-4679-b0e9-82886b6902ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Logistic Regression</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Regularization:\n",
    "            <ul>\n",
    "                <li><code>penalty=\"l2\"</code>: Regularization type (options are <code>\"l1\"</code>, <code>\"l2\"</code>, <code>\"elasticnet\"</code>, or <code>\"none\"</code>).</li>\n",
    "                <li><code>C=1.0</code>: Inverse of regularization strength (smaller = stronger regularization).</li>\n",
    "                <li><code>l1_ratio=None</code>: The mix ratio between L1 and L2 regularization (0 is L2 only, 1 is L1 only). Used with <code>penalty=\"elasticnet\"</code> and <code>solver=\"saga\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Optimization:\n",
    "            <ul>\n",
    "                <li><code>solver=\"lbfgs\"</code>: Optimization algorithm (options are <code>\"lbfgs\"</code>, <code>\"liblinear\"</code>, <code>\"saga\"</code>, <code>\"newton-cg\"</code>, or <code>\"sag\"</code>).</li>\n",
    "                <li><code>max_iter=100</code>: Maximum number of iterations for convergence.</li>\n",
    "                <li><code>tol=1e-4</code>: Tolerance for stopping criteria.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Model Behavior:\n",
    "            <ul>\n",
    "                <li><code>fit_intercept=True</code>: Whether to calculate the intercept.</li>\n",
    "                <li><code>intercept_scaling=1.0</code>: Scaling of the intercept (used with <code>solver=\"liblinear\"</code>).</li>\n",
    "                <li><code>warm_start=False</code>: Reuse previous solution for subsequent fits.</li>\n",
    "                <li><code>dual=False</code>: Use dual formulation (only with <code>penalty=\"l2\"</code> and <code>solver=\"liblinear\"</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Multi-Class Classification:\n",
    "            <ul>\n",
    "                <li><code>multi_class=\"auto\"</code>: Multi-class handling (options are <code>\"auto\"</code>, <code>\"ovr\"</code>, or <code>\"multinomial\"</code>).</li>\n",
    "                <li><code>class_weight=None</code>: Class weights; <code>\"balanced\"</code> adjusts for imbalanced class frequencies.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance:\n",
    "            <ul>\n",
    "                <li><code>random_state=None</code>: Seed for reproducibility.</li>\n",
    "                <li><code>n_jobs=None</code>: Number of CPU cores used during computation (only with <code>solver=\"liblinear\"</code>).</li>\n",
    "                <li><code>verbose=0</code>: Verbosity level; controls solver progress output.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\" target=\"_blank\">scikit-learn LogisticRegression documentation</a>.  \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c757c-444f-4fb0-b5ed-b0d1fa423ea7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Elastic Net</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Elastic Net Regression</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>alpha=1.0</code>: Regularization strength. Higher values increase the penalty, reducing overfitting but possibly underfitting the data.</li>\n",
    "                <li><code>l1_ratio=0.5</code>: The mix between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "                    <ul>\n",
    "                        <li><code>l1_ratio=1.0</code> corresponds to pure Lasso.</li>\n",
    "                        <li><code>l1_ratio=0.0</code> corresponds to pure Ridge.</li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Solver Configuration:\n",
    "            <ul>\n",
    "                <li><code>fit_intercept=True</code>: Whether to calculate the intercept for the model. If <code>False</code>, the model assumes data is centered.</li>\n",
    "                <li><code>precompute=False</code>: Whether to use precomputed Gram matrices to speed up calculations. Set to <code>True</code> for small datasets.</li>\n",
    "                <li><code>max_iter=1000</code>: The maximum number of iterations allowed for convergence during optimization.</li>\n",
    "                <li><code>tol=1e-4</code>: Stopping criterion for optimization. If the change in the cost function is smaller than <code>tol</code>, training stops.</li>\n",
    "                <li><code>warm_start=False</code>: If <code>True</code>, reuse the solution of the previous fit as initialization for the next fit.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>selection=\"cyclic\"</code>: Determines the strategy for updating coefficients. <code>\"cyclic\"</code> updates coefficients sequentially, <code>\"random\"</code> in a random order.</li>\n",
    "                <li><code>random_state=None</code>: Seed for random number generation when <code>selection=\"random\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Configuration:\n",
    "            <ul>\n",
    "                <li><code>copy_X=True</code>: Whether to copy the input data <code>X</code>. If <code>False</code>, training modifies the original data, saving memory.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\" target=\"_blank\">scikit-learn Elastic Net Regression documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfa52c-bb23-438a-b1a0-f0b74710f8d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Elastic Net Logistic Regression</strong> <br>\n",
    "    ‚ÑπÔ∏è Note: Uses the same <code>LogisticRegression</code> class and hyperparameters as the standard Logistic Regression, with the key difference being the hyperparameter values for the Elastic Net regularization. Specifically, the <code>penalty</code> is set to <code>\"elasticnet\"</code>, and the solver should be <code>\"saga\"</code>. The <code>l1_ratio</code> parameter controls the mix between L1 and L2 regularization.\n",
    "    <ul>\n",
    "        <li>Hyperparameters:\n",
    "            <ul>\n",
    "                <li><code>penalty=\"elasticnet\"</code>: Regularization type, combining both L1 and L2 penalties (<code>\"l1\"</code>, <code>\"l2\"</code>, <code>\"elasticnet\"</code>, or <code>\"none\"</code>).</li>\n",
    "                <li><code>l1_ratio=0.5</code>: The mix ratio between L1 and L2 regularization (0 is L2 only, 1 is L1 only). This parameter is specific to Elastic Net regularization.</li>\n",
    "                <li><code>solver=\"saga\"</code>: Optimization algorithm, recommended for Elastic Net (<code>\"lbfgs\"</code>, <code>\"liblinear\"</code>, <code>\"saga\"</code>, etc.).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b8d6b-8149-4d3e-bdcb-2245095f9295",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">K-Nearest Neighbors</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>K-Nearest Neighbors Regressor</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_neighbors=5</code>: The number of neighbors to use for prediction. A higher value makes the model more general, while a lower value may lead to overfitting.</li>\n",
    "                <li><code>weights=\"uniform\"</code>: Determines how neighbors are weighted during prediction. <code>\"uniform\"</code> gives equal weight to all neighbors, while <code>\"distance\"</code> gives closer neighbors more influence.</li>\n",
    "                <li><code>p=2</code>: The power parameter for the Minkowski distance. <code>p=2</code> corresponds to the Euclidean distance, commonly used in KNN regression.</li>\n",
    "                <li><code>algorithm=\"auto\"</code>: The algorithm used to compute nearest neighbors. <code>\"auto\"</code> selects the best algorithm based on the dataset (options include <code>\"ball_tree\"</code>, <code>\"kd_tree\"</code>, and <code>\"brute\"</code>).</li>\n",
    "                <li><code>leaf_size=30</code>: The size of the leaf in tree-based algorithms like Ball Tree and KD Tree. This parameter impacts the speed and memory usage during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Distance Metrics:\n",
    "            <ul>\n",
    "                <li><code>metric=\"minkowski\"</code>: The distance metric used to calculate the proximity between data points. The default is <code>\"minkowski\"</code>, but you can also use <code>\"euclidean\"</code>, <code>\"manhattan\"</code>, etc.</li>\n",
    "                <li><code>metric_params=None</code>: Additional parameters for the distance metric, usually left as <code>None</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>n_jobs=None</code>: The number of parallel jobs to run for neighbor search. Setting <code>n_jobs=-1</code> utilizes all available CPU cores for faster computation.</li>\n",
    "                <li><code>radius=1.0</code>: Defines the search radius for neighbors. Instead of a fixed number of neighbors, this parameter considers neighbors within a given radius. This can be more flexible than <code>n_neighbors</code>, but it should be used with care as it may return an inconsistent number of neighbors.</li>\n",
    "                <li><code>max_iter=None</code>: The maximum number of iterations for the neighbor search process. This is set to <code>None</code> to allow unlimited iterations.</li>\n",
    "                <li><code>verbose=False</code>: Whether or not to print progress messages during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\" target=\"_blank\">scikit-learn KNN Regressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df529c7-220a-48f0-8c18-0d6aee08f6f7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>K-Nearest Neighbors Classifier</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_neighbors=5</code>: The number of neighbors to use for prediction. A higher value makes the model more general, while a lower value may lead to overfitting.</li>\n",
    "                <li><code>weights=\"uniform\"</code>: Determines how neighbors are weighted during prediction. <code>\"uniform\"</code> gives equal weight to all neighbors, while <code>\"distance\"</code> gives closer neighbors more influence.</li>\n",
    "                <li><code>p=2</code>: The power parameter for the Minkowski distance. <code>p=2</code> corresponds to the Euclidean distance, commonly used in KNN classification.</li>\n",
    "                <li><code>algorithm=\"auto\"</code>: The algorithm used to compute nearest neighbors. <code>\"auto\"</code> selects the best algorithm based on the dataset (options include <code>\"ball_tree\"</code>, <code>\"kd_tree\"</code>, and <code>\"brute\"</code>).</li>\n",
    "                <li><code>leaf_size=30</code>: The size of the leaf in tree-based algorithms like Ball Tree and KD Tree. This parameter impacts the speed and memory usage during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Distance Metrics:\n",
    "            <ul>\n",
    "                <li><code>metric=\"minkowski\"</code>: The distance metric used to calculate the proximity between data points. The default is <code>\"minkowski\"</code>, but you can also use <code>\"euclidean\"</code>, <code>\"manhattan\"</code>, etc.</li>\n",
    "                <li><code>metric_params=None</code>: Additional parameters for the distance metric, usually left as <code>None</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>n_jobs=None</code>: The number of parallel jobs to run for neighbor search. Setting <code>n_jobs=-1</code> utilizes all available CPU cores for faster computation.</li>\n",
    "                <li><code>radius=1.0</code>: Defines the search radius for neighbors. Instead of a fixed number of neighbors, this parameter considers neighbors within a given radius. This can be more flexible than <code>n_neighbors</code>, but it should be used with care as it may return an inconsistent number of neighbors.</li>\n",
    "                <li><code>max_iter=None</code>: The maximum number of iterations for the neighbor search process. This is set to <code>None</code> to allow unlimited iterations.</li>\n",
    "                <li><code>verbose=False</code>: Whether or not to print progress messages during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" target=\"_blank\">scikit-learn KNN Classifier documentation</a>.  \n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc56c91-1c4c-4e23-be88-332e3ddb74a6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Support Vector Machine</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Support Vector Regressor</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>C=1.0</code>: Regularization parameter balancing error reduction and model complexity.</li>\n",
    "                <li><code>epsilon=0.1</code>: Margin of tolerance for predictions without penalty.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Kernel Configuration:\n",
    "            <ul>\n",
    "                <li><code>kernel=\"rbf\"</code>: Kernel function for mapping data to higher dimensions (default is radial basis function or <code>\"rbf\"</code>).</li>\n",
    "                <li><code>degree=3</code>: Degree of the polynomial kernel function (ignored by the rbf kernel).</li>\n",
    "                <li><code>gamma=\"scale\"</code>: Influence range of a single training example. <code>\"scale\"</code> means <code>1 / (n_features * X.var())</code>.</li>\n",
    "                <li><code>coef0=0.0</code>: Independent term in polynomial and sigmoid kernel function (ignored by the rbf kernel).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>tol=1e-3</code>: Stopping criterion for optimization. If the change in the cost function is less than this tolerance, training stops.</li>\n",
    "                <li><code>cache_size=200</code>: Memory (MB) allocated for kernel computation. Larger values speed up training.</li>\n",
    "                <li><code>shrinking=True</code>: Enables the shrinking heuristic, which can speed up training by eliminating unnecessary steps during optimization.</li>\n",
    "                <li><code>verbose=False</code>: Whether to print progress messages during training.</li>\n",
    "                <li><code>max_iter=-1</code>: Maximum number of iterations during training (<code>-1</code> for no limit).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\" target=\"_blank\">scikit-learn SVR documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9f6ae-b540-4791-bcc2-25a460bc3018",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Support Vector Classifier</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li><code>C=1.0</code>: Regularization strength (smaller = stronger regularization).</li>\n",
    "        <li><code>kernel=\"rbf\"</code>: Kernel type (<code>\"linear\"</code>, <code>\"poly\"</code>, <code>\"rbf\"</code>, <code>\"sigmoid\"</code>, or callable).</li>\n",
    "        <li><code>degree=3</code>: Degree of the polynomial kernel (ignored for other kernels).</li>\n",
    "        <li><code>gamma=\"scale\"</code>: Kernel coefficient (<code>\"scale\"</code>, <code>\"auto\"</code>, or float).</li>\n",
    "        <li><code>coef0=0.0</code>: Independent term in kernel functions (<code>\"poly\"</code> and <code>\"sigmoid\"</code> only).</li>\n",
    "        <li><code>class_weight=None</code>: Class weights; <code>\"balanced\"</code> adjusts for imbalance.</li>\n",
    "        <li><code>max_iter=-1</code>: Maximum iterations (unlimited if <code>-1</code>).</li>\n",
    "        <li><code>probability=False</code>: Enables probability estimates (slower training).</li>\n",
    "        <li><code>random_state=None</code>: Seed for reproducibility (affects <code>shrinking</code> and <code>probability</code>).</li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\" target=\"_blank\">scikit-learn SVC documentation</a>.  \n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f19fec-946e-4644-a538-c574a154b5ab",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Decision Tree</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Decision Tree Regressor</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>max_depth=None</code>: Maximum depth of the tree. <code>None</code> allows nodes to expand until all leaves are pure or contain fewer samples than <code>min_samples_split</code>.</li>\n",
    "                <li><code>min_samples_split=2</code>: Minimum number of samples required to split an internal node.</li>\n",
    "                <li><code>min_samples_leaf=1</code>: Minimum number of samples required to be at a leaf node.</li>\n",
    "                <li><code>criterion=\"squared_error\"</code>: Function to measure the quality of a split. Options include <code>\"squared_error\"</code> (mean squared error) and <code>\"friedman_mse\"</code> (Friedman‚Äôs mean squared error).</li>\n",
    "                <li><code>splitter=\"best\"</code>: Strategy to choose the split at each node. Options are <code>\"best\"</code> (best split) and <code>\"random\"</code> (random split).</li>\n",
    "                <li><code>max_features=None</code>: Number of features to consider when looking for the best split. If <code>None</code>, all features are considered.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization:\n",
    "            <ul>\n",
    "                <li><code>ccp_alpha=0.0</code>: Complexity parameter for pruning. A higher value encourages pruning by penalizing tree complexity.</li>\n",
    "                <li><code>min_impurity_decrease=0.0</code>: A node will split only if the impurity decrease exceeds this threshold.</li>\n",
    "                <li><code>max_leaf_nodes=None</code>: Maximum number of leaf nodes in the tree.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>min_weight_fraction_leaf=0.0</code>: Minimum weighted fraction of the sum of weights required at a leaf node.</li>\n",
    "                <li><code>max_samples=None</code>: (Only relevant for certain ensemble methods; ignored in standalone <code>DecisionTreeRegressor</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>presort=\"deprecated\"</code>: Pre-sorting data for faster splits has been deprecated in recent versions.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\" target=\"_blank\">scikit-learn DecisionTreeRegressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022677f-2f63-43e7-83df-ef2fbe671e58",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Decision Tree Classifier</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>max_depth=None</code>: Maximum depth of the tree. <code>None</code> allows nodes to expand until all leaves are pure or contain fewer samples than <code>min_samples_split</code>.</li>\n",
    "                <li><code>min_samples_split=2</code>: Minimum number of samples required to split an internal node.</li>\n",
    "                <li><code>min_samples_leaf=1</code>: Minimum number of samples required to be at a leaf node.</li>\n",
    "                <li><code>criterion=\"gini\"</code>: Function to measure the quality of a split. Options include <code>\"gini\"</code> (Gini impurity) and <code>\"entropy\"</code> (information gain).</li>\n",
    "                <li><code>splitter=\"best\"</code>: Strategy to choose the split at each node. Options are <code>\"best\"</code> (best split) and <code>\"random\"</code> (random split).</li>\n",
    "                <li><code>max_features=None</code>: Number of features to consider when looking for the best split. If <code>None</code>, all features are considered.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization:\n",
    "            <ul>\n",
    "                <li><code>ccp_alpha=0.0</code>: Complexity parameter for pruning. A higher value encourages pruning by penalizing tree complexity.</li>\n",
    "                <li><code>min_impurity_decrease=0.0</code>: A node will split only if the impurity decrease exceeds this threshold.</li>\n",
    "                <li><code>max_leaf_nodes=None</code>: Maximum number of leaf nodes in the tree.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>min_weight_fraction_leaf=0.0</code>: Minimum weighted fraction of the sum of weights required at a leaf node.</li>\n",
    "                <li><code>class_weight=None</code>: Weights associated with classes. If <code>None</code>, all classes are given equal weight. Can be <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies.</li>\n",
    "                <li><code>max_samples=None</code>: (Only relevant for certain ensemble methods; ignored in standalone <code>DecisionTreeClassifier</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>presort=\"deprecated\"</code>: Pre-sorting data for faster splits has been deprecated in recent versions.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" target=\"_blank\">scikit-learn DecisionTreeClassifier documentation</a>.  \n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175f3af-0b39-4117-93a2-7dd76d04d460",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Random Forest</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Random Forest Regressor</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_estimators=100</code>: Number of trees in the forest.</li>\n",
    "                <li><code>max_depth=None</code>: Maximum depth of each tree; <code>None</code> allows trees to grow until all leaves are pure or minimum samples are reached.</li>\n",
    "                <li><code>min_samples_split=2</code>: Minimum number of samples required to split a node.</li>\n",
    "                <li><code>min_samples_leaf=1</code>: Minimum number of samples required at a leaf node.</li>\n",
    "                <li><code>max_features=\"auto\"</code>: Number of features considered for the best split; default <code>auto</code> uses the square root of all features.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization:\n",
    "            <ul>\n",
    "                <li><code>max_leaf_nodes=None</code>: Maximum number of leaf nodes per tree.</li>\n",
    "                <li><code>min_impurity_decrease=0.0</code>: Splits a node only if it decreases impurity by this threshold.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>bootstrap=True</code>: Whether to use bootstrap samples for training each tree.</li>\n",
    "                <li><code>oob_score=False</code>: Whether to use out-of-bag samples to estimate prediction accuracy.</li>\n",
    "                <li><code>n_jobs=None</code>: Number of CPU threads used (<code>-1</code> for all processors).</li>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>verbose=0</code>: Controls the verbosity of output during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>max_samples=None</code>: Maximum number of samples used to train each tree, useful for subsampling large datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\" target=\"_blank\">scikit-learn RandomForestRegressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2183cf7-e218-4d81-bb0f-2e2f84409ea4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Random Forest Classifier</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_estimators=100</code>: Number of trees in the forest.</li>\n",
    "                <li><code>max_depth=None</code>: Maximum depth of each tree; <code>None</code> allows trees to grow until all leaves are pure or minimum samples are reached.</li>\n",
    "                <li><code>min_samples_split=2</code>: Minimum number of samples required to split a node.</li>\n",
    "                <li><code>min_samples_leaf=1</code>: Minimum number of samples required at a leaf node.</li>\n",
    "                <li><code>max_features=\"auto\"</code>: Number of features considered for the best split; default <code>\"auto\"</code> uses the square root of all features.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization:\n",
    "            <ul>\n",
    "                <li><code>max_leaf_nodes=None</code>: Maximum number of leaf nodes per tree.</li>\n",
    "                <li><code>min_impurity_decrease=0.0</code>: Splits a node only if it decreases impurity by this threshold.</li>\n",
    "                <li><code>class_weight=None</code>: Weights associated with classes. If <code>None</code>, all classes are supposed to have weight one. Use <code>\"balanced\"</code> to automatically adjust weights inversely proportional to class frequencies in the input data.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>bootstrap=True</code>: Whether to use bootstrap samples for training each tree.</li>\n",
    "                <li><code>oob_score=False</code>: Whether to use out-of-bag samples to estimate prediction accuracy.</li>\n",
    "                <li><code>n_jobs=None</code>: Number of CPU threads used (<code>-1</code> for all processors).</li>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>verbose=0</code>: Controls the verbosity of output during training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>max_samples=None</code>: Maximum number of samples used to train each tree, useful for subsampling large datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\" target=\"_blank\">scikit-learn RandomForestClassifier documentation</a>.  \n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d879d09-9ee1-4318-9714-01548595c18e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Multi-Layer Perceptron</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Multi-Layer Perceptron Regressor</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Architecture:\n",
    "            <ul>\n",
    "                <li><code>hidden_layer_sizes=(100,)</code>: Defines the size and number of hidden layers; <code>(100,)</code> indicates one layer with 100 neurons.</li>\n",
    "                <li><code>activation=\"relu\"</code>: Activation function for the hidden layers; options include <code>\"relu\"</code>, <code>\"tanh\"</code>, <code>\"logistic\"</code>, or <code>\"identity\"</code>.</li>\n",
    "                <li><code>solver=\"adam\"</code>: Optimization algorithm; options are <code>\"adam\"</code> (default), <code>\"lbfgs\"</code>, or <code>\"sgd\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization and Learning:\n",
    "            <ul>\n",
    "                <li><code>alpha=0.0001</code>: L2 regularization term to prevent overfitting.</li>\n",
    "                <li><code>learning_rate=\"constant\"</code>: Strategy for learning rate adjustment; options are <code>\"constant\"</code>, <code>\"invscaling\"</code>, or <code>\"adaptive\"</code>.</li>\n",
    "                <li><code>learning_rate_init=0.001</code>: Initial learning rate for weight updates.</li>\n",
    "                <li><code>power_t=0.5</code>: Exponent for inverse scaling of learning rate (used when <code>learning_rate=\"invscaling\"</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>max_iter=200</code>: Maximum number of iterations for training.</li>\n",
    "                <li><code>tol=1e-4</code>: Tolerance for stopping criteria; training stops if loss improvement is below this value.</li>\n",
    "                <li><code>momentum=0.9</code>: Momentum parameter for gradient descent updates (used when <code>solver=\"sgd\"</code>).</li>\n",
    "                <li><code>n_iter_no_change=10</code>: Number of iterations with no improvement to stop early.</li>\n",
    "                <li><code>early_stopping=False</code>: Enables early stopping when validation score doesn‚Äôt improve.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>batch_size=\"auto\"</code>: Number of samples per batch for training; <code>\"auto\"</code> uses <code>min(200, n_samples)</code>.</li>\n",
    "                <li><code>shuffle=True</code>: Whether to shuffle training data before each epoch.</li>\n",
    "                <li><code>random_state=None</code>: Random seed for reproducibility.</li>\n",
    "                <li><code>verbose=False</code>: Controls verbosity of output during training.</li>\n",
    "                <li><code>warm_start=False</code>: Reuses previous solution to initialize weights for additional fitting.</li>\n",
    "                <li><code>beta_1=0.9</code>, <code>beta_2=0.999</code>: Exponential decay rates for moving averages of gradients and squared gradients (used in <code>solver=\"adam\"</code>).</li>\n",
    "                <li><code>epsilon=1e-8</code>: Small value to prevent division by zero in <code>solver=\"adam\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\" target=\"_blank\">scikit-learn MLPRegressor documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc18546-2772-4b82-9c3f-f2282989ee64",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>Multi-Layer Perceptron Classifier</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Architecture:\n",
    "            <ul>\n",
    "                <li><code>hidden_layer_sizes=(100,)</code>: Number and size of hidden layers; e.g., <code>(100,)</code> = 1 layer with 100 neurons.</li>\n",
    "                <li><code>activation=\"relu\"</code>: Activation function; options: <code>\"relu\"</code>, <code>\"tanh\"</code>, <code>\"logistic\"</code>, <code>\"identity\"</code>.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Optimization:\n",
    "            <ul>\n",
    "                <li><code>solver=\"adam\"</code>: Optimization algorithm; options: <code>\"adam\"</code>, <code>\"lbfgs\"</code>, <code>\"sgd\"</code>.</li>\n",
    "                <li><code>alpha=0.0001</code>: L2 regularization to reduce overfitting.</li>\n",
    "                <li><code>learning_rate=\"constant\"</code>: Learning rate strategy; options: <code>\"constant\"</code>, <code>\"invscaling\"</code>, <code>\"adaptive\"</code>.</li>\n",
    "                <li><code>learning_rate_init=0.001</code>: Initial learning rate.</li>\n",
    "                <li><code>power_t=0.5</code>: Used for <code>\"invscaling\"</code> learning rate.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>max_iter=200</code>: Maximum training iterations.</li>\n",
    "                <li><code>tol=1e-4</code>: Stopping criterion for improvement tolerance.</li>\n",
    "                <li><code>momentum=0.9</code>: Momentum for SGD (used with <code>solver=\"sgd\"</code>).</li>\n",
    "                <li><code>early_stopping=False</code>: Stop early if no improvement on validation set.</li>\n",
    "                <li><code>n_iter_no_change=10</code>: Iterations without improvement to stop training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance:\n",
    "            <ul>\n",
    "                <li><code>batch_size=\"auto\"</code>: Batch size; <code>\"auto\"</code> = <code>min(200, n_samples)</code>.</li>\n",
    "                <li><code>shuffle=True</code>: Shuffle training data every epoch.</li>\n",
    "                <li><code>random_state=None</code>: Seed for reproducibility.</li>\n",
    "                <li><code>verbose=False</code>: Control output verbosity.</li>\n",
    "                <li><code>warm_start=False</code>: Retain model state for further training.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Classification-Specific:\n",
    "            <ul>\n",
    "                <li><code>validation_fraction=0.1</code>: Fraction of data for validation (used with <code>early_stopping=True</code>).</li>\n",
    "                <li><code>out_activation_=\"softmax\"</code>: Output activation for multi-class classification.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\" target=\"_blank\">scikit-learn MLPClassifier documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fc356-392f-4cd9-bd3e-26044a3556ba",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">XGBoost</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>XGBoost Regressor</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_estimators=100</code>: Number of trees.</li>\n",
    "                <li><code>max_depth=6</code>: Maximum depth of each tree.</li>\n",
    "                <li><code>learning_rate=0.3</code>: Step size shrinking to prevent overfitting.</li>\n",
    "                <li><code>subsample=1.0</code>: Fraction of training samples used for each tree.</li>\n",
    "                <li><code>colsample_bytree=1.0</code>: Fraction of features used for each tree.</li>\n",
    "                <li><code>colsample_bylevel=1.0</code>: Fraction of features used at each tree level.</li>\n",
    "                <li><code>colsample_bynode=1.0</code>: Fraction of features used at each node.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization and Learning:\n",
    "            <ul>\n",
    "                <li><code>gamma=0</code>: Minimum loss reduction required to make a further partition on a leaf node.</li>\n",
    "                <li><code>min_child_weight=1</code>: Minimum sum of instance weight (hessian) in a child.</li>\n",
    "                <li><code>scale_pos_weight=1</code>: Controls the balance of positive and negative weights; used for imbalanced datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>objective=\"reg:squarederror\"</code>: Objective function for regression; default is for squared error.</li>\n",
    "                <li><code>booster=\"gbtree\"</code>: Booster type; options include <code>\"gbtree\"</code> (default), <code>\"gblinear\"</code>, and <code>\"dart\"</code>.</li>\n",
    "                <li><code>tree_method=\"auto\"</code>: Tree construction algorithm; <code>\"auto\"</code> chooses based on system configuration. Options include <code>\"exact\"</code>, <code>\"approx\"</code>, and <code>\"hist\"</code>.</li>\n",
    "                <li><code>eval_metric=\"rmse\"</code>: Metric used for validation during training; default is root mean square error (<code>rmse</code>).</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>early_stopping_rounds=None</code>: Stops training if validation metric does not improve after specified rounds.</li>\n",
    "                <li><code>n_jobs=1</code>: Number of threads used for parallel computation (<code>-1</code> for all processors).</li>\n",
    "                <li><code>random_state=None</code>: Seed for reproducibility.</li>\n",
    "                <li><code>verbose=1</code>: Verbosity level for training output; <code>0</code> for silent, higher values show more details.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Advanced Parameters:\n",
    "            <ul>\n",
    "                <li><code>lambda=1</code>: L2 regularization term on weights.</li>\n",
    "                <li><code>alpha=0</code>: L1 regularization term on weights.</li>\n",
    "                <li><code>max_delta_step=0</code>: Used to help with convergence in highly imbalanced datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d4310-1aeb-483e-a66a-2f28083f4745",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    <strong>XGBoost Classifier</strong> <br>\n",
    "    ‚ÑπÔ∏è Hyperparameters and Default Values:\n",
    "    <ul>\n",
    "        <li>Model Complexity:\n",
    "            <ul>\n",
    "                <li><code>n_estimators=100</code>: Number of trees (boosting rounds).</li>\n",
    "                <li><code>max_depth=6</code>: Maximum depth of each tree.</li>\n",
    "                <li><code>learning_rate=0.3</code>: Step size shrinkage to prevent overfitting.</li>\n",
    "                <li><code>subsample=1.0</code>: Fraction of training samples used per tree.</li>\n",
    "                <li><code>colsample_bytree=1.0</code>: Fraction of features used per tree.</li>\n",
    "                <li><code>colsample_bylevel=1.0</code>: Fraction of features used per tree level.</li>\n",
    "                <li><code>colsample_bynode=1.0</code>: Fraction of features used per split node.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Regularization and Learning:\n",
    "            <ul>\n",
    "                <li><code>gamma=0</code>: Minimum loss reduction required to split a leaf node.</li>\n",
    "                <li><code>min_child_weight=1</code>: Minimum sum of instance weights (hessian) in a child.</li>\n",
    "                <li><code>scale_pos_weight=1</code>: Balances positive and negative class weights for imbalanced datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Training Behavior:\n",
    "            <ul>\n",
    "                <li><code>objective=\"binary:logistic\"</code>: Objective function for binary classification; alternatives include <code>\"multi:softmax\"</code> or <code>\"multi:softprob\"</code>.</li>\n",
    "                <li><code>booster=\"gbtree\"</code>: Booster type; options include <code>\"gbtree\"</code> (default), <code>\"gblinear\"</code>, and <code>\"dart\"</code>.</li>\n",
    "                <li><code>tree_method=\"auto\"</code>: Tree construction algorithm; options: <code>\"exact\"</code>, <code>\"approx\"</code>, <code>\"hist\"</code>, <code>\"gpu_hist\"</code>.</li>\n",
    "                <li><code>eval_metric=\"logloss\"</code>: Default evaluation metric for binary classification. Options include <code>\"error\"</code>, <code>\"auc\"</code>, and others.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Performance Optimization:\n",
    "            <ul>\n",
    "                <li><code>early_stopping_rounds=None</code>: Stops training if validation metric does not improve after specified rounds.</li>\n",
    "                <li><code>n_jobs=1</code>: Number of threads for parallel computation (<code>-1</code> for all processors).</li>\n",
    "                <li><code>random_state=None</code>: Seed for reproducibility.</li>\n",
    "                <li><code>verbose=1</code>: Verbosity level; 0 for silent, higher values for detailed output.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Advanced Parameters:\n",
    "            <ul>\n",
    "                <li><code>lambda=1</code>: L2 regularization term on weights.</li>\n",
    "                <li><code>alpha=0</code>: L1 regularization term on weights.</li>\n",
    "                <li><code>max_delta_step=0</code>: Helps with convergence in imbalanced datasets.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    For more details, refer to the official <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html\" target=\"_blank\">XGBoost documentation</a>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-venv",
   "language": "python",
   "name": "machine-learning-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
