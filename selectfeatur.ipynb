{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c64ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Boruta\n",
      "  Downloading Boruta-0.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\program files\\python311\\lib\\site-packages (from Boruta) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in c:\\program files\\python311\\lib\\site-packages (from Boruta) (1.6.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\program files\\python311\\lib\\site-packages (from Boruta) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\program files\\python311\\lib\\site-packages (from scikit-learn>=0.17.1->Boruta) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\program files\\python311\\lib\\site-packages (from scikit-learn>=0.17.1->Boruta) (3.5.0)\n",
      "Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
      "Installing collected packages: Boruta\n",
      "Successfully installed Boruta-0.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install Boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56168e",
   "metadata": {},
   "source": [
    "$$\\text{D·ªØ li·ªáu G·ªëc} \\rightarrow \\text{EDA} \\rightarrow \\text{X·ª≠ l√Ω Gi√° tr·ªã Thi·∫øu/Encoding} \n",
    "\n",
    "\\rightarrow \\text{TRAIN-TEST SPLIT} \\rightarrow \\text{Scaling/PCA} \\rightarrow \\text{Training}\n",
    "\n",
    " \\rightarrow \\text{Evaluation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde51e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# instantiate random forest\n",
    "forest = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
    "\n",
    "# fit boruta\n",
    "boruta_selector = BorutaPy(forest, n_estimators = 'auto', random_state = 0)\n",
    "boruta_selector.fit(np.array(X_trn), np.array(y_trn))\n",
    "\n",
    "# store results\n",
    "boruta_ranking = boruta_selector.ranking_\n",
    "selected_features = np.array(feature_names)[boruta_ranking <= 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efd1b1",
   "metadata": {},
   "source": [
    "# Unsupervise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7c6f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     14\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     15\u001b[0m data_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# 5 C·ªôt s·ªë (Num): A v√† D t∆∞∆°ng quan cao, E ph∆∞∆°ng sai th·∫•p\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_A\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(data_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_B\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m10\u001b[39m, data_size),\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_C\u001b[39m\u001b[38;5;124m'\u001b[39m: (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(data_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m) \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(data_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.8\u001b[39m), \n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_D\u001b[39m\u001b[38;5;124m'\u001b[39m: (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(data_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_A\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.95\u001b[39m), \u001b[38;5;66;03m# T∆∞∆°ng quan r·∫•t cao v·ªõi Num_A\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum_E\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(data_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;66;03m# Ph∆∞∆°ng sai th·∫•p\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# 4 C·ªôt ph√¢n lo·∫°i (Cat): H c√≥ t·∫ßn su·∫•t th·ªëng tr·ªã (>95%)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCat_F\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlue\u001b[39m\u001b[38;5;124m'\u001b[39m], data_size, p\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]),\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCat_G\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m], data_size, p\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]),\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCat_H\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m490\u001b[39m \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCat_I\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m], data_size)\n\u001b[0;32m     29\u001b[0m })\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Th√™m 40 c·ªôt gi·∫£ l·∫≠p kh√°c ƒë·ªÉ m√¥ ph·ªèng 49 c·ªôt\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m41\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. T·∫†O D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P V√Ä X√ÅC ƒê·ªäNH C·ªòT (Thay th·∫ø b·∫±ng data th·ª±c c·ªßa b·∫°n)\n",
    "# ==============================================================================\n",
    "np.random.seed(42)\n",
    "data_size = 500\n",
    "df = pd.DataFrame({\n",
    "    # 5 C·ªôt s·ªë (Num): A v√† D t∆∞∆°ng quan cao, E ph∆∞∆°ng sai th·∫•p\n",
    "    'Num_A': np.random.rand(data_size) * 10,\n",
    "    'Num_B': np.random.normal(50, 10, data_size),\n",
    "    'Num_C': (np.random.rand(data_size) * 0.9) + (np.random.rand(data_size) * 0.8), \n",
    "    'Num_D': (np.random.rand(data_size) * 0.1) + (df['Num_A'] * 0.95), # T∆∞∆°ng quan r·∫•t cao v·ªõi Num_A\n",
    "    'Num_E': np.random.rand(data_size) * 0.001 + 100, # Ph∆∞∆°ng sai th·∫•p\n",
    "\n",
    "    # 4 C·ªôt ph√¢n lo·∫°i (Cat): H c√≥ t·∫ßn su·∫•t th·ªëng tr·ªã (>95%)\n",
    "    'Cat_F': np.random.choice(['Red', 'Green', 'Blue'], data_size, p=[0.7, 0.2, 0.1]),\n",
    "    'Cat_G': np.random.choice(['A', 'B', 'C', 'D'], data_size, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'Cat_H': ['High'] * 490 + ['Low'] * 10, \n",
    "    'Cat_I': np.random.choice(['X', 'Y', 'Z'], data_size)\n",
    "})\n",
    "# Th√™m 40 c·ªôt gi·∫£ l·∫≠p kh√°c ƒë·ªÉ m√¥ ph·ªèng 49 c·ªôt\n",
    "for i in range(1, 41):\n",
    "    df[f'Other_Col_{i}'] = np.random.rand(data_size)\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng ph·∫£i s·ªë/chu·ªói n·∫øu c√≥\n",
    "df = df.select_dtypes(include=['number', 'object'])\n",
    "print(f\"T·ªïng s·ªë c·ªôt ban ƒë·∫ßu: {df.shape[1]}\")\n",
    "\n",
    "# Ph√¢n lo·∫°i c·ªôt ban ƒë·∫ßu\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. EDA V√Ä L·ª∞A CH·ªåN BI·∫æN (FEATURE SELECTION) TR√äN TO√ÄN B·ªò D·ªÆ LI·ªÜU G·ªêC\n",
    "# ==============================================================================\n",
    "\n",
    "# Kh·ªüi t·∫°o t·∫≠p h·ª£p c√°c c·ªôt c·∫ßn lo·∫°i b·ªè\n",
    "cols_to_drop = set()\n",
    "\n",
    "# --- 2.1. Lo·∫°i b·ªè Missing Values (>50%) ---\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "cols_to_drop_missing = missing_percentage[missing_percentage > 50].index.tolist()\n",
    "cols_to_drop.update(cols_to_drop_missing)\n",
    "\n",
    "if cols_to_drop_missing:\n",
    "    print(f\"üö´ Lo·∫°i b·ªè {len(cols_to_drop_missing)} c·ªôt c√≥ >50% Missing Values.\")\n",
    "else:\n",
    "    print(\"‚úÖ Kh√¥ng c√≥ c·ªôt n√†o b·ªã lo·∫°i b·ªè do Missing Values qu√° cao.\")\n",
    "\n",
    "# C·∫≠p nh·∫≠t danh s√°ch c·ªôt sau khi lo·∫°i b·ªè Missing\n",
    "num_cols = [col for col in num_cols if col not in cols_to_drop]\n",
    "cat_cols = [col for col in cat_cols if col not in cols_to_drop]\n",
    "\n",
    "# --- 2.2. L·ª±a ch·ªçn Bi·∫øn S·ªë (Numerical Feature Selection) ---\n",
    "STD_THRESHOLD = 0.01 \n",
    "CORR_THRESHOLD = 0.9 \n",
    "\n",
    "# Ph∆∞∆°ng sai Th·∫•p\n",
    "low_variance_cols = [col for col in num_cols if df[col].std() < STD_THRESHOLD]\n",
    "cols_to_drop.update(low_variance_cols)\n",
    "if low_variance_cols:\n",
    "    print(f\"\\nüö´ Lo·∫°i b·ªè do Ph∆∞∆°ng sai Th·∫•p (STD < {STD_THRESHOLD}): {low_variance_cols}\")\n",
    "\n",
    "# T∆∞∆°ng quan Cao\n",
    "num_cols_after_var = [col for col in num_cols if col not in cols_to_drop]\n",
    "corr_matrix = df[num_cols_after_var].corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "for column in upper.columns:\n",
    "    correlated_cols = upper.index[upper[column] > CORR_THRESHOLD].tolist()\n",
    "    for row in correlated_cols:\n",
    "        if column not in cols_to_drop:\n",
    "            cols_to_drop.add(column) \n",
    "            print(f\"   - T∆∞∆°ng quan cao ({upper.loc[row, column]:.2f}): Gi·ªØ '{row}', Lo·∫°i b·ªè '{column}'\")\n",
    "\n",
    "# --- 2.3. L·ª±a ch·ªçn Bi·∫øn Ph√¢n lo·∫°i (Categorical Feature Selection) ---\n",
    "FREQ_THRESHOLD = 0.95 \n",
    "\n",
    "# T·∫ßn su·∫•t Th·ªëng tr·ªã\n",
    "cat_to_drop_freq = [col for col in cat_cols if df[col].value_counts(normalize=True).iloc[0] > FREQ_THRESHOLD]\n",
    "cols_to_drop.update(cat_to_drop_freq)\n",
    "if cat_to_drop_freq:\n",
    "    print(f\"\\nüö´ Lo·∫°i b·ªè c·ªôt Cat do T·∫ßn su·∫•t Th·ªëng tr·ªã (>95%): {cat_to_drop_freq}\")\n",
    "\n",
    "# T∆∞∆°ng quan Cramer's V (ƒë∆°n gi·∫£n h√≥a v√¨ hi·∫øm khi d√πng cho l∆∞·ª£ng l·ªõn c·ªôt)\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(r - 1, k - 1))\n",
    "\n",
    "# (B·ªè qua vi·ªác t√≠nh Cramer's V l·∫∑p v√¨ qu√° t·ªën k√©m tr√™n code m·∫´u 49 c·ªôt)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. K·∫æT QU·∫¢ FEATURE SELECTION V√Ä CHIA TRAIN-TEST SPLIT\n",
    "# ==============================================================================\n",
    "final_selected_cols = [col for col in df.columns if col not in cols_to_drop]\n",
    "final_num_cols = [col for col in num_cols if col not in cols_to_drop]\n",
    "final_cat_cols = [col for col in cat_cols if col not in cols_to_drop]\n",
    "\n",
    "# √Åp d·ª•ng Feature Selection\n",
    "df_clean = df[final_selected_cols]\n",
    "\n",
    "# CHIA TRAIN/TEST\n",
    "df_train, df_test = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"T·ªîNG K·∫æT FS: ƒê√£ gi·∫£m t·ª´ {df.shape[1]} c·ªôt xu·ªëng c√≤n {len(final_selected_cols)} c·ªôt.\")\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p Train ƒë√£ l√†m s·∫°ch: {df_train.shape}\")\n",
    "print(f\"C·ªôt S·ªë cu·ªëi c√πng ({len(final_num_cols)}): {final_num_cols[:5]}...\")\n",
    "print(f\"C·ªôt Cat cu·ªëi c√πng ({len(final_cat_cols)}): {final_cat_cols}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TI·ªÄN X·ª¨ L√ù (PREPROCESSING) SAU KHI CHIA (√Åp d·ª•ng cho H·ªçc Kh√¥ng Gi√°m S√°t)\n",
    "# ==============================================================================\n",
    "\n",
    "# Ch√∫ng ta ch·ªâ FIT tr√™n t·∫≠p TRAIN ƒë·ªÉ tr√°nh Data Leakage\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Scaling cho c·ªôt s·ªë (Quan tr·ªçng cho K-Means, PCA)\n",
    "        ('num', StandardScaler(), final_num_cols),\n",
    "        # Encoding cho c·ªôt ph√¢n lo·∫°i\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), final_cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough' # Gi·ªØ l·∫°i c√°c c·ªôt kh√°c (n·∫øu c√≥)\n",
    ")\n",
    "\n",
    "# X√¢y d·ª±ng Pipeline (D√π kh√¥ng c√≥ m√¥ h√¨nh, v·∫´n d√πng Pipeline ƒë·ªÉ √°p d·ª•ng Preprocessing)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# FIT v√† TRANSFORM tr√™n TRAIN\n",
    "df_train_processed = pipeline.fit_transform(df_train)\n",
    "\n",
    "# CH·ªà TRANSFORM tr√™n TEST\n",
    "df_test_processed = pipeline.transform(df_test)\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ DataFrame ƒë·ªÉ d·ªÖ nh√¨n h∆°n\n",
    "feature_names = final_num_cols + list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(final_cat_cols))\n",
    "df_train_final = pd.DataFrame(df_train_processed, columns=feature_names)\n",
    "df_test_final = pd.DataFrame(df_test_processed, columns=feature_names)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p Train SAU PREPROCESSING: {df_train_final.shape} (S·ªë c·ªôt ƒë√£ tƒÉng do One-Hot Encoding)\")\n",
    "print(\"D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho c√°c thu·∫≠t to√°n Unsupervised Learning (K-Means, PCA, v.v.).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232afc2",
   "metadata": {},
   "source": [
    "# Supervise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb74dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "\n",
    "# ==============================================================================\n",
    "# 1. N·∫†P D·ªÆ LI·ªÜU V√Ä CHIA TRAIN-TEST SPLIT (Feature Selection ƒë∆∞·ª£c b·ªè qua cho ng·∫Øn g·ªçn)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- T·∫†O D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P V√Ä BI·∫æN M·ª§C TI√äU ---\n",
    "np.random.seed(42)\n",
    "data_size = 500\n",
    "df = pd.DataFrame({\n",
    "    'Num_A': np.random.rand(data_size) * 10,\n",
    "    'Num_B': np.random.normal(50, 10, data_size),\n",
    "    'Cat_F': np.random.choice(['Red', 'Green', 'Blue'], data_size, p=[0.7, 0.2, 0.1]),\n",
    "    'Cat_G': np.random.choice(['A', 'B', 'C', 'D'], data_size, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'Target': np.where((np.random.rand(data_size) + np.random.rand(data_size)) > 1, 1, 0)\n",
    "})\n",
    "\n",
    "# X√°c ƒë·ªãnh c·ªôt\n",
    "TARGET_COL = 'Target'\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Chia Train/Test (s·ª≠ d·ª•ng stratify)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Ph√¢n lo·∫°i c·ªôt sau khi chia\n",
    "num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc X_train: {X_train.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. TI·ªÄN X·ª¨ L√ù (PREPROCESSING) TH·ª¶ C√îNG (KH√îNG D√ôNG PIPELINE)\n",
    "# ==============================================================================\n",
    "\n",
    "# Kh·ªüi t·∫°o c√°c b·ªô bi·∫øn ƒë·ªïi\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# --- A. X·ª¨ L√ù C·ªòT S·ªê (SCALING) ---\n",
    "\n",
    "print(\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c·ªôt s·ªë (Scaling)...\")\n",
    "\n",
    "# 1. FIT (ch·ªâ tr√™n X_train)\n",
    "scaler.fit(X_train[num_cols])\n",
    "\n",
    "# 2. TRANSFORM (tr√™n X_train v√† X_test)\n",
    "X_train_scaled = scaler.transform(X_train[num_cols])\n",
    "X_test_scaled = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ DataFrame t·∫°m th·ªùi (ƒë·ªÉ d·ªÖ d√†ng gh√©p n·ªëi)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=num_cols, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=num_cols, index=X_test.index)\n",
    "\n",
    "\n",
    "# --- B. X·ª¨ L√ù C·ªòT PH√ÇN LO·∫†I (ENCODING) ---\n",
    "\n",
    "print(\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c·ªôt ph√¢n lo·∫°i (One-Hot Encoding)...\")\n",
    "\n",
    "# 1. FIT (ch·ªâ tr√™n X_train)\n",
    "encoder.fit(X_train[cat_cols])\n",
    "\n",
    "# L·∫•y t√™n c·ªôt m·ªõi sau One-Hot Encoding\n",
    "encoded_cols = encoder.get_feature_names_out(cat_cols)\n",
    "\n",
    "# 2. TRANSFORM (tr√™n X_train v√† X_test)\n",
    "X_train_encoded = encoder.transform(X_train[cat_cols])\n",
    "X_test_encoded = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ DataFrame t·∫°m th·ªùi\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_cols, index=X_train.index)\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_cols, index=X_test.index)\n",
    "\n",
    "\n",
    "# --- C. GH√âP N·ªêI (CONCATENATE) D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù ---\n",
    "\n",
    "# Lo·∫°i b·ªè c√°c c·ªôt g·ªëc ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω kh·ªèi t·∫≠p d·ªØ li·ªáu ban ƒë·∫ßu\n",
    "X_train_removed = X_train.drop(columns=num_cols + cat_cols)\n",
    "X_test_removed = X_test.drop(columns=num_cols + cat_cols)\n",
    "\n",
    "# Gh√©p n·ªëi c√°c t·∫≠p d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n",
    "X_train_final = pd.concat([X_train_scaled_df, X_train_encoded_df, X_train_removed], axis=1)\n",
    "X_test_final = pd.concat([X_test_scaled_df, X_test_encoded_df, X_test_removed], axis=1)\n",
    "\n",
    "print(f\"\\nK√≠ch th∆∞·ªõc X_train sau x·ª≠ l√Ω: {X_train_final.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. HU·∫§N LUY·ªÜN V√Ä ƒê√ÅNH GI√Å M√î H√åNH\n",
    "# ==============================================================================\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n t·∫≠p TRAIN cu·ªëi c√πng\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# D·ª± ƒëo√°n tr√™n t·∫≠p TEST cu·ªëi c√πng\n",
    "y_pred = model.predict(X_test_final)\n",
    "\n",
    "# ƒê√°nh gi√°\n",
    "print(\"--- ƒê√ÅNH GI√Å M√î H√åNH TR√äN T·∫¨P TEST ---\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
